# 딥러닝 기반 자연어 처리의 발전 
## 1. 통계적 자연어 처리의 등장

### 1.1 규칙 기반에서 통계 기반으로의 전환

1980년대 후반 이후 말뭉치(corpus) 축적과 계산 자원의 향상으로 자연어 처리가 **규칙 기반 방식**에서 **통계 기반 방식**으로 전환

* 대표적 사례는 **통계적 기계번역(Statistical Machine Translation, SMT)**

    : 입력 문장을 단어 단위로 분할하고 병렬 말뭉치에서 단어 대응 확률을 계산하여 가장 가능성이 높은 번역 결과를 선택

* 예시:
“나는 학교에 간다” → “I go to school”
(각 단어의 출현 확률에 기반한 조합 선택)
---

## 2. 단어 임베딩 (Word Embedding)

### 2.1 Bag of Words (BoW)

말뭉치 내 모든 단어를 인덱스로 정의하고, 문장을 단어 출현 여부 또는 빈도로 표현하는 방식
**한계점**

* 차원이 매우 커지고 희소(sparse)한 벡터 구조가 발생
* 단어 간 의미적 유사성 표현 불가
* **OOV(Out-of-Vocabulary)** 문제 발생 -> 새로운 단어가 등장하면 처리 불가

### 2.2 Word2Vec 

단어를 **밀집 벡터(Dense Vector)**로 변환하여 의미적 관계를 반영
**학습 구조**

* **CBOW (Continuous Bag-of-Words)**: 주변 단어로 중심 단어 예측
* **Skip-gram**: 중심 단어로 주변 단어 예측
  **특징**
* 코사인 유사도 기반의 의미적 거리 계산 가능
* 벡터 연산으로 의미 유추 가능
  예: `king - man + woman ≈ queen`

### 2.3 GloVe와 FastText

* **GloVe (2014)**: 전체 말뭉치의 동시출현 통계(global co-occurrence) 활용
* **FastText (2017)**: 단어를 n-gram 형태의 **서브워드(subword)** 단위로 학습하여 OOV 문제 완화

---

## 3. 시퀀스 기반 모델: Seq2Seq

### 3.1 구조

![]()
입력 시퀀스를 인코더 RNN이 처리하여 **맥락 벡터(context vector)**로 요약하고,
디코더 RNN이 해당 벡터를 입력받아 출력 시퀀스를 순차적으로 생성
* 맥락 벡터만 최종적으로 디코더에게 전달
### 3.2 한계

1. **장기 의존성 문제(Long-term Dependency)** 발생 : 맥락벡터가 자연어의 복잡한 정보를 담기에 충분히 크지 않을때 입력데이터 전체를 대표하기 어려움
2. **경사 소실 및 폭발 문제(Vanishing/Exploding Gradient)**로 학습 불안정성 발생 - 특시 시퀀스 길이가 길어질 수록(2개의 순환신경망으로 되어 있어서 문제가 더 큼)
3. 문맥 변화에 따른 의미 해석 반영의 한계(동음이의어 처리 등)

---

## 4. 어텐션 메커니즘 

### 4.1 개념

Seq2Seq의 문제를 해결하기 위해 어텐션 메커니즘을 제안
* 출력 단어 예측 시 입력 전체를 다시 참조하여 **관련성이 높은 단어에 더 큰 가중치 부여**
![]()

* 빨간 직사각형 크기 : 소프트 맥스 함수의 결과, 이게 클 수록 중요함

### 4.2 계산 과정

1. 어텐션 스코어 계산 : 인코더 시점별 은닉상태와 디코더 t시점 은닉상태를 내적
   $$ e_{ti} = s_t^T h_i $$
2. 가중치 계산 : 어텐션 스코어를 소프트 맥스 통과시켜서 가중치 어텐션 가중치 얻음
   $$ \alpha_{ti} = \frac{\exp(e_{ti})}{\sum_j \exp(e_{tj})} $$
3. 어텐션 값 계산 : 인코더의 은닉상태와 어텐션 가중치를 가중합
   $$ a_t = \sum_i \alpha_{ti} h_i $$
![]()
4. 출력 벡터 계산 : 디코더 은닉상태와 어텐션 값을 concat 
   $$ \tilde{s}_t = \tanh(W_c[a_t; s_t]) $$
5. 예측 벡터 : Concat한 애를 소프트 맥스 통과시킴
![]()
---
## 5. 트랜스포머 

### 5.1 등장 배경

“Attention is All You Need” 논문에서 제안된 모델
RNN 구조를 완전히 제거하고 **어텐션 메커니즘만으로 시퀀스 간 의존성 학습**
병렬 연산이 가능하여 학습 효율 및 속도 향상

* 어텐션은 순환신경망 + 행렬 연산으로 연산의 부담이 크고 순차적으로 연산할 수 밖에 없는 제약 존재

### 5.2 주요 구성 요소
![]()

#### 인코더–디코더 구조

* **인코더**: 입력 문장을 벡터로 변환
* **디코더**: 인코더의 출력을 참조하여 출력 시퀀스 생성, N개의 인코더 블록을 거친 최종 출력은 디코더의 masked 멀티 헤드 어텐션의 입력으로 사용된다. 이후 N개의 디코더 블록을 통과, 소프트 맥수 함수 적용해 다음 토큰 예측

#### 위치 인코딩 (Positional Encoding)

입력 시퀀스의 순서 정보를 반영하기 위해 위치 인코딩 사용(순환신경망이 아니기 때문에)
* 인코딩/디코딩 모두 sequential 하게 받는 거싱 아닌 전체 문장을 한번에 신경망에 입력

#### 어텐션의 두가지 형태 : 셀프 어텐션, 크로스 어텐션
* 크로스 어텐션 : K = V, 하지만 쿼리는 다름 -> 디코더에서만 사용됨
* Self Attention : Q = K = V, 하나의 시퀀스에서 차용 = 문장 내 의미 관계를 파악함 -> 인코더 디코더 모두 사용됨
* 위 둘을 Scaled dot product attention 으로 통합함 

#### 스케일드 닷 프로덕트 어텐션(연산 방법)

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

#### 멀티-헤드 어텐션(구조)

여러 개의 어텐션 헤드를 병렬로 수행
$$\text{MultiHead}(Q,K,V) = \text{Concat}(head_1, ..., head_h)W^O$$
* 입력 벡터를 여러개의 헤드로 나누고 독립적으로 어텐션 적용(Scaled dot product) 후 Concat

#### 마스킹

디코더에서 미래 시점 단어를 참조하지 않도록 마스크 행렬 적용. 디코더의 셀프 어텐션 부분에 사용함
* 마스킹 행렬(하삼각 행렬)을 적용하여 현재 시점 이후의 정보에 대한 가중치를 0으로 설정하므로 디코더는 자연스럽게 순차적으로 작동한다
* 즉 디코더 또한 전체 시퀀스를 한번에 받지만 마스킹으로 순차적으로 작동하게 됨

#### 스킵 연결 및 층 정규화

* 경사소실 문제 해결을 위해 멀티헤드 어텐션을 거치지 않고 이전 입력 값을 바로 넘겨 경사를 전달
* 각 서브레이어 후 Layer Normalization 적용


#### 피드포워드 네트워크 (FFN)

$$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

* 인코더 디코더에서 멀티 헤드 어텐션을 적용하면 입력 토큰들 간 고나계를 계되어 중요 정보를 추출했을 거임
* 이후 Add & Norm으로 정규화 한 다음 Relu를 적용한 FFn으로 개별 토큰 강화(전체 표현력 증가)

#### 디코더 블록에서 Cross Attention
![]()
* 사진속 가운데 있는 애가 크로스 어텐션을 적용(아래 쪽은 셀프 어텐션)
* 인코더 정보와 디코더 정보를 섞기 위한 어텐션, Q와 K,V의 참조 위치가 다름
    * Q : Masked multi head attention의 출력
    * K,V : 인코더 출력에서 나옴

* 즉 Masekd multi head attention에서 나온 hidden state를 쿼리로, 인코더의 출력을 Key, Value로 사용해서 인코더의 출력 중 어느부분이 디코더의 현재 출력에 중요한지 판단한다

### 5.3 트랜스포머의 훈련과 추론
* 훈련 : 한번에 전체 시퀀스 입력받고 정답이 존재함. 이를 디코더 입력값으로 설정함. 이때 미래 정보 참조 X이므로 마스킹 처리하여 정답과 출력값 비교해 손실 계산
* 추론
    * 하나씩 순차적으로 입력받음
    * 이전 출력값 = 입력값으로
    * 마스킹은 없음


---

## 6. 문맥적 표현: ELMo 

### 6.1 개요

기존 Word2Vec, GloVe는 문맥에 따른 의미 변화를 반영하지 못함
ELMo는 언어 모델 기반의 **문맥적 임베딩(Contextual Embedding)** 방식 제안
* 순방향과 역방향으로 RNN model 따로 학습 시킴

### 6.2 구조

**양방향 언어 모델(BiLM)** 기반

* 순방향 RNN: $P(w_t | w_{1:t-1})$
* 역방향 RNN: $P(w_t | w_{t+1:T})$
  두 방향의 출력 벡터를 결합하여 단어 표현 생성

### 6.3 특징

Feature-based 접근
사전 학습된 모델의 파라미터를 고정(freeze)하고, 태스크별 비율 가중치로 조합
완전한 양방향 구조가 아니라는 한계 존재

---

## 7. BERT의 등장 배경

### 7.1 기존 언어 모델의 한계

| 모델   | 방향성    | 한계              |
| :--- | :----- | :-------------- |
| GPT  | 단방향    | 문맥의 일부만 활용      |
| ELMo | 얕은 양방향 | 완전한 상호의존성 학습 불가 |

BERT는 트랜스포머 인코더 구조를 기반으로 **진정한 양방향 문맥 학습**을 실현

### 7.2 마스크 언어 모델 (Masked Language Model, MLM)

입력 토큰 중 15%를 `[MASK]`로 가리고 해당 토큰 예측
**80/10/10 규칙**으로 학습–추론 불일치 완화

