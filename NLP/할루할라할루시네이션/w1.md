# 딥러닝을 이용한 자연어 처리 입문

# ch.2 텍스트 전처리


>### 텍스트 전처리: 토큰화(Tokenization)

텍스트 전처리는 원시(raw) 데이터를 모델이 이해할 수 있는 형태로 바꾸는 과정. 그중 **토큰화**는 문장을 의미 있는 작은 단위인 토큰(Token)으로 나누는 작업.

토큰의 기준은 상황에 따라 다양. 때로는 단어가 토큰, 때로는 문장이 토큰, 심지어 글자 하나하나가 토큰이 되기도 함.

### 1\. 단어 토큰화 (Word Tokenization)

단어 토큰화는 문장을 띄어쓰기나 구두점 등을 기준으로 단어 단위로 나누는 가장 일반적인 방식.

예를 들어, "Time is an illusion. Lunchtime double so\!"라는 문장을 토큰화하면 "Time", "is", "an", "illusion", "Lunchtime", "double", "so"와 같은 단어들.

하지만 단순히 띄어쓰기나 구두점을 기준으로 나누는 것은 완벽하지 않음. 다음과 같은 예외 상황들을 고려해야 함.

  * **구두점**: `Mr.`, `Ph.D.`, `$45.55`처럼 단어 중간에 구두점이 있는 경우.
  * **줄임말**: `Don't` (Do not), `I'm` (I am)처럼 축약된 단어.
  * **복합 명사**: `New York`, `rock 'n' roll`처럼 띄어쓰기가 포함된 하나의 단어.

이러한 문제 해결을 위해 **NLTK**, **Keras** 등 다양한 라이브러리가 여러 방식의 토큰화 도구를 제공.

#### 💡 NLTK, Keras를 이용한 단어 토큰화 예제

```python
import nltk
from nltk.tokenize import word_tokenize, WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence

text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."

# NLTK의 word_tokenize: '와 구두점을 단어와 분리
print('NLTK word_tokenize:', word_tokenize(text))

# NLTK의 WordPunctTokenizer: 모든 구두점을 분리
print('NLTK WordPunctTokenizer:', WordPunctTokenizer().tokenize(text))

# Keras의 text_to_word_sequence: 소문자화하고 구두점 제거 (단, '는 유지)
print('Keras text_to_word_sequence:', text_to_word_sequence(text))
```

-----

### 2\. 문장 토큰화 (Sentence Tokenization)

문장 토큰화는 텍스트를 문장 단위로 나누는 작업. `.`(마침표), `?`(물음표), `!`(느낌표)를 기준으로 문장을 나눌 수 있음. 하지만 `Ph.D.`와 같이 단어 중간에 마침표가 있는 경우를 고려해야 함.

#### 💡 NLTK, KSS를 이용한 문장 토큰화 예제

```python
import nltk
from nltk.tokenize import sent_tokenize
import kss

# NLTK를 사용하기 전에 다운로드가 필요할 수 있습니다.
# nltk.download('punkt')

# 영어 문장 토큰화 (NLTK)
text_en = "I am actively looking for Ph.D. students. And you are a Ph.D. student."
print('영어 문장 토큰화 (NLTK):', sent_tokenize(text_en))

# 한국어 문장 토큰화 (KSS)
# KSS는 한국어에 특화된 문장 분리 도구입니다.
text_ko = "딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?"
print('한국어 문장 토큰화 (KSS):', kss.split_sentences(text_ko))
```

-----

### 3\. 한국어 토큰화의 어려움

한국어는 영어와는 달리 **교착어**라는 특징. 조사나 어미가 단어 뒤에 붙기 때문에, 띄어쓰기만으로는 의미 있는 단어(토큰)를 나누기 어려움.

예를 들어, '책을 읽었다'라는 문장에서 `책을`은 '책' + '을'로 분리되어야 함. '책'은 의미를 가진 독립적인 단어인 **자립 형태소**이고, '을'은 다른 형태소에 붙어서 사용되는 **의존 형태소**(조사).

이러한 이유로 한국어는 단순히 띄어쓰기만으로 토큰화를 하는 것보다 **형태소(morpheme)** 단위로 분리하는 형태소 분석(Morpheme Analysis)의 사용이 일반적.

#### 💡 KoNLPy를 이용한 한국어 형태소 분석 예제

```python
from konlpy.tag import Okt, Kkma

text = "열심히 코딩한 당신, 연휴에는 여행을 가봐요"

# Okt(Open Korea Text) 형태소 분석기 사용
okt = Okt()
print('Okt 형태소 추출:', okt.morphs(text))
print('Okt 품사 태깅:', okt.pos(text))
print('Okt 명사 추출:', okt.nouns(text))

print("-" * 30)

# Kkma(꼬꼬마) 형태소 분석기 사용
kkma = Kkma()
print('Kkma 형태소 추출:', kkma.morphs(text))
print('Kkma 품사 태깅:', kkma.pos(text))
print('Kkma 명사 추출:', kkma.nouns(text))
```

위 코드를 실행하면, 두 분석기의 결과가 약간씩 다름. 각 형태소 분석기는 고유한 규칙과 알고리즘을 가지고 있기 때문에, 프로젝트의 목적에 가장 적합한 것을 선택하는 것이 중요. 보통 속도가 중요한 경우 Mecab을, 정확도가 중요한 경우 다른 분석기를 비교하여 사용.

-----

### 4\. 품사 태깅 (Part-of-speech tagging)

**품사 태깅**은 토큰화된 단어에 품사를 붙여주는 작업. 단어의 의미는 품사에 따라 달라질 수 있기 때문에, 품사 정보는 자연어 처리에서 매우 유용한 정보. 예를 들어 영어 단어 'fly'는 동사('날다') 또는 명사('파리')로 사용될 수 있음.

#### 💡 NLTK를 이용한 영어 품사 태깅 예제

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

# NLTK를 사용하기 전에 다운로드가 필요할 수 있습니다.
# nltk.download('averaged_perceptron_tagger')

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('품사 태깅:', pos_tag(tokenized_sentence))
```

이 예제에서는 NLTK의 **Penn Treebank POS Tags** 기준에 따라 `I`는 `PRP`(인칭 대명사), `am`은 `VBP`(동사) 등으로 품사가 태깅.

---
>### 텍스트 전처리: 정제(Cleaning)와 정규화(Normalization)


토큰화 전후로 이루어지는 **정제**와 **정규화**. 이 두 작업은 텍스트 데이터를 용도에 맞게 다듬는 필수 과정.

  * **정제**: 불필요한 노이즈 데이터의 제거.
  * **정규화**: 표기 방식이 다른 단어들을 하나의 형태로 통일.

정제는 토큰화에 방해가 되는 요소를 사전에 제거하거나, 토큰화 이후에도 남아있는 노이즈를 없애는 지속적인 작업. 완벽한 정제는 어려워 대부분은 적절한 합의점의 선택.

-----

### 1\. 규칙 기반 단어 통합

직접 규칙을 정해 같은 의미를 가진 다른 표기의 단어를 통합하는 방법. 예를 들어 **USA**와 **US**를 같은 단어로 취급하거나, **uh-huh**와 **uhhuh**를 동일하게 처리. 이러한 정규화는 검색 효율을 높이는 효과. 뒤에서 다룰 어간 추출(Stemming)과 표제어 추출(Lemmatization)이 이와 유사한 작업.

-----

### 2\. 대소문자 통합

영어나 다른 서구권 언어에서 대소문자 통합은 단어의 다양성을 줄이는 효과적인 정규화 방법. 일반적으로는 모든 대문자를 소문자로 변환. 예를 들어 문장 첫머리의 **Automobile**을 **automobile**로 바꾸면, `automobile` 검색 시 `Automobile`도 함께 찾을 수 있음.

하지만 무조건적인 통합은 때때로 문제가 될 수 있음. 예를 들어, 미국을 의미하는 **US**와 우리를 뜻하는 **us**는 구분이 필요. 회사명이나 사람 이름처럼 고유명사는 대문자로 유지하는 것이 바람직. 이러한 예외를 고려하는 방법도 있지만, 대부분의 경우 모든 텍스트를 소문자로 변환하는 것이 실용적인 해결책.

-----

### 3\. 불필요한 단어 제거

정제 작업은 의미 없는 특수 문자뿐만 아니라, 분석 목적에 맞지 않는 단어들을 노이즈로 간주하고 제거하는 작업. **불용어(Stopword)** 제거, 등장 빈도가 적은 단어 제거, 길이가 짧은 단어 제거가 대표적인 방법.

  * **등장 빈도가 적은 단어**: 전체 코퍼스에서 너무 적게 등장하는 단어는 모델 학습에 거의 도움이 되지 않으므로 제거.
  * **길이가 짧은 단어**: 영어의 경우, 길이가 2\~3 이하인 단어는 대부분 불용어에 해당. 예를 들어 `a`, `I`, `it`, `at` 등. 이러한 단어를 제거하는 것은 불용어 제거에 효과적. 하지만 한글은 한 글자만으로도 의미를 가지는 경우가 많아 (예: 용(龍), 귤), 무조건적인 길이 기반 삭제는 의미 있는 단어 손실의 위험.

#### 💡 길이가 짧은 단어 제거 예제

정규 표현식(`re`)을 이용해 길이가 짧은 단어를 삭제하는 방법.

```python
import re

text = "I was wondering if anyone out there could enlighten me on this car."

# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
```

위 코드를 실행하면, 'I', 'on', 'if' 등의 짧은 단어가 제거된 결과.

-----

### 4\. 정규 표현식 (Regular Expression)

**정규 표현식**은 텍스트 내에서 특정 패턴을 가진 문자열을 찾고 처리하는 강력한 도구. HTML 태그나 불필요한 특수 문자 등 규칙적으로 나타나는 노이즈 데이터를 한 번에 제거하는 데 매우 유용. 위 예제에서도 길이가 짧은 단어를 제거하는 데 정규 표현식의 활용. 앞으로의 전처리 과정에서 자주 사용될 핵심 도구.

---


>### 텍스트 전처리: 어간 추출(Stemming)과 표제어 추출(Lemmatization)

**표제어 추출**과 **어간 추출**은 정규화의 핵심 기법. 겉으로 다른 단어들을 하나의 단어로 통합해 문서의 단어 수를 줄이는 것이 목적. 주로 단어 빈도 기반의 NLP 모델에서 활용.

-----

### 1\. 표제어 추출 (Lemmatization)

단어의 기본 사전형인 표제어(Lemma)를 찾는 과정. **am, are, is**의 표제어는 **be**가 되는 것처럼, 단어의 뿌리 형태를 찾아감.

  * **어간**은 단어의 핵심 의미를 담은 부분.
  * **접사**는 추가적인 의미를 부여하는 부분.
  * NLTK의 **WordNetLemmatizer** 사용. 정확한 표제어 추출을 위해 단어의 품사 정보(동사, 명사 등)를 함께 입력하는 것이 중요.

#### 💡 표제어 추출 예제

```python
import nltk
from nltk.stem import WordNetLemmatizer

# WordNet 다운로드 필요
# nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('표제어 추출 전:', words)
print('표제어 추출 후:', [lemmatizer.lemmatize(word) for word in words])
```

위 코드는 `lives`가 `life`로, `dies`가 `dy`로 변환되는 등 일부는 제대로, 일부는 잘못 변환. 품사 정보를 추가하면 정확도 향상.

```python
# 품사 정보(동사 'v') 추가하여 표제어 추출
print(lemmatizer.lemmatize('dies', 'v'))
print(lemmatizer.lemmatize('watched', 'v'))
print(lemmatizer.lemmatize('has', 'v'))
```

-----

### 2\. 어간 추출 (Stemming)

형태론적 분석을 단순화한 방식. 접미사를 자르는 규칙 기반의 작업. 결과 단어가 사전에 없는 경우가 많음.

  * NLTK의 **PorterStemmer** 사용.
  * 예: **organization**의 어간은 **organ**으로 추출되는데, 이는 의미가 달라져 과하게 일반화되는 문제.

#### 💡 어간 추출 예제

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
tokenized_sentence = word_tokenize(sentence)

print('어간 추출 전:', tokenized_sentence)
print('어간 추출 후:', [stemmer.stem(word) for word in tokenized_sentence])
```

-----

### 3\. 표제어 추출 vs. 어간 추출

두 방식의 가장 큰 차이점은 결과의 정확성.

  * **표제어 추출**: 단어의 품사를 고려해 사전에 있는 단어를 결과로 도출하려는 정밀한 작업.
  * **어간 추출**: 단순 규칙에 따라 접미사를 제거하는 작업. 결과 단어가 사전에 없을 수 있고, 의미가 변질될 수 있음. 일반적으로 어간 추출이 더 빠름.

-----

### 4\. 한국어에서의 어간 추출

한국어는 동사와 형용사 같은 **용언**이 **어간**과 **어미**의 결합으로 이루어짐.

  * **어간**: 용언의 의미를 담고 변하지 않는 부분 (일부 불규칙 변화).
  * **어미**: 어간 뒤에 붙어 활용하며 변하는 부분.

**규칙 활용**은 어간이 일정하므로 어미만 분리하면 어간 추출이 쉬움. 하지만 **불규칙 활용**은 어간의 형태가 변하기 때문에 더 복잡한 규칙이 필요.

---

>### 텍스트 전처리: 불용어(Stopword)

*불용어(Stopword)는 자주 등장하지만 실제 의미 분석에 큰 도움이 되지 않는 단어들. 예를 들어 영어의 'I', 'me', 'my'나 한국어의 조사, 접속사 같은 단어. NLP 모델의 효율을 높이고 노이즈를 줄이기 위해 제거하는 대상.


### 1\. NLTK를 이용한 불용어 제거

NLTK는 영어 불용어 리스트를 기본으로 제공. `stopwords.words('english')`로 이 리스트를 불러와 텍스트에서 불용어를 제거할 수 있음.

#### 💡 NLTK 불용어 제거 예제

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# NLTK 불용어 리스트 확인
stop_words_list = stopwords.words('english')
print('NLTK 불용어 개수:', len(stop_words_list))
print('NLTK 불용어 10개:', stop_words_list[:10])

# 불용어 제거 실행
example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english'))
word_tokens = word_tokenize(example)

result = [word for word in word_tokens if word not in stop_words]
print('불용어 제거 전:', word_tokens)
print('불용어 제거 후:', result)
```

-----

### 2\. 한국어 불용어 제거

한국어는 NLTK처럼 보편적인 불용어 리스트가 없어, 분석 목적에 맞게 직접 불용어 사전을 만들어야 함. 토큰화 후, 직접 정의한 불용어 리스트에 포함된 단어를 제거.

#### 💡 한국어 불용어 제거 예제

```python
from konlpy.tag import Okt

okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는" # 직접 정의한 불용어
stop_words = set(stop_words.split(' '))

word_tokens = okt.morphs(example)
result = [word for word in word_tokens if word not in stop_words]

print('불용어 제거 전:', word_tokens)
print('불용어 제거 후:', result)
```

불용어 리스트는 텍스트 파일이나 CSV 파일로 관리하기도 함.

---

>### 텍스트 전처리: 정수 인코딩(Integer Encoding)

**정수 인코딩**은 자연어 처리 모델이 텍스트를 숫자로 이해하도록 단어들을 고유한 정수로 바꾸는 작업. 일반적으로 단어의 빈도수를 기준으로 높은 빈도의 단어에 낮은 정수(인덱스)를 부여.

-----

### 1\. Dictionary를 이용한 정수 인코딩

파이썬 딕셔너리를 사용해 단어의 빈도수를 계산하고, 이를 기반으로 정수 인덱스를 부여할 수 있음. 이 과정에서 빈도수가 낮은 단어는 제거하여 데이터의 복잡성을 줄임. 또한, 단어 집합에 없는 단어(Out-Of-Vocabulary, **OOV**)는 'OOV' 토큰으로 대체.

#### 💡 예제 코드

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import re

raw_text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

# 텍스트 전처리 및 단어 빈도수 계산
sentences = sent_tokenize(raw_text)
preprocessed_sentences = []
vocab = {}
stop_words = set(stopwords.words('english'))

for sentence in sentences:
    tokenized_sentence = word_tokenize(sentence)
    result = []
    for word in tokenized_sentence:
        word = word.lower()
        if word not in stop_words and len(word) > 2:
            result.append(word)
            if word not in vocab:
                vocab[word] = 0
            vocab[word] += 1
    preprocessed_sentences.append(result)

# 빈도수 순으로 정렬 및 정수 인덱스 부여 (빈도수 1인 단어 제외)
vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)
word_to_index = {}
i = 0
for (word, frequency) in vocab_sorted:
    if frequency > 1:
        i += 1
        word_to_index[word] = i

# OOV(단어 집합에 없는 단어) 처리
word_to_index['OOV'] = len(word_to_index) + 1

# 문장 정수 인코딩
encoded_sentences = []
for sentence in preprocessed_sentences:
    encoded_sentence = []
    for word in sentence:
        try:
            encoded_sentence.append(word_to_index[word])
        except KeyError:
            encoded_sentence.append(word_to_index['OOV'])
    encoded_sentences.append(encoded_sentence)

print("단어-정수 맵핑:", word_to_index)
print("인코딩된 문장:", encoded_sentences)
```

-----

### 2\. 케라스(Keras) Tokenizer를 이용한 정수 인코딩

케라스의 **Tokenizer**는 정수 인코딩을 더 편리하게 수행하는 도구. `fit_on_texts`로 단어 집합을 생성하고, `texts_to_sequences`로 텍스트를 정수 시퀀스로 변환.

#### 💡 예제 코드

```python
from tensorflow.keras.preprocessing.text import Tokenizer

# 전처리된 문장 데이터
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

# Tokenizer 객체 생성
tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)

# 단어-정수 맵핑 확인
print('단어-정수 맵핑:', tokenizer.word_index)

# 텍스트를 정수 시퀀스로 변환
print('정수 인코딩 결과:', tokenizer.texts_to_sequences(preprocessed_sentences))
```

**Tokenizer**에 `num_words` 인자를 사용하여 사용할 단어의 개수를 제한할 수 있음. 또한, `oov_token` 인자를 사용해 단어 집합에 없는 단어를 특정 토큰으로 대체할 수 있음.

-----
>### 텍스트 전처리: 패딩(Padding)

**패딩**은 길이가 다른 문장들을 동일한 길이로 맞추는 작업. 딥러닝 모델은 입력 데이터의 길이가 같아야 하기 때문에, 병렬 처리를 위해 필수적인 전처리 단계. 일반적으로 길이가 짧은 문장의 뒤나 앞에 0을 채워 넣는 **제로 패딩**을 사용.

-----

### 1\. NumPy를 이용한 패딩

가장 긴 문장의 길이를 기준으로, 짧은 문장에 0을 추가하여 길이를 맞추는 방법.

#### 💡 예제 코드

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

# 정수 인코딩된 문장 데이터
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)

# 가장 긴 문장의 길이 확인
max_len = max(len(item) for item in encoded)
print('최대 길이:', max_len)

# 짧은 문장 뒤에 0을 추가하여 패딩
for sentence in encoded:
    while len(sentence) < max_len:
        sentence.append(0)

padded_np = np.array(encoded)
print('패딩 결과:\n', padded_np)
```

-----

### 2\. 케라스(Keras) `pad_sequences` 이용

케라스는 패딩을 위한 전용 함수 `pad_sequences`를 제공. 이 함수는 훨씬 간단하고 유연한 패딩 방법을 제공.

  * `padding='pre'` (기본값)는 문장 앞에 0을 채움.
  * `padding='post'`는 문장 뒤에 0을 채움.
  * `maxlen` 인자를 사용해 패딩할 최대 길이를 직접 지정 가능. 이보다 긴 문장은 잘림.
  * `truncating='pre'` (기본값)는 문장 앞에서, `truncating='post'`는 문장 뒤에서 자름.
  * `value` 인자를 사용해 0이 아닌 다른 숫자로 패딩할 수 있음.

#### 💡 예제 코드

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 정수 인코딩된 문장 데이터
encoded = [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

# 뒤에 0을 채우는 패딩
padded_post = pad_sequences(encoded, padding='post')
print('패딩(padding=\'post\') 결과:\n', padded_post)

# 최대 길이 5로 제한하고 뒤에 0을 채우는 패딩
padded_maxlen = pad_sequences(encoded, padding='post', maxlen=5)
print('패딩(maxlen=5, padding=\'post\') 결과:\n', padded_maxlen)

# 최대 길이 5로 제한하고 뒤를 자르는(truncating) 패딩
padded_trunc = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)
print('패딩(maxlen=5, truncating=\'post\') 결과:\n', padded_trunc)
```

---

>### 텍스트 전처리: 데이터 분리(Splitting Data)

쉬워서 pass

---

>### 텍스트 전처리: 한국어 전처리 패키지

한국어는 띄어쓰기, 맞춤법, 신조어 등 전처리 과정에 어려움이 있음. 이를 위해 다양한 한국어 전용 패키지를 사용.

-----

### 1\. PyKoSpacing: 띄어쓰기 교정

**PyKoSpacing**은 딥러닝 기반으로 띄어쓰기가 없는 문장을 교정하는 패키지. 대용량 코퍼스를 학습하여 뛰어난 성능을 보임.

#### 💡 예제 코드

```python
# 설치: pip install git+https://github.com/haven-jeon/PyKoSpacing.git

from pykospacing import Spacing

sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'

new_sent = sent.replace(" ", '') # 띄어쓰기 제거
print('띄어쓰기 없는 문장:', new_sent)

spacing = Spacing()
kospacing_sent = spacing(new_sent)

print('PyKoSpacing으로 교정된 문장:', kospacing_sent)
print('원본 문장:', sent)
```

-----

### 2\. Py-Hanspell: 맞춤법 교정

**Py-Hanspell**은 네이버 맞춤법 검사기를 기반으로 맞춤법과 띄어쓰기를 함께 교정.

#### 💡 예제 코드

```python
# 설치: pip install git+https://github.com/ssut/py-hanspell.git

from hanspell import spell_checker

sent = "맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 "
spelled_sent = spell_checker.check(sent)

print('교정된 문장:', spelled_sent.checked)
```

-----

### 3\. SOYNLP: 비지도 학습 단어 토큰화

**SOYNLP**는 **응집 확률**과 **브랜칭 엔트로피**를 이용한 비지도 학습 기반 토크나이저. 기존 형태소 분석기가 처리하지 못하는 신조어나 미등록 단어 문제에 강점.

  * **응집 확률(Cohesion Probability)**: 한 단어 내 문자열이 얼마나 응집하여 자주 나타나는지를 측정하는 척도.
  * **브랜칭 엔트로피(Branching Entropy)**: 주어진 문자열 뒤에 올 수 있는 다음 문자의 다양성을 측정하는 척도. 단어가 끝나는 지점에서 이 값이 증가하는 경향.

#### 💡 SOYNLP 토크나이저 예제

```python
# 설치: pip install soynlp

import urllib.request
from soynlp.word import WordExtractor
from soynlp.tokenizer import MaxScoreTokenizer

# 학습을 위한 데이터 다운로드
# urllib.request.urlretrieve("https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt", filename="2016-10-20.txt")

# WordExtractor로 학습
word_extractor = WordExtractor()
word_extractor.train("2016-10-20.txt")
word_score_table = word_extractor.extract()

# 최대 점수 토크나이저 사용
maxscore_tokenizer = MaxScoreTokenizer(scores=word_score_table)
print('SOYNLP 토큰화 결과:', maxscore_tokenizer.tokenize("국제사회와우리의노력들로범죄를척결하자"))

# SOYNLP를 이용한 반복 문자 정규화
from soynlp.normalizer import emoticon_normalize
print('반복 이모티콘 정규화:', emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))
```

-----

### 4\. Customized KoNLPy: 사용자 사전 추가

기존 형태소 분석기가 고유명사 등을 잘못 분리할 때, 사용자 사전을 추가해 올바른 토큰을 얻는 방법. **Customized KoNLPy**는 이 과정을 쉽게 만들어주는 패키지.

#### 💡 사용자 사전 추가 예제

```python
# 설치: pip install customized_konlpy

from ckonlpy.tag import Twitter

twitter = Twitter()
print('사전 추가 전:', twitter.morphs('은경이는 사무실로 갔습니다.'))

# '은경이'를 하나의 명사로 사용자 사전 추가
twitter.add_dictionary('은경이', 'Noun')
print('사전 추가 후:', twitter.morphs('은경이는 사무실로 갔습니다.'))
```



# ch.7 딥러닝 개요

>### 딥러닝 개요: 퍼셉트론(Perceptron)


**퍼셉트론**은 초기 형태의 인공 신경망. 뇌의 뉴런을 모방해 여러 개의 입력 신호를 받아 하나의 출력 신호를 내보내는 알고리즘. 각 입력값에는 중요도를 나타내는 가중치(Weight)가 곱해지고, 이 합이 임계치(Threshold)를 넘으면 1, 아니면 0을 출력.

-----

### 단층 퍼셉트론(Single-Layer Perceptron)

입력층과 출력층, 단 두 개의 층으로 구성. 이 모델은 **AND**, **NAND**, **OR** 게이트와 같은 선형 분리 가능한 문제만 해결할 수 있음. 하지만 XOR 게이트처럼 하나의 직선으로 나눌 수 없는 문제는 해결 불가.

#### 💡 파이썬으로 구현한 AND 게이트

```python
def AND_gate(x1, x2):
    w1 = 0.5
    w2 = 0.5
    b = -0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1

print(AND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1))
```

-----

### 다층 퍼셉트론(MultiLayer Perceptron, MLP)

단층 퍼셉트론의 한계를 극복하기 위해 중간에은닉층(Hidden layer)을 추가한 구조. 은닉층을 여러 개 쌓아 복잡한 비선형 문제도 해결할 수 있음. 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 함.

딥러닝은 이러한 심층 신경망을 손실 함수(Loss function)\와 옵티마이저(Optimizer)를 사용해 최적의 가중치를 자동으로 찾아 학습시키는 과정.

---


>### 딥러닝 개요: 인공 신경망(Artificial Neural Network)

**인공 신경망**은 딥러닝의 기본 구조. 여러 개의 층(layer)으로 구성. 각 층은 여러 개의 뉴런으로 이루어져 있으며, 각 뉴런은 입력값을 받아 가중치를 곱하고, **활성화 함수**를 거쳐 다음 층으로 값을 전달.

-----

### 1\. 피드 포워드 신경망(FFNN)

**피드 포워드 신경망**은 입력층에서 출력층으로 오직 한 방향으로만 연산이 진행되는 신경망 구조. 정보가 순차적으로 흐르는 형태.

-----

### 2\. 전결합층(Fully-connected layer)

**전결합층**은 한 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결된 형태. 케라스에서는 **`Dense`** 층으로 구현.

-----

### 3\. 활성화 함수(Activation Function)

활성화 함수는 뉴런의 출력값을 결정하는 비선형 함수. 인공 신경망의 은닉층은 비선형성을 가져야만 층을 깊게 쌓아 복잡한 문제를 풀 수 있음.

  * **시그모이드(Sigmoid)**: 출력값이 0과 1 사이. 이진 분류의 출력층에 주로 사용. 하지만 미분값이 0에 가까워지면 학습이 멈추는 **기울기 소실(Vanishing Gradient)** 문제 발생.
  * **하이퍼볼릭탄젠트(tanh)**: 출력값이 -1과 1 사이. 시그모이드보다 기울기 소실 문제가 덜해 은닉층에 시그모이드보다 선호.
  * **렐루(ReLU)**: 입력이 양수면 그대로, 음수면 0을 출력. 계산이 빠르고 기울기 소실 문제도 적어 은닉층에서 가장 인기. 하지만 입력이 음수일 때 기울기가 0이 되어 뉴런이 죽는 **죽은 렐루(dying ReLU)** 문제가 있음.
  * **리키 렐루(Leaky ReLU)**: 렐루의 변형으로, 음수 입력에 대해 아주 작은 양수 값을 반환해 죽은 렐루 문제 보완.
  * **소프트맥스(Softmax)**: 출력값이 확률 분포를 형성. 다중 클래스 분류의 출력층에 주로 사용.


---

>### 딥러닝 학습 방법

딥러닝의 학습은 **손실 함수**의 값을 최소화하는 방향으로 모델의 매개변수를 조정하는 과정. 이 과정은 **옵티마이저**, **배치 크기**, **에포크** 등의 개념을 통해 이루어짐.

---

### 1. 손실 함수(Loss function)

**손실 함수**는 모델의 예측값과 실제값의 차이(오차)를 수치화하는 함수. 오차가 클수록 손실 함수의 값도 커짐.

* **평균 제곱 오차(MSE)**: 회귀 모델에 주로 사용.
* **이진 교차 엔트로피(Binary Cross-Entropy)**: 이진 분류 모델의 출력층에 사용.
* **범주형 교차 엔트로피(Categorical Cross-Entropy)**: 다중 클래스 분류 모델에 사용. 원-핫 인코딩된 레이블에 적용.
* **희소 범주형 교차 엔트로피(Sparse Categorical Cross-Entropy)**: 범주형 교차 엔트로피와 동일하나, 정수형 레이블에 직접 적용.

---

### 2. 경사 하강법과 배치 크기

**경사 하강법**은 손실 함수 값을 줄여나가며 최적의 매개변수를 찾는 방법. 이때 가중치 업데이트에 사용하는 데이터의 양을 **배치(Batch)**라고 함.

* **배치 경사 하강법(Batch Gradient Descent)**: 전체 데이터를 한 번에 사용해 매개변수 업데이트. 계산 시간이 오래 걸리고 메모리 사용량이 많음.
* **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**: 한 번에 1개의 데이터만 사용해 업데이트. 계산이 빠르지만 불안정.
* **미니 배치 경사 하강법(Mini-Batch Gradient Descent)**: 정해진 수의 데이터(보통 128, 256 등)를 사용해 업데이트. 배치 경사 하강법보다 빠르고 SGD보다 안정적이라 가장 많이 사용.

---

### 3. 옵티마이저(Optimizer)

**옵티마이저**는 손실 함수를 최소화하는 경사 하강법의 구체적인 알고리즘.

* **모멘텀(Momentum)**: 경사 하강법에 관성을 더해 최적화 속도를 높임.
* **아다그라드(Adagrad)**: 각 매개변수에 다른 학습률을 적용. 변화가 많은 매개변수는 학습률이 작게, 적은 매개변수는 크게 설정.
* **아담(Adam)**: 모멘텀과 아다그라드의 장점을 결합한 방식. 현재 가장 널리 사용되는 옵티마이저.

---

### 4. 에포크, 배치 크기, 이터레이션

이 세 가지 개념은 모델 학습 방식을 이해하는 데 필수.

* **에포크(Epoch)**: 전체 데이터셋을 한 번 학습하는 횟수. 전체 문제지를 한 번 다 푼 것.
* **배치 크기(Batch size)**: 한 번의 가중치 업데이트에 사용하는 데이터 샘플의 수. 한 번에 푸는 문제의 개수.
* **이터레이션(Iteration)**: 한 에포크를 완료하는 데 필요한 배치 수. `(전체 데이터 수 / 배치 크기)`. 가중치 업데이트 횟수.

---

>### 딥러닝 학습: 역전파(Backpropagation)

**역전파**는 딥러닝 모델의 가중치를 업데이트하는 핵심 알고리즘. 순전파를 통해 얻은 예측값과 실제값의 오차를 계산한 뒤, 이 오차를 바탕으로 미분(기울기)을 구해 출력층에서 입력층 방향으로 거꾸로 전달하며 가중치를 업데이트.

---

### 1. 순전파(Forward Propagation)

**순전파**는 입력 데이터가 신경망의 입력층에서 출력층으로 순차적으로 계산되는 과정. 각 층의 뉴런은 이전 층의 출력값에 가중치를 곱하고, 이 값들을 합산해 활성화 함수를 통과시키는 과정을 반복. 최종적으로 출력층에서 모델의 **예측값**이 나옴.

---

### 2. 역전파 과정

1.  **오차 계산**: 예측값과 실제값의 차이를 **손실 함수**를 이용해 계산.
2.  **기울기 계산**: 손실 함수를 미분하여 각 가중치에 대한 기울기를 구함. 이 기울기는 가중치를 얼마나, 어느 방향으로 업데이트할지 결정.
3.  **가중치 업데이트**: 계산된 기울기를 바탕으로 **경사 하강법**을 적용해 가중치를 수정. 이 과정은 출력층에서 시작해 입력층 방향으로 거꾸로 진행.

역전파의 핵심은 **미분의 연쇄 법칙(Chain rule)**. 복잡한 신경망의 각 가중치에 대한 기울기를 효율적으로 계산할 수 있게 함.

#### 💡 역전파를 통한 가중치 업데이트 예시

예시로 주어진 신경망에서 가중치 $W_3$를 업데이트하는 과정은 다음과 같음:

1.  **오차 계산**: 전체 오차 $E_{total}$를 MSE 함수로 계산.
    $E_{total} = \frac{1}{2}\sum (target - output)^2$
2.  **기울기 계산**: $\frac{\partial E_{total}}{\partial W_3}$를 미분의 연쇄 법칙으로 분해하여 계산.
    $\frac{\partial E_{total}}{\partial W_3} = \frac{\partial E_{total}}{\partial output_1} \times \frac{\partial output_1}{\partial net_1} \times \frac{\partial net_1}{\partial W_3}$
    * $\frac{\partial E_{total}}{\partial output_1}$: 오차를 출력값으로 미분.
    * $\frac{\partial output_1}{\partial net_1}$: 활성화 함수(시그모이드)를 미분.
    * $\frac{\partial net_1}{\partial W_3}$: 가중치에 대한 입력값.

3.  **가중치 업데이트**: 학습률($\eta$)을 곱하여 가중치 $W_3$를 업데이트.
    $W_3^{new} = W_3^{old} - \eta \frac{\partial E_{total}}{\partial W_3}$

이 과정을 모든 가중치에 대해 반복하면 모델의 오차가 점차 감소. 이것이 딥러닝 학습의 기본 원리.

---

>### 딥러닝 개요: 과적합(Overfitting)을 막는 방법들

**과적합**은 모델이 훈련 데이터를 지나치게 암기해 새로운 데이터에 대한 성능이 떨어지는 현상. 모델이 훈련 데이터의 노이즈까지 학습했기 때문. 과적합을 막는 여러 방법이 있음.

-----

### 1\. 데이터 증강(Data Augmentation)

**데이터 증강**은 데이터의 양이 적을 때 기존 데이터를 변형하여 학습 데이터의 양을 인위적으로 늘리는 방법. 이미지의 경우 회전, 노이즈 추가 등으로, 텍스트의 경우 역번역 등을 사용.

-----

### 2\. 모델 복잡도 줄이기

모델의 복잡도는 은닉층의 수나 매개변수(가중치)의 수로 결정. 과적합이 발생하면 모델의 복잡도를 줄여 일반화 성능을 높이는 조치. 모델의 매개변수 수를 수용력(capacity)이라고도 함.

-----

### 3\. 가중치 규제(Regularization)

복잡한 모델을 간단하게 만드는 방법 중 하나. **가중치 규제**는 모델의 비용 함수에 가중치 값을 작게 만드는 항을 추가.

  * **L1 규제**: 가중치들의 절대값 합을 비용 함수에 추가. 일부 가중치를 0으로 만들어 특정 특성이 모델에 미치는 영향을 제거.
  * **L2 규제**: 가중치들의 제곱 합을 비용 함수에 추가. 가중치 값을 0에 가깝게 만들지만, 완전히 0으로 만들지는 않음. 일반적으로 L2 규제가 더 잘 작동.

-----

### 4\. 드롭아웃(Dropout)

**드롭아웃**은 학습 과정에서 신경망의 일부 뉴런을 랜덤으로 사용하지 않는 방법. 학습 시에만 적용하고 예측 시에는 사용하지 않음. 모델이 특정 뉴런에 의존하는 것을 방지하고, 여러 신경망을 앙상블한 효과를 내어 과적합을 방지.

#### 💡 케라스(Keras) 드롭아웃 예제

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense

max_words = 10000
num_classes = 46

model = Sequential()
model.add(Dense(256, input_shape=(max_words,), activation='relu'))
model.add(Dropout(0.5))  # 50%의 비율로 드롭아웃 추가
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # 50%의 비율로 드롭아웃 추가
model.add(Dense(num_classes, activation='softmax'))
```


---

>### 딥러닝 개요: 기울기 소실(Gradient Vanishing)과 폭주(Exploding)

딥러닝 모델 학습 시 역전파 과정에서 발생하는 문제.

* **기울기 소실(Gradient Vanishing)**: 입력층으로 갈수록 기울기(경사)가 점차 작아져 가중치 업데이트가 제대로 이루어지지 않는 현상. 주로 시그모이드나 tanh 같은 활성화 함수를 깊은 신경망에서 사용할 때 발생.
* **기울기 폭주(Gradient Exploding)**: 기울기가 비정상적으로 커져 가중치들이 발산하는 현상.

---

### 1. 활성화 함수 사용

기울기 소실을 완화하는 가장 간단한 방법은 은닉층에서 시그모이드나 tanh 대신 **ReLU**나 그 변형 함수들을 사용. ReLU는 양수 입력에 대해 기울기가 1이므로 기울기 소실 문제 해결에 효과적. **Leaky ReLU**는 음수 입력에 대해서도 작은 기울기를 반환하여 `죽은 렐루` 문제를 해결.

---

### 2. 그래디언트 클리핑(Gradient Clipping)

**그래디언트 클리핑**은 기울기 값이 특정 임계치를 넘지 않도록 자르는 기법. 기울기 폭주를 방지하는 데 유용하며, RNN에서 특히 효과적.

---

### 3. 가중치 초기화(Weight Initialization)

가중치 초기값 설정에 따라 모델의 학습 결과가 달라질 수 있음. 적절한 초기화는 기울기 소실/폭주 문제를 완화.

* **세이비어 초기화(Xavier Initialization)**: 시그모이드나 tanh 함수에 적합. 이전 층과 다음 층의 뉴런 수를 고려해 가중치를 초기화.
* **He 초기화(He Initialization)**: ReLU 계열 함수에 적합. 주로 이전 층의 뉴런 수를 고려해 초기화.

---

### 4. 배치 정규화(Batch Normalization)

**배치 정규화**는 각 층의 입력값을 정규화하여 학습을 안정시키고 빠르게 만드는 방법. **내부 공변량 변화(Internal Covariate Shift)**, 즉 학습 과정에서 층별 입력 데이터 분포가 달라지는 문제를 해결.

* **효과**: 기울기 소실 문제 개선, 가중치 초기화에 덜 민감해짐, 학습률을 높일 수 있어 학습 속도 향상, 과적합 방지.
* **한계**: 미니 배치 크기에 의존적이고, RNN에 적용하기 어려움.

---

### 5. 층 정규화(Layer Normalization)

배치 정규화의 한계를 극복하기 위해 등장. 배치 크기에 의존하지 않고, RNN에도 쉽게 적용할 수 있음.

---

>### 딥러닝 개요: 케라스(Keras)

**케라스**는 딥러닝 모델을 쉽게 만들 수 있는 파이썬 라이브러리. 사용이 직관적인 상위 레벨의 인터페이스를 제공.

---

### 1. 전처리 도구

* **`Tokenizer()`**: 텍스트를 토큰화하고 각 단어를 고유한 정수(인덱스)로 변환.
    * `fit_on_texts`: 단어 빈도수를 기반으로 단어 집합 생성.
    * `texts_to_sequences`: 텍스트를 정수 시퀀스로 변환.

* **`pad_sequences()`**: 길이가 다른 시퀀스(문장)들을 동일한 길이로 맞추는 패딩 작업 수행.

---

### 2. 워드 임베딩(Word Embedding)

**워드 임베딩**은 단어를 밀집 벡터(dense vector)로 표현하는 방법. 단어 간 의미적 유사도를 벡터 공간에 반영.

* **`Embedding()`**: 정수 인코딩된 단어를 밀집 벡터로 변환하는 층(layer). 모델의 첫 번째 층으로 사용.

| | 원-핫 벡터 | 임베딩 벡터 |
|---|---|---|
| 차원 | 고차원 (단어 집합 크기) | 저차원 (보통 256, 512 등) |
| 표현 | 0과 1로 이루어진 희소 벡터 | 실수값을 가진 밀집 벡터 |
| 생성 | 수동 | 학습을 통해 자동 생성 |

---

### 3. 모델링(Modeling)

케라스의 `Sequential()`은 딥러닝 모델의 층을 쌓는 기본적인 방법. `add()` 함수를 이용해 층을 하나씩 추가.

* **`Dense()`**: 모든 뉴런이 이전 층의 모든 뉴런과 연결된 **전결합층(Fully-connected layer)**.
    * 첫 번째 인자는 출력 뉴런 수.
    * `input_dim`: 입력 뉴런 수 (첫 층에만 지정).
    * `activation`: 사용할 활성화 함수. ('relu', 'sigmoid', 'softmax' 등)
    

---

### 4. 컴파일(Compile)과 훈련(Training)

* **`compile()`**: 모델 학습 전에 손실 함수, 옵티마이저, 평가 지표를 설정하는 단계.
    * `optimizer`: 가중치 업데이트 알고리즘. (예: 'adam', 'rmsprop')
    * `loss`: 손실 함수. ('binary_crossentropy', 'categorical_crossentropy' 등)
    * `metrics`: 모델 성능을 모니터링할 지표. (예: 'acc')

* **`fit()`**: 모델을 훈련시키는 함수.
    * `epochs`: 전체 데이터를 학습할 횟수.
    * `batch_size`: 한 번에 처리할 데이터의 양.
    * `validation_data` 또는 `validation_split`: 훈련 중 모델의 과적합 여부 확인.

---

### 5. 평가(Evaluation)와 예측(Prediction)

* **`evaluate()`**: 테스트 데이터를 사용해 모델의 최종 성능을 평가.
* **`predict()`**: 새로운 입력에 대한 모델의 예측값을 얻음.

---

### 6. 모델 저장(Save)과 불러오기(Load)

* **`save()`**: 학습된 모델의 구조와 가중치를 파일로 저장.
* **`load_model()`**: 저장된 모델을 다시 불러와 사용.


---

>### 딥러닝 개요: 케라스 함수형 API(Keras Functional API)

**함수형 API**는 Sequential API의 한계를 넘어, 복잡한 신경망 모델을 유연하게 설계하는 방법. 각 층을 함수처럼 정의하고, 이 함수들을 조합하여 모델을 만듦. 다중 입력, 다중 출력, 층 공유 등의 복잡한 구조 구현이 가능.

-----

### 1\. 함수형 API 모델 구성

함수형 API 모델은 다음의 3단계로 구성.

1.  **입력층 정의**: **`Input()`** 함수를 사용해 입력 데이터의 크기(`shape`)를 명시.
2.  **층 연결**: 이전 층을 다음 층 함수의 입력으로 넣어 연결.
3.  **모델 생성**: **`Model()`** 함수에 입력과 출력을 지정해 모델을 완성.

#### 💡 피드 포워드 신경망(FFNN) 예제

```python
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

# 입력층 정의
inputs = Input(shape=(10,))

# 은닉층과 출력층 연결
hidden1 = Dense(64, activation='relu')(inputs)
hidden2 = Dense(64, activation='relu')(hidden1)
output = Dense(1, activation='sigmoid')(hidden2)

# 입력과 출력을 지정하여 모델 완성
model = Model(inputs=inputs, outputs=output)
```

-----

### 2\. 다중 입력 모델

함수형 API의 가장 큰 장점 중 하나. 여러 개의 입력층을 정의하고, 각 층을 독립적으로 처리한 뒤 합치는 구조를 만들 수 있음.

#### 💡 다중 입력 모델 예제

```python
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model

# 첫 번째 입력층과 신경망
inputA = Input(shape=(64,))
x = Dense(8, activation="relu")(inputA)
x = Model(inputs=inputA, outputs=x)

# 두 번째 입력층과 신경망
inputB = Input(shape=(128,))
y = Dense(8, activation="relu")(inputB)
y = Model(inputs=inputB, outputs=y)

# 두 신경망의 출력을 병합(concatenate)
combined = concatenate([x.output, y.output])

# 병합된 출력을 이용해 최종 출력층 설계
z = Dense(1, activation="linear")(combined)

# 다중 입력을 가진 모델 완성
model = Model(inputs=[x.input, y.input], outputs=z)
```

-----

### 3\. RNN 은닉층 사용

**LSTM** 같은 순환 신경망 층을 포함하는 모델도 함수형 API로 설계 가능.

#### 💡 RNN 모델 예제

```python
from tensorflow.keras.layers import Input, Dense, LSTM
from tensorflow.keras.models import Model

inputs = Input(shape=(50,1))
lstm_layer = LSTM(10)(inputs)
x = Dense(10, activation='relu')(lstm_layer)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=inputs, outputs=output)
```

함수형 API는 복잡하고 비선형적인 모델 구조를 유연하게 표현할 수 있어, 딥러닝 전문가들이 선호하는 방식. 

---

>### 딥러닝 개요: 다층 퍼셉트론(MLP)으로 텍스트 분류

다층 퍼셉트론(MLP)은 여러 층을 쌓은 기본 형태의 신경망. 입력에서 출력 방향으로만 연산이 진행되는 피드 포워드 신경망(FFNN)의 일종. 이 모델로 텍스트 분류를 수행할 수 있음.

-----

### 1\. `texts_to_matrix()` 이해하기

케라스의 `Tokenizer`가 제공하는 `texts_to_matrix()`는 텍스트 데이터를 행렬로 변환. 이 함수는 단어 순서 정보를 고려하지 않음. 네 가지 모드가 있음.

  * **`binary`**: 단어의 존재 유무만 표현 (0 또는 1).
  * **`count`**: 각 단어의 등장 횟수(빈도수)를 표현. **문서 단어 행렬(DTM)** 생성.
  * **`tfidf`**: TF-IDF 값을 계산해 표현.
  * **`freq`**: 각 단어의 빈도를 문서 크기로 나눈 값을 표현.

#### 💡 `texts_to_matrix()` 예제

```python
from tensorflow.keras.preprocessing.text import Tokenizer

texts = ['먹고 싶은 사과', '먹고 싶은 바나나', '길고 노란 바나나 바나나', '저는 과일이 좋아요']
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

print('count 모드:\n', tokenizer.texts_to_matrix(texts, mode='count'))
print('binary 모드:\n', tokenizer.texts_to_matrix(texts, mode='binary'))
```

-----

### 2\. 20개 뉴스 그룹 데이터셋

20개의 다양한 주제를 가진 이메일 텍스트 데이터셋. 이 데이터로 텍스트 분류 모델을 훈련하고 평가.

  * **훈련 데이터(X\_train)**: 이메일 본문
  * **훈련 레이블(y\_train)**: 이메일의 주제(0\~19)

텍스트 데이터를 `texts_to_matrix()`로 벡터화하고, 레이블은 `to_categorical()`로 **원-핫 인코딩** 변환.

-----

### 3\. MLP 모델 설계와 훈련

MLP 모델은 **`Sequential`** API로 쉽게 구현.

  * **입력층**: `input_shape`으로 `vocab_size` 지정.
  * **은닉층**: `Dense` 층을 여러 개 추가하고, 과적합 방지를 위해 **`Dropout`** 층을 함께 사용.
  * **출력층**: 20개의 주제를 분류하는 다중 클래스 분류 문제이므로 `Dense` 층의 뉴런 수는 20개, 활성화 함수는 **`softmax`** 사용.


---
>### 딥러닝 개요: 피드 포워드 신경망 언어 모델(NNLM)

NNLM(Neural Network Language Model)은 인공 신경망을 활용해 언어를 학습하는 초기 모델. 기존의 통계 기반 언어 모델의 한계를 극복하기 위해 제안.

---

### 1. 기존 N-gram 언어 모델의 한계

**N-gram 언어 모델**은 다음 단어를 예측할 때 바로 앞의 `n-1`개의 단어만 참고. 이 방식은 훈련 코퍼스에 없는 단어 시퀀스에 대한 확률을 0으로 처리하는 희소 문제(Sparsity problem)를 가짐.

예를 들어, "발표 자료를 톺아보다"라는 문장이 코퍼스에 없다면, 모델은 `발표 자료를` 다음에 `톺아보다`가 올 확률을 0으로 예측. 이는 단어의 의미적 유사성을 고려하지 못하기 때문.

---

### 2. NNLM의 구조 및 특징

NNLM은 단어의 의미적 유사성을 학습하여 희소 문제를 해결.

* **입력**: 다음 단어를 예측하기 위해 정해진 개수(`n`)의 단어들. 이 단어들은 **원-핫 벡터**로 표현.
* **투사층(Projection layer)**: 입력된 원-핫 벡터를 밀집 벡터인 **임베딩 벡터**로 변환. 이 과정은 **룩업 테이블(lookup table)**과 유사. 임베딩 벡터는 초기에는 무작위 값이나, 학습을 통해 단어의 의미적 유사성을 반영하도록 값이 변함.
* **은닉층**: 투사층의 결과를 받아 일반적인 신경망 연산을 수행.
* **출력층**: 소프트맥스 함수를 사용해 다음 단어가 될 확률을 계산.

NNLM은 학습 과정에서 유사한 문맥에 등장하는 단어들이 유사한 임베딩 벡터를 갖도록 훈련. 이를 통해 코퍼스에 없는 단어 시퀀스에 대한 예측도 가능.



---

### 3. NNLM의 이점과 한계

* **이점**: 단어의 의미적 유사성을 학습해 **희소 문제**를 해결.
* **한계**: N-gram 모델과 마찬가지로, 예측 시 모든 이전 단어를 참고하지 않고 정해진 길이의 입력(`n`)만 사용. 이 한계는 **순환 신경망(RNN)** 모델에서 극복.

---




# ch.8 순환신경망

>### 순환 신경망(Recurrent Neural Network, RNN)

**RNN**은 입력과 출력을 시퀀스(Sequence) 단위로 처리하는 신경망. 문장과 같은 순차적인 데이터를 다루는 데 특화. RNN의 핵심은 **메모리 셀**이 이전 시점의 정보를 은닉 상태(hidden state)로 기억하고 다음 시점의 입력으로 사용한다는 점.

-----

### 1\. RNN의 구조

RNN은 피드 포워드 신경망과 달리 은닉층의 결과가 다시 은닉층의 다음 계산에 입력되는 **순환적인 구조**.

  - **입력층**: 입력 벡터. 주로 단어의 임베딩 벡터.
  - **은닉층(메모리 셀)**: 이전 시점의 은닉 상태($h\_{t-1}$)와 현재 시점의 입력($x\_t$)을 함께 받아 새로운 은닉 상태($h\_t$)를 계산.
  - **출력층**: 은닉 상태를 받아 최종 예측값을 출력.

RNN은 입력과 출력의 길이를 다르게 설계 가능. 이를 통해 다양한 문제 해결에 사용.

  * **다 대 일(many-to-one)**: 텍스트 분류 (예: 스팸 메일 분류, 감성 분석).
  * **일 대 다(one-to-many)**: 이미지 캡셔닝 (예: 이미지 -\> 문장).
  * **다 대 다(many-to-many)**: 번역, 챗봇, 품사 태깅.

-----

### 2\. 케라스(Keras)로 RNN 구현

케라스의 **`SimpleRNN`** 층을 사용해 RNN 모델을 쉽게 구현할 수 있음.

  * **`hidden_units`**: 은닉 상태의 크기. RNN의 용량(capacity)을 결정.
  * **`input_shape`**: 입력 시퀀스의 형태. `(timesteps, input_dim)`로 지정.
  * **`return_sequences=True`**: 모든 시점의 은닉 상태를 출력.
  * **`return_sequences=False`** (기본값): 마지막 시점의 은닉 상태만 출력.

<!-- end list -->

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN

# 마지막 은닉 상태만 출력하는 모델 (many-to-one)
model = Sequential()
model.add(SimpleRNN(hidden_units=3, input_shape=(2, 10)))
model.summary()

# 모든 시점의 은닉 상태를 출력하는 모델 (many-to-many)
model = Sequential()
model.add(SimpleRNN(hidden_units=3, input_shape=(2, 10), return_sequences=True))
model.summary()
```

-----

### 3\. 양방향 순환 신경망(Bidirectional RNN)

**양방향 RNN**은 순방향과 역방향 두 개의 메모리 셀을 사용. 과거 시점의 정보뿐만 아니라 미래 시점의 정보까지 활용해 현재 시점의 예측을 더 정확하게 만듦.

```python
from tensorflow.keras.layers import Bidirectional

# 양방향 RNN 모델 (many-to-many)
model = Sequential()
model.add(Bidirectional(SimpleRNN(hidden_units=3, return_sequences=True), input_shape=(10, 5)))
model.summary()
```

---

>### 순환 신경망: 장단기 메모리(LSTM)


LSTM(Long Short-Term Memory)은 기존 RNN의 장기 의존성 문제(Long-Term Dependencies)를 해결하기 위해 고안된 모델. 시퀀스 길이가 길어지면 앞쪽 정보가 뒤쪽으로 충분히 전달되지 못해 정보가 손실되는 RNN의 한계를 극복.

---

### 1. LSTM의 핵심 개념

LSTM은 RNN의 은닉 상태 외에 **셀 상태(Cell State)**라는 새로운 값을 도입. 셀 상태는 이전 시점의 중요한 정보를 다음 시점으로 전달하는 고속도로 역할. 이 셀 상태를 조절하기 위해 세 개의 **게이트**를 사용.

* **입력 게이트(Input Gate)**: 현재 시점의 정보를 셀 상태에 얼마나 반영할지 결정.
* **삭제 게이트(Forget Gate)**: 이전 시점의 셀 상태에서 어떤 정보를 지울지 결정.
* **출력 게이트(Output Gate)**: 셀 상태를 바탕으로 현재 시점의 은닉 상태를 얼마나 출력할지 결정.

이 게이트들은 모두 **시그모이드 함수**를 통과해 0과 1 사이의 값을 가지며, 정보를 얼마나 통과시킬지 조절하는 필터 역할.

---

### 2. LSTM의 내부 동작

1.  **삭제 게이트($f_t$)**: 이전 셀 상태($C_{t-1}$)의 정보를 얼마나 기억할지 결정. 0에 가까우면 삭제, 1에 가까우면 기억.

2.  **입력 게이트($i_t$)**: 현재 시점의 새로운 정보($\tilde{C}_t$)를 얼마나 반영할지 결정.
    -   $\tilde{C}_t$: 현재 입력($x_t$)과 이전 은닉 상태($h_{t-1}$)를 바탕으로 만든 새로운 후보 정보.
    -   $i_t$: 이 후보 정보를 얼마나 반영할지 결정하는 게이트.

3.  **셀 상태 업데이트($C_t$)**: 삭제 게이트의 결과($f_t$)로 이전 셀 상태($C_{t-1}$)를 걸러내고, 입력 게이트의 결과($i_t$)로 새로운 후보 정보($\tilde{C}_t$)를 더해 현재 시점의 셀 상태($C_t$)를 만듦.
    $C_t = f_t \otimes C_{t-1} + i_t \otimes \tilde{C}_t$

4.  **은닉 상태 업데이트($h_t$)**: 출력 게이트($o_t$)를 통해 최종 은닉 상태($h_t$)를 결정. 셀 상태($C_t$)를 tanh 함수에 통과시켜 활성화시킨 뒤, 출력 게이트의 결과($o_t$)와 곱해 은닉 상태를 만듦.
    $h_t = o_t \otimes \text{tanh}(C_t)$

이러한 복잡한 게이트 구조 덕분에 LSTM은 필요한 정보를 장기간 기억하고 불필요한 정보는 삭제할 수 있어, 긴 시퀀스에서도 효과적인 학습이 가능.

---

>### 순환 신경망: 게이트 순환 유닛(GRU)

LSTM과 유사한 성능을 보이지만, 더 간단한 구조를 가진 RNN의 변형 모델. LSTM의 복잡한 세 가지 게이트를 두 가지 게이트로 간소화해 계산 효율을 높였음.

-----

### 1\. GRU와 LSTM의 비교

GRU의 핵심은 업데이트 게이트(Update Gate)와 **리셋 게이트(Reset Gate)**.

  * **업데이트 게이트**: 삭제 게이트와 입력 게이트를 결합한 역할. 과거 정보와 현재 정보 중 어느 것을 더 많이 반영할지 결정.
  * **리셋 게이트**: 과거의 은닉 상태를 얼마나 무시할지 결정.

GRU는 LSTM과 비슷한 성능을 내면서도 매개변수가 적어 학습 속도가 더 빠르다는 장점. 데이터가 적을 때 GRU가 조금 더 나은 성능을 보이기도 함.

-----

### 2\. 케라스(Keras)로 GRU 구현

케라스에서 GRU를 사용하는 방법은 SimpleRNN이나 LSTM과 거의 동일.

#### 💡 GRU 모델 예제

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU

# GRU 층을 포함한 Sequential 모델 생성
model = Sequential()
model.add(GRU(hidden_units=128, input_shape=(10, 5)))
model.summary()
```

이 코드는 입력 시퀀스 길이 10, 입력 차원 5인 데이터를 받아 은닉 상태 크기가 128인 GRU 층을 추가. `LSTM` 대신 `GRU`만 바꿔 사용하면 됨.

---

>### RNN 언어 모델(RNNLM)

**RNNLM**은 RNN의 시퀀스 처리 능력을 활용한 언어 모델. N-gram이나 NNLM과 달리 고정된 입력 길이에 얽매이지 않고, 문맥을 기반으로 다음 단어를 예측.

---

### 1. RNNLM의 동작 원리

RNNLM은 각 시점($t$)에서 이전 시점의 은닉 상태($h_{t-1}$)와 현재 시점의 입력($x_t$)을 받아 다음 단어를 예측. 이 과정에서 각 시점의 예측 결과를 다음 시점의 입력으로 사용하는 구조.

* **훈련 과정 (교사 강요)**: 모델은 다음 단어를 예측할 때, 이전 시점의 예측값 대신 실제 정답(레이블)을 다음 시점의 입력으로 사용. 이 방법을 **교사 강요(Teacher Forcing)**라 함. 잘못된 예측이 누적되어 학습이 불안정해지는 것을 막고 훈련 속도를 높임.
* **테스트 과정**: 훈련이 끝난 모델은 이전 시점의 예측값을 다음 시점의 입력으로 사용해 다음 단어를 예측.

---

### 2. RNNLM의 내부 구조

1.  **입력층**: 현재 시점의 단어를 **원-핫 벡터**로 받음.
2.  **임베딩층(Embedding layer)**: 원-핫 벡터를 밀집 벡터인 **임베딩 벡터**로 변환. 이 임베딩 벡터는 모델 학습 과정에서 단어 간의 의미적 유사성을 반영하도록 업데이트됨.
3.  **은닉층**: 현재 시점의 임베딩 벡터와 이전 시점의 은닉 상태를 입력으로 받아 새로운 은닉 상태를 계산.
4.  **출력층**: 은닉 상태를 받아 **소프트맥스 함수**를 통해 다음 단어가 될 확률을 계산.

이 과정에서 임베딩 행렬을 포함한 모든 가중치 행렬들이 학습. RNNLM은 단어의 유사성을 학습할 수 있어 희소 문제를 해결.



---




