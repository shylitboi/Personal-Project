# 딥러닝을 이용한 자연어 처리 입문

## ch.2 텍스트 전처리


>### 텍스트 전처리: 토큰화(Tokenization)

텍스트 전처리는 원시(raw) 데이터를 모델이 이해할 수 있는 형태로 바꾸는 과정. 그중 **토큰화**는 문장을 의미 있는 작은 단위인 토큰(Token)으로 나누는 작업.

토큰의 기준은 상황에 따라 다양. 때로는 단어가 토큰, 때로는 문장이 토큰, 심지어 글자 하나하나가 토큰이 되기도 함.

### 1\. 단어 토큰화 (Word Tokenization)

단어 토큰화는 문장을 띄어쓰기나 구두점 등을 기준으로 단어 단위로 나누는 가장 일반적인 방식.

예를 들어, "Time is an illusion. Lunchtime double so\!"라는 문장을 토큰화하면 "Time", "is", "an", "illusion", "Lunchtime", "double", "so"와 같은 단어들.

하지만 단순히 띄어쓰기나 구두점을 기준으로 나누는 것은 완벽하지 않음. 다음과 같은 예외 상황들을 고려해야 함.

  * **구두점**: `Mr.`, `Ph.D.`, `$45.55`처럼 단어 중간에 구두점이 있는 경우.
  * **줄임말**: `Don't` (Do not), `I'm` (I am)처럼 축약된 단어.
  * **복합 명사**: `New York`, `rock 'n' roll`처럼 띄어쓰기가 포함된 하나의 단어.

이러한 문제 해결을 위해 **NLTK**, **Keras** 등 다양한 라이브러리가 여러 방식의 토큰화 도구를 제공.

#### 💡 NLTK, Keras를 이용한 단어 토큰화 예제

```python
import nltk
from nltk.tokenize import word_tokenize, WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence

text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."

# NLTK의 word_tokenize: '와 구두점을 단어와 분리
print('NLTK word_tokenize:', word_tokenize(text))

# NLTK의 WordPunctTokenizer: 모든 구두점을 분리
print('NLTK WordPunctTokenizer:', WordPunctTokenizer().tokenize(text))

# Keras의 text_to_word_sequence: 소문자화하고 구두점 제거 (단, '는 유지)
print('Keras text_to_word_sequence:', text_to_word_sequence(text))
```

-----

### 2\. 문장 토큰화 (Sentence Tokenization)

문장 토큰화는 텍스트를 문장 단위로 나누는 작업. `.`(마침표), `?`(물음표), `!`(느낌표)를 기준으로 문장을 나눌 수 있음. 하지만 `Ph.D.`와 같이 단어 중간에 마침표가 있는 경우를 고려해야 함.

#### 💡 NLTK, KSS를 이용한 문장 토큰화 예제

```python
import nltk
from nltk.tokenize import sent_tokenize
import kss

# NLTK를 사용하기 전에 다운로드가 필요할 수 있습니다.
# nltk.download('punkt')

# 영어 문장 토큰화 (NLTK)
text_en = "I am actively looking for Ph.D. students. And you are a Ph.D. student."
print('영어 문장 토큰화 (NLTK):', sent_tokenize(text_en))

# 한국어 문장 토큰화 (KSS)
# KSS는 한국어에 특화된 문장 분리 도구입니다.
text_ko = "딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?"
print('한국어 문장 토큰화 (KSS):', kss.split_sentences(text_ko))
```

-----

### 3\. 한국어 토큰화의 어려움

한국어는 영어와는 달리 **교착어**라는 특징. 조사나 어미가 단어 뒤에 붙기 때문에, 띄어쓰기만으로는 의미 있는 단어(토큰)를 나누기 어려움.

예를 들어, '책을 읽었다'라는 문장에서 `책을`은 '책' + '을'로 분리되어야 함. '책'은 의미를 가진 독립적인 단어인 **자립 형태소**이고, '을'은 다른 형태소에 붙어서 사용되는 **의존 형태소**(조사).

이러한 이유로 한국어는 단순히 띄어쓰기만으로 토큰화를 하는 것보다 **형태소(morpheme)** 단위로 분리하는 형태소 분석(Morpheme Analysis)의 사용이 일반적.

#### 💡 KoNLPy를 이용한 한국어 형태소 분석 예제

```python
from konlpy.tag import Okt, Kkma

text = "열심히 코딩한 당신, 연휴에는 여행을 가봐요"

# Okt(Open Korea Text) 형태소 분석기 사용
okt = Okt()
print('Okt 형태소 추출:', okt.morphs(text))
print('Okt 품사 태깅:', okt.pos(text))
print('Okt 명사 추출:', okt.nouns(text))

print("-" * 30)

# Kkma(꼬꼬마) 형태소 분석기 사용
kkma = Kkma()
print('Kkma 형태소 추출:', kkma.morphs(text))
print('Kkma 품사 태깅:', kkma.pos(text))
print('Kkma 명사 추출:', kkma.nouns(text))
```

위 코드를 실행하면, 두 분석기의 결과가 약간씩 다름. 각 형태소 분석기는 고유한 규칙과 알고리즘을 가지고 있기 때문에, 프로젝트의 목적에 가장 적합한 것을 선택하는 것이 중요. 보통 속도가 중요한 경우 Mecab을, 정확도가 중요한 경우 다른 분석기를 비교하여 사용.

-----

### 4\. 품사 태깅 (Part-of-speech tagging)

**품사 태깅**은 토큰화된 단어에 품사를 붙여주는 작업. 단어의 의미는 품사에 따라 달라질 수 있기 때문에, 품사 정보는 자연어 처리에서 매우 유용한 정보. 예를 들어 영어 단어 'fly'는 동사('날다') 또는 명사('파리')로 사용될 수 있음.

#### 💡 NLTK를 이용한 영어 품사 태깅 예제

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

# NLTK를 사용하기 전에 다운로드가 필요할 수 있습니다.
# nltk.download('averaged_perceptron_tagger')

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('품사 태깅:', pos_tag(tokenized_sentence))
```

이 예제에서는 NLTK의 **Penn Treebank POS Tags** 기준에 따라 `I`는 `PRP`(인칭 대명사), `am`은 `VBP`(동사) 등으로 품사가 태깅.

---
>### 텍스트 전처리: 정제(Cleaning)와 정규화(Normalization)


토큰화 전후로 이루어지는 **정제**와 **정규화**. 이 두 작업은 텍스트 데이터를 용도에 맞게 다듬는 필수 과정.

  * **정제**: 불필요한 노이즈 데이터의 제거.
  * **정규화**: 표기 방식이 다른 단어들을 하나의 형태로 통일.

정제는 토큰화에 방해가 되는 요소를 사전에 제거하거나, 토큰화 이후에도 남아있는 노이즈를 없애는 지속적인 작업. 완벽한 정제는 어려워 대부분은 적절한 합의점의 선택.

-----

### 1\. 규칙 기반 단어 통합

직접 규칙을 정해 같은 의미를 가진 다른 표기의 단어를 통합하는 방법. 예를 들어 **USA**와 **US**를 같은 단어로 취급하거나, **uh-huh**와 **uhhuh**를 동일하게 처리. 이러한 정규화는 검색 효율을 높이는 효과. 뒤에서 다룰 어간 추출(Stemming)과 표제어 추출(Lemmatization)이 이와 유사한 작업.

-----

### 2\. 대소문자 통합

영어나 다른 서구권 언어에서 대소문자 통합은 단어의 다양성을 줄이는 효과적인 정규화 방법. 일반적으로는 모든 대문자를 소문자로 변환. 예를 들어 문장 첫머리의 **Automobile**을 **automobile**로 바꾸면, `automobile` 검색 시 `Automobile`도 함께 찾을 수 있음.

하지만 무조건적인 통합은 때때로 문제가 될 수 있음. 예를 들어, 미국을 의미하는 **US**와 우리를 뜻하는 **us**는 구분이 필요. 회사명이나 사람 이름처럼 고유명사는 대문자로 유지하는 것이 바람직. 이러한 예외를 고려하는 방법도 있지만, 대부분의 경우 모든 텍스트를 소문자로 변환하는 것이 실용적인 해결책.

-----

### 3\. 불필요한 단어 제거

정제 작업은 의미 없는 특수 문자뿐만 아니라, 분석 목적에 맞지 않는 단어들을 노이즈로 간주하고 제거하는 작업. **불용어(Stopword)** 제거, 등장 빈도가 적은 단어 제거, 길이가 짧은 단어 제거가 대표적인 방법.

  * **등장 빈도가 적은 단어**: 전체 코퍼스에서 너무 적게 등장하는 단어는 모델 학습에 거의 도움이 되지 않으므로 제거.
  * **길이가 짧은 단어**: 영어의 경우, 길이가 2\~3 이하인 단어는 대부분 불용어에 해당. 예를 들어 `a`, `I`, `it`, `at` 등. 이러한 단어를 제거하는 것은 불용어 제거에 효과적. 하지만 한글은 한 글자만으로도 의미를 가지는 경우가 많아 (예: 용(龍), 귤), 무조건적인 길이 기반 삭제는 의미 있는 단어 손실의 위험.

#### 💡 길이가 짧은 단어 제거 예제

정규 표현식(`re`)을 이용해 길이가 짧은 단어를 삭제하는 방법.

```python
import re

text = "I was wondering if anyone out there could enlighten me on this car."

# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
```

위 코드를 실행하면, 'I', 'on', 'if' 등의 짧은 단어가 제거된 결과.

-----

### 4\. 정규 표현식 (Regular Expression)

**정규 표현식**은 텍스트 내에서 특정 패턴을 가진 문자열을 찾고 처리하는 강력한 도구. HTML 태그나 불필요한 특수 문자 등 규칙적으로 나타나는 노이즈 데이터를 한 번에 제거하는 데 매우 유용. 위 예제에서도 길이가 짧은 단어를 제거하는 데 정규 표현식의 활용. 앞으로의 전처리 과정에서 자주 사용될 핵심 도구.

---


>### 텍스트 전처리: 어간 추출(Stemming)과 표제어 추출(Lemmatization)

**표제어 추출**과 **어간 추출**은 정규화의 핵심 기법. 겉으로 다른 단어들을 하나의 단어로 통합해 문서의 단어 수를 줄이는 것이 목적. 주로 단어 빈도 기반의 NLP 모델에서 활용.

-----

### 1\. 표제어 추출 (Lemmatization)

단어의 기본 사전형인 표제어(Lemma)를 찾는 과정. **am, are, is**의 표제어는 **be**가 되는 것처럼, 단어의 뿌리 형태를 찾아감.

  * **어간**은 단어의 핵심 의미를 담은 부분.
  * **접사**는 추가적인 의미를 부여하는 부분.
  * NLTK의 **WordNetLemmatizer** 사용. 정확한 표제어 추출을 위해 단어의 품사 정보(동사, 명사 등)를 함께 입력하는 것이 중요.

#### 💡 표제어 추출 예제

```python
import nltk
from nltk.stem import WordNetLemmatizer

# WordNet 다운로드 필요
# nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('표제어 추출 전:', words)
print('표제어 추출 후:', [lemmatizer.lemmatize(word) for word in words])
```

위 코드는 `lives`가 `life`로, `dies`가 `dy`로 변환되는 등 일부는 제대로, 일부는 잘못 변환. 품사 정보를 추가하면 정확도 향상.

```python
# 품사 정보(동사 'v') 추가하여 표제어 추출
print(lemmatizer.lemmatize('dies', 'v'))
print(lemmatizer.lemmatize('watched', 'v'))
print(lemmatizer.lemmatize('has', 'v'))
```

-----

### 2\. 어간 추출 (Stemming)

형태론적 분석을 단순화한 방식. 접미사를 자르는 규칙 기반의 작업. 결과 단어가 사전에 없는 경우가 많음.

  * NLTK의 **PorterStemmer** 사용.
  * 예: **organization**의 어간은 **organ**으로 추출되는데, 이는 의미가 달라져 과하게 일반화되는 문제.

#### 💡 어간 추출 예제

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
tokenized_sentence = word_tokenize(sentence)

print('어간 추출 전:', tokenized_sentence)
print('어간 추출 후:', [stemmer.stem(word) for word in tokenized_sentence])
```

-----

### 3\. 표제어 추출 vs. 어간 추출

두 방식의 가장 큰 차이점은 결과의 정확성.

  * **표제어 추출**: 단어의 품사를 고려해 사전에 있는 단어를 결과로 도출하려는 정밀한 작업.
  * **어간 추출**: 단순 규칙에 따라 접미사를 제거하는 작업. 결과 단어가 사전에 없을 수 있고, 의미가 변질될 수 있음. 일반적으로 어간 추출이 더 빠름.

-----

### 4\. 한국어에서의 어간 추출

한국어는 동사와 형용사 같은 **용언**이 **어간**과 **어미**의 결합으로 이루어짐.

  * **어간**: 용언의 의미를 담고 변하지 않는 부분 (일부 불규칙 변화).
  * **어미**: 어간 뒤에 붙어 활용하며 변하는 부분.

**규칙 활용**은 어간이 일정하므로 어미만 분리하면 어간 추출이 쉬움. 하지만 **불규칙 활용**은 어간의 형태가 변하기 때문에 더 복잡한 규칙이 필요.

---

>### 텍스트 전처리: 불용어(Stopword)

*불용어(Stopword)는 자주 등장하지만 실제 의미 분석에 큰 도움이 되지 않는 단어들. 예를 들어 영어의 'I', 'me', 'my'나 한국어의 조사, 접속사 같은 단어. NLP 모델의 효율을 높이고 노이즈를 줄이기 위해 제거하는 대상.


### 1\. NLTK를 이용한 불용어 제거

NLTK는 영어 불용어 리스트를 기본으로 제공. `stopwords.words('english')`로 이 리스트를 불러와 텍스트에서 불용어를 제거할 수 있음.

#### 💡 NLTK 불용어 제거 예제

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# NLTK 불용어 리스트 확인
stop_words_list = stopwords.words('english')
print('NLTK 불용어 개수:', len(stop_words_list))
print('NLTK 불용어 10개:', stop_words_list[:10])

# 불용어 제거 실행
example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english'))
word_tokens = word_tokenize(example)

result = [word for word in word_tokens if word not in stop_words]
print('불용어 제거 전:', word_tokens)
print('불용어 제거 후:', result)
```

-----

### 2\. 한국어 불용어 제거

한국어는 NLTK처럼 보편적인 불용어 리스트가 없어, 분석 목적에 맞게 직접 불용어 사전을 만들어야 함. 토큰화 후, 직접 정의한 불용어 리스트에 포함된 단어를 제거.

#### 💡 한국어 불용어 제거 예제

```python
from konlpy.tag import Okt

okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는" # 직접 정의한 불용어
stop_words = set(stop_words.split(' '))

word_tokens = okt.morphs(example)
result = [word for word in word_tokens if word not in stop_words]

print('불용어 제거 전:', word_tokens)
print('불용어 제거 후:', result)
```

불용어 리스트는 텍스트 파일이나 CSV 파일로 관리하기도 함.

---

>### 텍스트 전처리: 정수 인코딩(Integer Encoding)

**정수 인코딩**은 자연어 처리 모델이 텍스트를 숫자로 이해하도록 단어들을 고유한 정수로 바꾸는 작업. 일반적으로 단어의 빈도수를 기준으로 높은 빈도의 단어에 낮은 정수(인덱스)를 부여.

-----

### 1\. Dictionary를 이용한 정수 인코딩

파이썬 딕셔너리를 사용해 단어의 빈도수를 계산하고, 이를 기반으로 정수 인덱스를 부여할 수 있음. 이 과정에서 빈도수가 낮은 단어는 제거하여 데이터의 복잡성을 줄임. 또한, 단어 집합에 없는 단어(Out-Of-Vocabulary, **OOV**)는 'OOV' 토큰으로 대체.

#### 💡 예제 코드

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import re

raw_text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

# 텍스트 전처리 및 단어 빈도수 계산
sentences = sent_tokenize(raw_text)
preprocessed_sentences = []
vocab = {}
stop_words = set(stopwords.words('english'))

for sentence in sentences:
    tokenized_sentence = word_tokenize(sentence)
    result = []
    for word in tokenized_sentence:
        word = word.lower()
        if word not in stop_words and len(word) > 2:
            result.append(word)
            if word not in vocab:
                vocab[word] = 0
            vocab[word] += 1
    preprocessed_sentences.append(result)

# 빈도수 순으로 정렬 및 정수 인덱스 부여 (빈도수 1인 단어 제외)
vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)
word_to_index = {}
i = 0
for (word, frequency) in vocab_sorted:
    if frequency > 1:
        i += 1
        word_to_index[word] = i

# OOV(단어 집합에 없는 단어) 처리
word_to_index['OOV'] = len(word_to_index) + 1

# 문장 정수 인코딩
encoded_sentences = []
for sentence in preprocessed_sentences:
    encoded_sentence = []
    for word in sentence:
        try:
            encoded_sentence.append(word_to_index[word])
        except KeyError:
            encoded_sentence.append(word_to_index['OOV'])
    encoded_sentences.append(encoded_sentence)

print("단어-정수 맵핑:", word_to_index)
print("인코딩된 문장:", encoded_sentences)
```

-----

### 2\. 케라스(Keras) Tokenizer를 이용한 정수 인코딩

케라스의 **Tokenizer**는 정수 인코딩을 더 편리하게 수행하는 도구. `fit_on_texts`로 단어 집합을 생성하고, `texts_to_sequences`로 텍스트를 정수 시퀀스로 변환.

#### 💡 예제 코드

```python
from tensorflow.keras.preprocessing.text import Tokenizer

# 전처리된 문장 데이터
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

# Tokenizer 객체 생성
tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)

# 단어-정수 맵핑 확인
print('단어-정수 맵핑:', tokenizer.word_index)

# 텍스트를 정수 시퀀스로 변환
print('정수 인코딩 결과:', tokenizer.texts_to_sequences(preprocessed_sentences))
```

**Tokenizer**에 `num_words` 인자를 사용하여 사용할 단어의 개수를 제한할 수 있음. 또한, `oov_token` 인자를 사용해 단어 집합에 없는 단어를 특정 토큰으로 대체할 수 있음.

-----
>### 텍스트 전처리: 패딩(Padding)

**패딩**은 길이가 다른 문장들을 동일한 길이로 맞추는 작업. 딥러닝 모델은 입력 데이터의 길이가 같아야 하기 때문에, 병렬 처리를 위해 필수적인 전처리 단계. 일반적으로 길이가 짧은 문장의 뒤나 앞에 0을 채워 넣는 **제로 패딩**을 사용.

-----

### 1\. NumPy를 이용한 패딩

가장 긴 문장의 길이를 기준으로, 짧은 문장에 0을 추가하여 길이를 맞추는 방법.

#### 💡 예제 코드

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer

# 정수 인코딩된 문장 데이터
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)

# 가장 긴 문장의 길이 확인
max_len = max(len(item) for item in encoded)
print('최대 길이:', max_len)

# 짧은 문장 뒤에 0을 추가하여 패딩
for sentence in encoded:
    while len(sentence) < max_len:
        sentence.append(0)

padded_np = np.array(encoded)
print('패딩 결과:\n', padded_np)
```

-----

### 2\. 케라스(Keras) `pad_sequences` 이용

케라스는 패딩을 위한 전용 함수 `pad_sequences`를 제공. 이 함수는 훨씬 간단하고 유연한 패딩 방법을 제공.

  * `padding='pre'` (기본값)는 문장 앞에 0을 채움.
  * `padding='post'`는 문장 뒤에 0을 채움.
  * `maxlen` 인자를 사용해 패딩할 최대 길이를 직접 지정 가능. 이보다 긴 문장은 잘림.
  * `truncating='pre'` (기본값)는 문장 앞에서, `truncating='post'`는 문장 뒤에서 자름.
  * `value` 인자를 사용해 0이 아닌 다른 숫자로 패딩할 수 있음.

#### 💡 예제 코드

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 정수 인코딩된 문장 데이터
encoded = [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

# 뒤에 0을 채우는 패딩
padded_post = pad_sequences(encoded, padding='post')
print('패딩(padding=\'post\') 결과:\n', padded_post)

# 최대 길이 5로 제한하고 뒤에 0을 채우는 패딩
padded_maxlen = pad_sequences(encoded, padding='post', maxlen=5)
print('패딩(maxlen=5, padding=\'post\') 결과:\n', padded_maxlen)

# 최대 길이 5로 제한하고 뒤를 자르는(truncating) 패딩
padded_trunc = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)
print('패딩(maxlen=5, truncating=\'post\') 결과:\n', padded_trunc)
```

---

>### 텍스트 전처리: 데이터 분리(Splitting Data)

쉬워서 pass

---

>### 텍스트 전처리: 한국어 전처리 패키지

한국어는 띄어쓰기, 맞춤법, 신조어 등 전처리 과정에 어려움이 있음. 이를 위해 다양한 한국어 전용 패키지를 사용.

-----

### 1\. PyKoSpacing: 띄어쓰기 교정

**PyKoSpacing**은 딥러닝 기반으로 띄어쓰기가 없는 문장을 교정하는 패키지. 대용량 코퍼스를 학습하여 뛰어난 성능을 보임.

#### 💡 예제 코드

```python
# 설치: pip install git+https://github.com/haven-jeon/PyKoSpacing.git

from pykospacing import Spacing

sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'

new_sent = sent.replace(" ", '') # 띄어쓰기 제거
print('띄어쓰기 없는 문장:', new_sent)

spacing = Spacing()
kospacing_sent = spacing(new_sent)

print('PyKoSpacing으로 교정된 문장:', kospacing_sent)
print('원본 문장:', sent)
```

-----

### 2\. Py-Hanspell: 맞춤법 교정

**Py-Hanspell**은 네이버 맞춤법 검사기를 기반으로 맞춤법과 띄어쓰기를 함께 교정.

#### 💡 예제 코드

```python
# 설치: pip install git+https://github.com/ssut/py-hanspell.git

from hanspell import spell_checker

sent = "맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 "
spelled_sent = spell_checker.check(sent)

print('교정된 문장:', spelled_sent.checked)
```

-----

### 3\. SOYNLP: 비지도 학습 단어 토큰화

**SOYNLP**는 **응집 확률**과 **브랜칭 엔트로피**를 이용한 비지도 학습 기반 토크나이저. 기존 형태소 분석기가 처리하지 못하는 신조어나 미등록 단어 문제에 강점.

  * **응집 확률(Cohesion Probability)**: 한 단어 내 문자열이 얼마나 응집하여 자주 나타나는지를 측정하는 척도.
  * **브랜칭 엔트로피(Branching Entropy)**: 주어진 문자열 뒤에 올 수 있는 다음 문자의 다양성을 측정하는 척도. 단어가 끝나는 지점에서 이 값이 증가하는 경향.

#### 💡 SOYNLP 토크나이저 예제

```python
# 설치: pip install soynlp

import urllib.request
from soynlp.word import WordExtractor
from soynlp.tokenizer import MaxScoreTokenizer

# 학습을 위한 데이터 다운로드
# urllib.request.urlretrieve("https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt", filename="2016-10-20.txt")

# WordExtractor로 학습
word_extractor = WordExtractor()
word_extractor.train("2016-10-20.txt")
word_score_table = word_extractor.extract()

# 최대 점수 토크나이저 사용
maxscore_tokenizer = MaxScoreTokenizer(scores=word_score_table)
print('SOYNLP 토큰화 결과:', maxscore_tokenizer.tokenize("국제사회와우리의노력들로범죄를척결하자"))

# SOYNLP를 이용한 반복 문자 정규화
from soynlp.normalizer import emoticon_normalize
print('반복 이모티콘 정규화:', emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))
```

-----

### 4\. Customized KoNLPy: 사용자 사전 추가

기존 형태소 분석기가 고유명사 등을 잘못 분리할 때, 사용자 사전을 추가해 올바른 토큰을 얻는 방법. **Customized KoNLPy**는 이 과정을 쉽게 만들어주는 패키지.

#### 💡 사용자 사전 추가 예제

```python
# 설치: pip install customized_konlpy

from ckonlpy.tag import Twitter

twitter = Twitter()
print('사전 추가 전:', twitter.morphs('은경이는 사무실로 갔습니다.'))

# '은경이'를 하나의 명사로 사용자 사전 추가
twitter.add_dictionary('은경이', 'Noun')
print('사전 추가 후:', twitter.morphs('은경이는 사무실로 갔습니다.'))
```



## ch.7 딥러닝 개요

>### 딥러닝 개요: 퍼셉트론(Perceptron)


**퍼셉트론**은 초기 형태의 인공 신경망. 뇌의 뉴런을 모방해 여러 개의 입력 신호를 받아 하나의 출력 신호를 내보내는 알고리즘. 각 입력값에는 중요도를 나타내는 가중치(Weight)가 곱해지고, 이 합이 임계치(Threshold)를 넘으면 1, 아니면 0을 출력.

-----

### 단층 퍼셉트론(Single-Layer Perceptron)

입력층과 출력층, 단 두 개의 층으로 구성. 이 모델은 **AND**, **NAND**, **OR** 게이트와 같은 선형 분리 가능한 문제만 해결할 수 있음. 하지만 XOR 게이트처럼 하나의 직선으로 나눌 수 없는 문제는 해결 불가.

#### 💡 파이썬으로 구현한 AND 게이트

```python
def AND_gate(x1, x2):
    w1 = 0.5
    w2 = 0.5
    b = -0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1

print(AND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1))
```

-----

### 다층 퍼셉트론(MultiLayer Perceptron, MLP)

단층 퍼셉트론의 한계를 극복하기 위해 중간에은닉층(Hidden layer)을 추가한 구조. 은닉층을 여러 개 쌓아 복잡한 비선형 문제도 해결할 수 있음. 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 함.

딥러닝은 이러한 심층 신경망을 손실 함수(Loss function)\와 옵티마이저(Optimizer)를 사용해 최적의 가중치를 자동으로 찾아 학습시키는 과정.

---


>### 딥러닝 개요: 인공 신경망(Artificial Neural Network)

**인공 신경망**은 딥러닝의 기본 구조. 여러 개의 층(layer)으로 구성. 각 층은 여러 개의 뉴런으로 이루어져 있으며, 각 뉴런은 입력값을 받아 가중치를 곱하고, **활성화 함수**를 거쳐 다음 층으로 값을 전달.

-----

### 1\. 피드 포워드 신경망(FFNN)

**피드 포워드 신경망**은 입력층에서 출력층으로 오직 한 방향으로만 연산이 진행되는 신경망 구조. 정보가 순차적으로 흐르는 형태.

-----

### 2\. 전결합층(Fully-connected layer)

**전결합층**은 한 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결된 형태. 케라스에서는 **`Dense`** 층으로 구현.

-----

### 3\. 활성화 함수(Activation Function)

활성화 함수는 뉴런의 출력값을 결정하는 비선형 함수. 인공 신경망의 은닉층은 비선형성을 가져야만 층을 깊게 쌓아 복잡한 문제를 풀 수 있음.

  * **시그모이드(Sigmoid)**: 출력값이 0과 1 사이. 이진 분류의 출력층에 주로 사용. 하지만 미분값이 0에 가까워지면 학습이 멈추는 **기울기 소실(Vanishing Gradient)** 문제 발생.
  * **하이퍼볼릭탄젠트(tanh)**: 출력값이 -1과 1 사이. 시그모이드보다 기울기 소실 문제가 덜해 은닉층에 시그모이드보다 선호.
  * **렐루(ReLU)**: 입력이 양수면 그대로, 음수면 0을 출력. 계산이 빠르고 기울기 소실 문제도 적어 은닉층에서 가장 인기. 하지만 입력이 음수일 때 기울기가 0이 되어 뉴런이 죽는 **죽은 렐루(dying ReLU)** 문제가 있음.
  * **리키 렐루(Leaky ReLU)**: 렐루의 변형으로, 음수 입력에 대해 아주 작은 양수 값을 반환해 죽은 렐루 문제 보완.
  * **소프트맥스(Softmax)**: 출력값이 확률 분포를 형성. 다중 클래스 분류의 출력층에 주로 사용.


---

>### 딥러닝 학습 방법

딥러닝의 학습은 **손실 함수**의 값을 최소화하는 방향으로 모델의 매개변수를 조정하는 과정. 이 과정은 **옵티마이저**, **배치 크기**, **에포크** 등의 개념을 통해 이루어짐.

---

### 1. 손실 함수(Loss function)

**손실 함수**는 모델의 예측값과 실제값의 차이(오차)를 수치화하는 함수. 오차가 클수록 손실 함수의 값도 커짐.

* **평균 제곱 오차(MSE)**: 회귀 모델에 주로 사용.
* **이진 교차 엔트로피(Binary Cross-Entropy)**: 이진 분류 모델의 출력층에 사용.
* **범주형 교차 엔트로피(Categorical Cross-Entropy)**: 다중 클래스 분류 모델에 사용. 원-핫 인코딩된 레이블에 적용.
* **희소 범주형 교차 엔트로피(Sparse Categorical Cross-Entropy)**: 범주형 교차 엔트로피와 동일하나, 정수형 레이블에 직접 적용.

---

### 2. 경사 하강법과 배치 크기

**경사 하강법**은 손실 함수 값을 줄여나가며 최적의 매개변수를 찾는 방법. 이때 가중치 업데이트에 사용하는 데이터의 양을 **배치(Batch)**라고 함.

* **배치 경사 하강법(Batch Gradient Descent)**: 전체 데이터를 한 번에 사용해 매개변수 업데이트. 계산 시간이 오래 걸리고 메모리 사용량이 많음.
* **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**: 한 번에 1개의 데이터만 사용해 업데이트. 계산이 빠르지만 불안정.
* **미니 배치 경사 하강법(Mini-Batch Gradient Descent)**: 정해진 수의 데이터(보통 128, 256 등)를 사용해 업데이트. 배치 경사 하강법보다 빠르고 SGD보다 안정적이라 가장 많이 사용.

---

### 3. 옵티마이저(Optimizer)

**옵티마이저**는 손실 함수를 최소화하는 경사 하강법의 구체적인 알고리즘.

* **모멘텀(Momentum)**: 경사 하강법에 관성을 더해 최적화 속도를 높임.
* **아다그라드(Adagrad)**: 각 매개변수에 다른 학습률을 적용. 변화가 많은 매개변수는 학습률이 작게, 적은 매개변수는 크게 설정.
* **아담(Adam)**: 모멘텀과 아다그라드의 장점을 결합한 방식. 현재 가장 널리 사용되는 옵티마이저.

---

### 4. 에포크, 배치 크기, 이터레이션

이 세 가지 개념은 모델 학습 방식을 이해하는 데 필수.

* **에포크(Epoch)**: 전체 데이터셋을 한 번 학습하는 횟수. 전체 문제지를 한 번 다 푼 것.
* **배치 크기(Batch size)**: 한 번의 가중치 업데이트에 사용하는 데이터 샘플의 수. 한 번에 푸는 문제의 개수.
* **이터레이션(Iteration)**: 한 에포크를 완료하는 데 필요한 배치 수. `(전체 데이터 수 / 배치 크기)`. 가중치 업데이트 횟수.

---

>### 딥러닝 학습: 역전파(Backpropagation)

**역전파**는 딥러닝 모델의 가중치를 업데이트하는 핵심 알고리즘. 순전파를 통해 얻은 예측값과 실제값의 오차를 계산한 뒤, 이 오차를 바탕으로 미분(기울기)을 구해 출력층에서 입력층 방향으로 거꾸로 전달하며 가중치를 업데이트.

---

### 1. 순전파(Forward Propagation)

**순전파**는 입력 데이터가 신경망의 입력층에서 출력층으로 순차적으로 계산되는 과정. 각 층의 뉴런은 이전 층의 출력값에 가중치를 곱하고, 이 값들을 합산해 활성화 함수를 통과시키는 과정을 반복. 최종적으로 출력층에서 모델의 **예측값**이 나옴.

---

### 2. 역전파 과정

1.  **오차 계산**: 예측값과 실제값의 차이를 **손실 함수**를 이용해 계산.
2.  **기울기 계산**: 손실 함수를 미분하여 각 가중치에 대한 기울기를 구함. 이 기울기는 가중치를 얼마나, 어느 방향으로 업데이트할지 결정.
3.  **가중치 업데이트**: 계산된 기울기를 바탕으로 **경사 하강법**을 적용해 가중치를 수정. 이 과정은 출력층에서 시작해 입력층 방향으로 거꾸로 진행.

역전파의 핵심은 **미분의 연쇄 법칙(Chain rule)**. 복잡한 신경망의 각 가중치에 대한 기울기를 효율적으로 계산할 수 있게 함.

#### 💡 역전파를 통한 가중치 업데이트 예시

예시로 주어진 신경망에서 가중치 $W_3$를 업데이트하는 과정은 다음과 같음:

1.  **오차 계산**: 전체 오차 $E_{total}$를 MSE 함수로 계산.
    $E_{total} = \frac{1}{2}\sum (target - output)^2$
2.  **기울기 계산**: $\frac{\partial E_{total}}{\partial W_3}$를 미분의 연쇄 법칙으로 분해하여 계산.
    $\frac{\partial E_{total}}{\partial W_3} = \frac{\partial E_{total}}{\partial output_1} \times \frac{\partial output_1}{\partial net_1} \times \frac{\partial net_1}{\partial W_3}$
    * $\frac{\partial E_{total}}{\partial output_1}$: 오차를 출력값으로 미분.
    * $\frac{\partial output_1}{\partial net_1}$: 활성화 함수(시그모이드)를 미분.
    * $\frac{\partial net_1}{\partial W_3}$: 가중치에 대한 입력값.

3.  **가중치 업데이트**: 학습률($\eta$)을 곱하여 가중치 $W_3$를 업데이트.
    $W_3^{new} = W_3^{old} - \eta \frac{\partial E_{total}}{\partial W_3}$

이 과정을 모든 가중치에 대해 반복하면 모델의 오차가 점차 감소. 이것이 딥러닝 학습의 기본 원리.

---

>### 딥러닝 개요: 과적합(Overfitting)을 막는 방법들

**과적합**은 모델이 훈련 데이터를 지나치게 암기해 새로운 데이터에 대한 성능이 떨어지는 현상. 모델이 훈련 데이터의 노이즈까지 학습했기 때문. 과적합을 막는 여러 방법이 있음.

-----

### 1\. 데이터 증강(Data Augmentation)

**데이터 증강**은 데이터의 양이 적을 때 기존 데이터를 변형하여 학습 데이터의 양을 인위적으로 늘리는 방법. 이미지의 경우 회전, 노이즈 추가 등으로, 텍스트의 경우 역번역 등을 사용.

-----

### 2\. 모델 복잡도 줄이기

모델의 복잡도는 은닉층의 수나 매개변수(가중치)의 수로 결정. 과적합이 발생하면 모델의 복잡도를 줄여 일반화 성능을 높이는 조치. 모델의 매개변수 수를 수용력(capacity)이라고도 함.

-----

### 3\. 가중치 규제(Regularization)

복잡한 모델을 간단하게 만드는 방법 중 하나. **가중치 규제**는 모델의 비용 함수에 가중치 값을 작게 만드는 항을 추가.

  * **L1 규제**: 가중치들의 절대값 합을 비용 함수에 추가. 일부 가중치를 0으로 만들어 특정 특성이 모델에 미치는 영향을 제거.
  * **L2 규제**: 가중치들의 제곱 합을 비용 함수에 추가. 가중치 값을 0에 가깝게 만들지만, 완전히 0으로 만들지는 않음. 일반적으로 L2 규제가 더 잘 작동.

-----

### 4\. 드롭아웃(Dropout)

**드롭아웃**은 학습 과정에서 신경망의 일부 뉴런을 랜덤으로 사용하지 않는 방법. 학습 시에만 적용하고 예측 시에는 사용하지 않음. 모델이 특정 뉴런에 의존하는 것을 방지하고, 여러 신경망을 앙상블한 효과를 내어 과적합을 방지.

#### 💡 케라스(Keras) 드롭아웃 예제

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense

max_words = 10000
num_classes = 46

model = Sequential()
model.add(Dense(256, input_shape=(max_words,), activation='relu'))
model.add(Dropout(0.5))  # 50%의 비율로 드롭아웃 추가
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))  # 50%의 비율로 드롭아웃 추가
model.add(Dense(num_classes, activation='softmax'))
```


---

>### 딥러닝 개요: 기울기 소실(Gradient Vanishing)과 폭주(Exploding)

딥러닝 모델 학습 시 역전파 과정에서 발생하는 문제.

* **기울기 소실(Gradient Vanishing)**: 입력층으로 갈수록 기울기(경사)가 점차 작아져 가중치 업데이트가 제대로 이루어지지 않는 현상. 주로 시그모이드나 tanh 같은 활성화 함수를 깊은 신경망에서 사용할 때 발생.
* **기울기 폭주(Gradient Exploding)**: 기울기가 비정상적으로 커져 가중치들이 발산하는 현상.

---

### 1. 활성화 함수 사용

기울기 소실을 완화하는 가장 간단한 방법은 은닉층에서 시그모이드나 tanh 대신 **ReLU**나 그 변형 함수들을 사용. ReLU는 양수 입력에 대해 기울기가 1이므로 기울기 소실 문제 해결에 효과적. **Leaky ReLU**는 음수 입력에 대해서도 작은 기울기를 반환하여 `죽은 렐루` 문제를 해결.

---

### 2. 그래디언트 클리핑(Gradient Clipping)

**그래디언트 클리핑**은 기울기 값이 특정 임계치를 넘지 않도록 자르는 기법. 기울기 폭주를 방지하는 데 유용하며, RNN에서 특히 효과적.

---

### 3. 가중치 초기화(Weight Initialization)

가중치 초기값 설정에 따라 모델의 학습 결과가 달라질 수 있음. 적절한 초기화는 기울기 소실/폭주 문제를 완화.

* **세이비어 초기화(Xavier Initialization)**: 시그모이드나 tanh 함수에 적합. 이전 층과 다음 층의 뉴런 수를 고려해 가중치를 초기화.
* **He 초기화(He Initialization)**: ReLU 계열 함수에 적합. 주로 이전 층의 뉴런 수를 고려해 초기화.

---

### 4. 배치 정규화(Batch Normalization)

**배치 정규화**는 각 층의 입력값을 정규화하여 학습을 안정시키고 빠르게 만드는 방법. **내부 공변량 변화(Internal Covariate Shift)**, 즉 학습 과정에서 층별 입력 데이터 분포가 달라지는 문제를 해결.

* **효과**: 기울기 소실 문제 개선, 가중치 초기화에 덜 민감해짐, 학습률을 높일 수 있어 학습 속도 향상, 과적합 방지.
* **한계**: 미니 배치 크기에 의존적이고, RNN에 적용하기 어려움.

---

### 5. 층 정규화(Layer Normalization)

배치 정규화의 한계를 극복하기 위해 등장. 배치 크기에 의존하지 않고, RNN에도 쉽게 적용할 수 있음.

---

>### 딥러닝 개요: 케라스(Keras)

**케라스**는 딥러닝 모델을 쉽게 만들 수 있는 파이썬 라이브러리. 사용이 직관적인 상위 레벨의 인터페이스를 제공.

---

### 1. 전처리 도구

* **`Tokenizer()`**: 텍스트를 토큰화하고 각 단어를 고유한 정수(인덱스)로 변환.
    * `fit_on_texts`: 단어 빈도수를 기반으로 단어 집합 생성.
    * `texts_to_sequences`: 텍스트를 정수 시퀀스로 변환.

* **`pad_sequences()`**: 길이가 다른 시퀀스(문장)들을 동일한 길이로 맞추는 패딩 작업 수행.

---

### 2. 워드 임베딩(Word Embedding)

**워드 임베딩**은 단어를 밀집 벡터(dense vector)로 표현하는 방법. 단어 간 의미적 유사도를 벡터 공간에 반영.

* **`Embedding()`**: 정수 인코딩된 단어를 밀집 벡터로 변환하는 층(layer). 모델의 첫 번째 층으로 사용.

| | 원-핫 벡터 | 임베딩 벡터 |
|---|---|---|
| 차원 | 고차원 (단어 집합 크기) | 저차원 (보통 256, 512 등) |
| 표현 | 0과 1로 이루어진 희소 벡터 | 실수값을 가진 밀집 벡터 |
| 생성 | 수동 | 학습을 통해 자동 생성 |

---

### 3. 모델링(Modeling)

케라스의 `Sequential()`은 딥러닝 모델의 층을 쌓는 기본적인 방법. `add()` 함수를 이용해 층을 하나씩 추가.

* **`Dense()`**: 모든 뉴런이 이전 층의 모든 뉴런과 연결된 **전결합층(Fully-connected layer)**.
    * 첫 번째 인자는 출력 뉴런 수.
    * `input_dim`: 입력 뉴런 수 (첫 층에만 지정).
    * `activation`: 사용할 활성화 함수. ('relu', 'sigmoid', 'softmax' 등)
    

---

### 4. 컴파일(Compile)과 훈련(Training)

* **`compile()`**: 모델 학습 전에 손실 함수, 옵티마이저, 평가 지표를 설정하는 단계.
    * `optimizer`: 가중치 업데이트 알고리즘. (예: 'adam', 'rmsprop')
    * `loss`: 손실 함수. ('binary_crossentropy', 'categorical_crossentropy' 등)
    * `metrics`: 모델 성능을 모니터링할 지표. (예: 'acc')

* **`fit()`**: 모델을 훈련시키는 함수.
    * `epochs`: 전체 데이터를 학습할 횟수.
    * `batch_size`: 한 번에 처리할 데이터의 양.
    * `validation_data` 또는 `validation_split`: 훈련 중 모델의 과적합 여부 확인.

---

### 5. 평가(Evaluation)와 예측(Prediction)

* **`evaluate()`**: 테스트 데이터를 사용해 모델의 최종 성능을 평가.
* **`predict()`**: 새로운 입력에 대한 모델의 예측값을 얻음.

---

### 6. 모델 저장(Save)과 불러오기(Load)

* **`save()`**: 학습된 모델의 구조와 가중치를 파일로 저장.
* **`load_model()`**: 저장된 모델을 다시 불러와 사용.




## ch.8