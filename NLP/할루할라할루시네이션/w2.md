# 딥러닝을 이용한 자연어 처리 입문

# ch.9 워드 임베딩

> ### 9-01 워드 임베딩 (Word Embedding)

워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현(Dense Representation)으로 변환하는 기법.
희소 표현, 밀집 표현, 그리고 워드 임베딩 개념을 이해하는 것이 핵심.

---

### 1. 희소 표현 (Sparse Representation)

* **원-핫 인코딩(One-Hot Encoding)**에서 얻어진 벡터는 대부분이 0으로 채워진 벡터 → **희소 벡터(Sparse Vector)**.
* 단어 집합의 크기가 커질수록 벡터 차원도 커짐 → **차원의 저주** 발생.
* 예시: 단어 집합이 10,000개일 때, `강아지`라는 단어의 인덱스가 4라면:

  ```
  강아지 = [0 0 0 0 1 0 0 0 ... 0]  # 길이 10,000
  ```
* 문제점:

  * 차원이 너무 커서 **공간적 낭비** 발생.
  * 단어 간의 **의미적 유사성**을 전혀 반영하지 못함.
  * DTM(Document-Term Matrix) 같은 표현도 대부분 0으로 채워진 **희소 행렬(Sparse Matrix)**이 됨.

---

### 2. 밀집 표현 (Dense Representation)

* 희소 표현과 반대되는 개념.
* **차원을 단어 집합 크기로 정하지 않고**, 사용자가 정한 고정된 크기로 축소 (예: 128차원).
* 벡터의 원소가 **실수값**으로 채워져 단어 간 의미적 차이를 반영할 수 있음.

예시:

```
강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ...]  # 차원: 128
```

* 이렇게 조밀하게 표현된 벡터를 **밀집 벡터(Dense Vector)**라고 함.

---

### 3. 워드 임베딩 (Word Embedding)

* 단어를 **밀집 벡터**로 표현하는 방법.
* 워드 임베딩 과정을 거쳐 학습된 벡터를 **임베딩 벡터(Embedding Vector)**라고 부름.

#### 주요 방법론

* **LSA (Latent Semantic Analysis)**
* **Word2Vec**
* **FastText**
* **GloVe**

#### 케라스(Keras) `Embedding()` 층

* Word2Vec, GloVe 등을 직접 사용하지 않음.
* 단어를 **랜덤한 밀집 벡터**로 초기화한 뒤, 신경망 학습 과정에서 벡터 값을 함께 학습.
* 즉, 신경망의 **가중치 학습 방식**으로 단어 벡터를 학습.

---

### 4. 원-핫 벡터 vs. 임베딩 벡터

| 구분    | 원-핫 벡터         | 임베딩 벡터           |
| ----- | -------------- | ---------------- |
| 차원    | 고차원 (단어 집합 크기) | 저차원 (128, 256 등) |
| 표현 유형 | 희소 벡터          | 밀집 벡터            |
| 생성 방식 | 수동             | 데이터 학습을 통해 자동    |
| 값의 타입 | 0과 1           | 실수               |

> 📌 `Embedding()`을 사용하는 것과 Word2Vec, GloVe 등의 사전 훈련 임베딩을 사용하는 차이는 **사전 훈련된 워드 임베딩 파트**에서 다룸.

---

> ### 9-02 Word2Vec


### 0. Word2Vec 개요

* **원-핫 벡터**의 문제: 단어 간 유의미한 유사도 계산 불가.
* **Word2Vec**: 단어의 의미를 벡터 공간에 수치화하여 **단어 간 유사도**를 반영하는 임베딩 방법.
* 활용 예시:

  ```
  한국 - 서울 + 도쿄 ≈ 일본
  박찬호 - 야구 + 축구 ≈ 호나우두
  ```
* 이런 연산이 가능한 이유: Word2Vec 벡터는 **의미적 유사성을 반영**하여 학습되었기 때문.

---

### 1. 희소 표현 → 분산 표현

* **희소 표현 (Sparse Representation)**: 원-핫 벡터, DTM처럼 대부분이 0으로 채워짐 → 의미 정보 부족.
* **분산 표현 (Distributed Representation)**:

  * 기반 가정: **분포 가설 (Distributional Hypothesis)**
    → *비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다*
  * 단어의 의미를 **저차원의 여러 차원**에 분산하여 표현.
  * 예:

    ```
    강아지 (원-핫) = [0 0 0 0 1 0 ... 0]  # 10,000차원
    강아지 (Word2Vec) = [0.2, 0.3, 0.5, 0.7, ...]  # 128차원
    ```

---

### 2. Word2Vec 학습 방식

Word2Vec에는 **CBOW**와 **Skip-Gram** 두 가지 학습 방식이 존재.

---

#### (1) CBOW (Continuous Bag of Words)

* **아이디어**: 주변 단어(Context Words) → 중심 단어(Center Word) 예측.

* 예문: `"The fat cat sat on the mat"`

  * 중심 단어: `sat`
  * 주변 단어: `fat, cat, on, the` (윈도우 크기=2)

* **슬라이딩 윈도우**: 문장을 움직이며 `(주변 단어, 중심 단어)` 쌍 데이터셋 생성.

* **구조**:

  * 입력: 주변 단어들의 원-핫 벡터
  * 투사층 (Projection Layer): 가중치 행렬 `W`와 곱 → 각 단어의 임베딩 벡터 추출 (룩업 테이블)
  * 평균 → 하나의 벡터로 결합
  * 출력층: 소프트맥스 → 중심 단어 확률 분포 예측
  * 손실 함수: **크로스 엔트로피 (Cross-Entropy)**

* **특징**:

  * 은닉층 없음 (Shallow NN, 얕은 신경망)
  * 투사층의 크기 `M` = 임베딩 벡터 차원

---

#### (2) Skip-Gram

* **아이디어**: 중심 단어(Center Word) → 주변 단어(Context Words) 예측.

* 예문: `"The fat cat sat on the mat"`

  * 중심 단어: `sat`
  * 예측해야 할 주변 단어: `fat, cat, on, the`

* **구조**:

  * 입력: 중심 단어의 원-핫 벡터
  * 투사층: 임베딩 벡터 추출
  * 출력층: 소프트맥스 → 주변 단어 확률 분포 예측

* **차이점**: 벡터 평균을 구하지 않음 (중심 단어 하나만 입력).

* **성능**: 일반적으로 Skip-Gram이 CBOW보다 더 성능이 좋다고 알려져 있음.

---

### 3. NNLM vs. Word2Vec

* **NNLM (Neural Network Language Model)**

  * 목적: *다음 단어 예측*
  * 구조: 투사층 + 은닉층 + 출력층
  * 학습 속도: 느림

* **Word2Vec**

  * 목적: *워드 임베딩 자체 학습*
  * 구조: 투사층 → 바로 출력층 (은닉층 제거)
  * 예측 대상: 중심 단어 (CBOW) 또는 주변 단어 (Skip-Gram)
  * 학습 속도: 빠름 (은닉층 제거 + 최적화 기법 적용)

#### 최적화 기법

* **계층적 소프트맥스 (Hierarchical Softmax)**
* **네거티브 샘플링 (Negative Sampling)**
  → 학습 효율을 크게 향상.

---

### 4. Word2Vec의 핵심

* 원-핫 벡터의 한계를 극복 → **의미 기반 벡터 표현** 가능.
* 분포 가설에 기반한 **분산 표현** 학습.
* **CBOW**: 주변 단어 → 중심 단어
* **Skip-Gram**: 중심 단어 → 주변 단어
* **NNLM 개선판**으로, 더 빠르고 효율적으로 워드 임베딩을 학습.

---

>### 📌 9_03,04 생략~

---

> ### 9-05 GloVe (Global Vectors for Word Representation)

---

### 0. GloVe 개요

* **개발**: 2014년, 스탠포드대학
* **방식**: 카운트 기반(LSA) + 예측 기반(Word2Vec)을 결합한 **하이브리드 임베딩 방법론**
* **목표**: LSA와 Word2Vec 각각의 한계 보완
* **성능**: Word2Vec과 비슷하게 우수, 데이터 특성에 따라 둘 다 사용해보고 선택하는 것이 바람직

---

### 1. 기존 방법론의 한계

* **LSA (Latent Semantic Analysis)**

  * 카운트 기반 (DTM, TF-IDF)
  * 전체 통계 정보 활용
  * 하지만 **단어 유추(Analogy Task)**에는 약함 (ex: 왕 - 남자 + 여자 ≈ 여왕 잘 못함)

* **Word2Vec**

  * 예측 기반 (CBOW, Skip-Gram)
  * 단어 유추 작업에는 강함
  * 그러나 **윈도우 내 국소적 문맥만** 반영, 코퍼스 전체 통계는 반영 불가

➡ **GloVe**: 카운트 기반 + 예측 기반의 장점 모두 활용

---

### 2. 윈도우 기반 동시 등장 행렬 (Co-occurrence Matrix)

* 행/열 = 단어 집합의 단어들
* 셀 값 = 특정 단어 i 주변(윈도우 크기 내)에 단어 k가 등장한 횟수

예시 (윈도우 크기=1):

| Count    | I | like | enjoy | deep | learning | NLP | flying |
| -------- | - | ---- | ----- | ---- | -------- | --- | ------ |
| I        | 0 | 2    | 1     | 0    | 0        | 0   | 0      |
| like     | 2 | 0    | 0     | 1    | 0        | 1   | 0      |
| enjoy    | 1 | 0    | 0     | 0    | 0        | 0   | 1      |
| deep     | 0 | 1    | 0     | 0    | 1        | 0   | 0      |
| learning | 0 | 0    | 0     | 1    | 0        | 0   | 0      |
| NLP      | 0 | 1    | 0     | 0    | 0        | 0   | 0      |
| flying   | 0 | 0    | 1     | 0    | 0        | 0   | 0      |

* 대칭 행렬 (Transpose 해도 동일)
* 이유: i 주변에 k가 등장한 횟수 = k 주변에 i가 등장한 횟수

---

### 3. 동시 등장 확률 (Co-occurrence Probability)

* 정의:

  ```
  P(k | i) = i가 등장했을 때, k가 등장할 확률
  ```
* 계산 방법:

  ```
  P(k | i) = (동시 등장 횟수 Xik) / (단어 i의 전체 등장 횟수 Xi)
  ```
* 확률 비율을 통해 단어 관계 파악 가능

예시 (논문 예시 단순화):

| k   | solid  | gas | water | fashion |    |
| --- | ------ | --- | ----- | ------- | -- |
| P(k | ice)   | 크다  | 작다    | 크다      | 작다 |
| P(k | steam) | 작다  | 크다    | 크다      | 작다 |
| 비율  | 크다     | 작다  | ≈1    | ≈1      |    |

* **ice-solid**: 큰 비율 (연관 강함)
* **ice-gas**: 작은 비율 (steam과 더 연관)
* **ice-water**: 1에 가까움 (둘 다 관련 많음)
* **ice-fashion**: 1에 가까움 (둘 다 거의 무관)

---

### 4. 손실 함수 (Loss Function)

* GloVe의 아이디어:

  > **임베딩 벡터 내적 ≈ 동시 등장 확률 (또는 그 비율)의 로그 값**

* 기본식:

  ```
  wi · wj + bi + bj ≈ log(Xij)
  ```

  * `wi`: 중심 단어 임베딩 벡터
  * `wj`: 주변 단어 임베딩 벡터
  * `bi, bj`: 편향 항
  * `Xij`: 동시 등장 횟수

* 최종 손실 함수:

  ```
  J = Σ f(Xij) * (wi · wj + bi + bj - log(Xij))^2
  ```

  * `f(Xij)`: 가중치 함수 → 자주 등장하는 단어 쌍은 높은 가중치, 드문 단어 쌍은 낮은 가중치
  * 단, 불용어(예: it, the)처럼 지나치게 자주 등장하는 단어에는 과도한 가중치 X

---

### 5. 실습 

```python
from glove import Corpus, Glove

# 동시 등장 행렬 생성
corpus = Corpus()
corpus.fit(result, window=5)

# GloVe 모델 학습
glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

# 유사 단어 확인
print(glove.most_similar("man"))
# [('woman', 0.96), ('guy', 0.88), ('girl', 0.86), ('kid', 0.83)]
```

---

### 6. 핵심 요약

* **LSA(카운트 기반)** + **Word2Vec(예측 기반)** = **GloVe**
* **전체 코퍼스 통계 정보**와 **국소 문맥 정보**를 모두 반영
* 손실 함수: 임베딩 벡터 내적 ≈ 동시 등장 확률의 로그
* Word2Vec만큼 강력, 실제 적용 시 두 가지 모두 실험 후 선택

---


> ### 9-06 FastText (Facebook AI, 2016)

---

### 0. FastText 개요

* 개발: **Facebook AI Research** (2016)
* Word2Vec의 확장판 → **단어 내부의 subword(n-gram) 구조까지 학습**
* 장점:

  1. **OOV(Out-of-Vocabulary) 문제 해결**
  2. **희귀 단어(Rare word) 임베딩 개선**
  3. **오타/노이즈 데이터에 강함**

---

### 1. 내부 단어(Subword) 학습

* Word2Vec: 단어를 **쪼갤 수 없는 최소 단위**로 취급
* FastText: 단어를 글자 기반 **n-gram subword**들의 집합으로 취급

예시 (n=3, trigram)

```
apple → <ap, app, ppl, ple, le>, <apple>
```

* 실제로는 n의 최소값/최대값을 정해 학습 (기본: 3 ~ 6)
* 단어 벡터 = 내부 subword 벡터들의 **합**

```
apple = Σ(subword embeddings)
```

---

### 2. OOV(Out of Vocabulary) 문제 대응

* Word2Vec/GloVe → 학습에 없는 단어는 처리 불가
* FastText → subword 단위로 학습했기 때문에 **모르는 단어도 분해 후 벡터 생성 가능**

예:

* OOV 단어: `birthplace`
* 내부 subword: `birth`, `place`
* 두 subword 임베딩으로 합성하여 `birthplace` 벡터 생성

---

### 3. 희귀 단어(Rare Word) 대응

* Word2Vec: 등장 횟수 적으면 잘 학습되지 않음
* FastText:

  * 희귀 단어라도 **n-gram이 다른 단어와 공유**되면 의미 학습 가능
  * 오타(`appple`) 같은 경우도 대부분 동일한 subword 포함 → **임베딩 강건성 유지**

---

### 4. Word2Vec vs FastText 실습 비교

```python
# Word2Vec
model.wv.most_similar("electrofishing")
# KeyError: "word 'electrofishing' not in vocabulary"

# FastText
from gensim.models import FastText
model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)

model.wv.most_similar("electrofishing")
# [('electrolux', 0.79), ('electrolyte', 0.78), ('electro', 0.77), ...]
```

* Word2Vec → OOV 단어 처리 불가
* FastText → 유사 단어 탐색 가능 (subword 기반)

---

### 5. 한국어에서의 FastText

#### (1) 음절 단위

* 예: "자연어처리", n=3

```
<자연, 자연어, 연어처, 어처리, 처리>
```

#### (2) 자모 단위 (초성/중성/종성 분해)

* 예: "자연어처리" → ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _
* n=3 적용 시:

```
<ㅈㅏ_, ㅏ_ㅇ, ㅇㅕㄴ, ...>
```

* 장점: **오타·노이즈에도 강한 임베딩 가능**

---

### 6. 핵심 요약

* FastText = Word2Vec + Subword 학습
* 장점:
  ✅ OOV 단어 처리
  ✅ 희귀 단어/오타 대응
  ✅ 한국어 등 형태소적 특성이 강한 언어에서 효과적
* 실제 NLP 현업에서도 **노이즈 많은 데이터셋 처리**에 강력

---

> ### 9-08 사전 훈련된 워드 임베딩 (Pre-trained Word Embedding)

---

### 0. 개요

* **케라스 임베딩 층 (Embedding layer)**

  * 훈련 데이터에서 단어를 정수 인코딩 → 랜덤 초기화된 벡터 → 학습 과정에서 갱신
  * 장점: 작업(task)에 맞게 임베딩 벡터 최적화
  * 단점: 데이터가 적으면 일반화가 잘 안됨

* **사전 훈련된 임베딩 (Pre-trained Embedding)**

  * 방대한 코퍼스(Wikipedia, Google News 등)로 미리 학습된 벡터 (Word2Vec, GloVe, FastText 등) 불러오기
  * 장점: 적은 데이터에서도 성능 개선
  * 단점: 현재 작업 특화 정보는 부족할 수 있음

---

### 1. 케라스 임베딩 층 (Keras Embedding Layer)

* `Embedding()` 함수 사용
* 입력: 정수 인코딩된 시퀀스
* 출력: **임베딩 벡터 시퀀스 (3D 텐서)**

```python
from tensorflow.keras.layers import Embedding

vocab_size = 20000   # 단어 집합 크기
output_dim = 128     # 임베딩 차원
input_length = 500   # 입력 시퀀스 길이

embedding_layer = Embedding(vocab_size, output_dim, input_length=input_length)
```

* 동작 방식: **룩업 테이블(Lookup Table)**

  * 정수 인덱스 → 해당 행(임베딩 벡터) 반환
  * 역전파 시 가중치(임베딩 벡터 값)도 학습

---

### 2. 케라스 임베딩 층 실습

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

# 데이터 준비
sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd',
             'excellent work', 'supreme quality', 'bad', 'highly respectable']
y_train = [1, 0, 0, 1, 1, 0, 1]

# 토크나이저
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
vocab_size = len(tokenizer.word_index) + 1

# 정수 인코딩 + 패딩
X_encoded = tokenizer.texts_to_sequences(sentences)
max_len = max(len(l) for l in X_encoded)
X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')
y_train = np.array(y_train)

# 모델 설계
embedding_dim = 4
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.fit(X_train, y_train, epochs=100, verbose=2)
```

---

### 3. 사전 훈련된 워드 임베딩 활용

#### (1) GloVe

```python
embedding_dict = {}
f = open('glove.6B.100d.txt', encoding="utf8")

for line in f:
    word_vector = line.split()
    word = word_vector[0]
    vector = np.asarray(word_vector[1:], dtype='float32')
    embedding_dict[word] = vector
f.close()

# 임베딩 행렬 생성
embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    vector = embedding_dict.get(word)
    if vector is not None:
        embedding_matrix[index] = vector

# 모델 적용
e = Embedding(vocab_size, 100, weights=[embedding_matrix],
              input_length=max_len, trainable=False)
```

---

#### (2) Word2Vec (Google News, 3M words, 300-dim)


```python
import gensim
word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(
    'GoogleNews-vectors-negative300.bin.gz', binary=True
)

# 임베딩 행렬 생성
embedding_matrix = np.zeros((vocab_size, 300))
for word, index in tokenizer.word_index.items():
    if word in word2vec_model:
        embedding_matrix[index] = word2vec_model[word]

# 모델 적용
e = Embedding(vocab_size, 300, weights=[embedding_matrix],
              input_length=max_len, trainable=False)
```

---

### 4. 핵심 요약

| 구분      | 케라스 Embedding() | 사전 훈련된 Embedding         |
| ------- | --------------- | ------------------------ |
| 데이터 의존성 | 현재 데이터에 맞게 학습   | 대규모 코퍼스 기반               |
| 장점      | 특정 task에 최적화    | 적은 데이터에서도 성능 개선          |
| 단점      | 데이터 부족 시 일반화 약함 | task 특화 부족               |
| 활용 예시   | 감성 분류, 태깅 등     | 전이 학습(Transfer Learning) |

---


좋습니다 👍
이번에는 **ch.9-09 ELMo (Embeddings from Language Model)** 내용을 깃허브 마크다운 스타일로 정리해줄게요.

---

# ch.9 워드 임베딩 (Word Embedding)

> ### 9-09 ELMo (Embeddings from Language Model)

---

## 0. 개요

* **ELMo**: 2018년 제안된 **문맥 기반 워드 임베딩 (Contextualized Word Embedding)**
* 기존 Word2Vec, GloVe → **단어별 고정 벡터**
* ELMo → **문맥(Context)에 따라 단어 임베딩이 달라짐**
* 이름 의미: *Embeddings from Language Model*
* 기반 모델: **biLM (Bidirectional Language Model)**
* 구현: TensorFlow Hub 제공 (단, TF2.x에서는 사용 불가 → TF1.x 필요)

---

## 1. 한계점과 등장 배경

* Word2Vec, GloVe 문제점:

  * `bank` (은행 vs 강둑) 같은 다의어 구분 불가
  * 동일한 고정 벡터 사용

* 해결책:

  * **ELMo는 문맥을 반영**
  * 같은 단어라도 문장 맥락에 따라 다른 벡터 생성

---

## 2. biLM (Bidirectional Language Model)

* **언어 모델링 (Language Modeling)**:

  * 다음 단어 예측 (순방향)
  * 이전 단어 예측 (역방향)

* **ELMo의 biLM**:

  * 순방향 RNN + 역방향 RNN **동시에 학습**
  * 다층 구조 (multi-layer)
  * 입력: **문자 임베딩 (character-level CNN)**

    * subword 정보 반영
    * OOV 단어에도 강건함

⚠️ 주의:

* 일반적인 Bi-RNN = 은닉 상태를 concat
* ELMo의 biLM = 순/역 모델을 별도로 학습 후 활용

---

## 3. ELMo 표현 (Representation)

ELMo 벡터 생성 과정:

1. **biLM 각 층의 출력값** (임베딩 층 + 은닉층들) 수집
2. **순방향 + 역방향 출력 concat**
3. 층마다 학습 가능한 가중치 부여

   * ( s_1, s_2, s_3 )
4. 가중합(Weighted Sum)
5. 스칼라 파라미터 ( \gamma ) 곱함

📌 최종:
[
ELMo = \gamma \sum_i s_i h_i
]

* 여기서 ( h_i ) = 각 층의 출력값

* **기존 임베딩 (예: GloVe) + ELMo 벡터 concat → downstream task 입력으로 사용 가능**


---

## 4. 요약

| 특징     | Word2Vec/GloVe | ELMo                |
| ------ | -------------- | ------------------- |
| 벡터 유형  | 정적 (Static)    | 동적 (Contextualized) |
| 문맥 반영  | X              | O                   |
| 다의어 구분 | 불가능            | 가능                  |
| 입력 단위  | 단어             | 문자 임베딩 기반           |
| 장점     | 간단, 빠름         | 문맥 정보, OOV 강건       |

---
# ch.12 태깅

좋은 포인트야 👍 앞부분에 “태깅이란 무엇인지 → 왜 다-대-다 구조인지 → 왜 양방향 RNN이 필요한지”라는 **스토리 흐름**을 붙이면 훨씬 자연스럽게 정리돼. 방금 답변을 보강해서 깔끔하게 다시 정리해줄게.

---

# ch.12 태깅 작업 (Tagging Task)

> ### 12-01 케라스를 이용한 태깅 작업 개요 (Tagging Task using Keras)

---

## 0. 스토리 & 개요

자연어 처리(NLP)에는 크게 두 가지 유형의 문제가 있다:

* **문장 단위 예측 (다-대-일, Many-to-One)**
  → 입력 시퀀스 전체가 하나의 출력(레이블)로 매핑
  → 예: 감성 분석, 스팸 메일 분류

* **단어 단위 예측 (다-대-다, Many-to-Many)**
  → 입력 시퀀스의 각 단어가 개별 레이블과 매핑
  → 예: 품사 태깅, 개체명 인식

이번 챕터의 **태깅 작업(Tagging Task)** 은 후자에 해당한다.
즉, 문장의 각 단어마다 "품사"나 "개체 유형"을 붙여주는 문제다.

* 따라서 태깅은 **시퀀스 레이블링(Sequence Labeling)** 문제의 대표 사례이며, **RNN의 다-대-다 구조**가 필요하다.
* 또한 단어의 품사나 개체는 앞뒤 문맥 모두에 의존하므로 **양방향 RNN(Bidirectional RNN)** 이 효과적이다.

---

## 1. 훈련 데이터 구조

* 입력(X): 토큰화된 문장 (단어 시퀀스)
* 출력(y): 각 단어에 해당하는 태그 시퀀스
* 특징: **X와 y는 병렬 구조이며 길이가 동일**

예시 (개체명 인식, NER 데이터):

| idx | X_train (단어 시퀀스)                                                          | y_train (태그 시퀀스)                                         | 길이 |
| --- | ------------------------------------------------------------------------- | -------------------------------------------------------- | -- |
| 0   | `['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']` | `['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']` | 8  |
| 1   | `['peter', 'blackburn']`                                                  | `['B-PER', 'I-PER']`                                     | 2  |
| 2   | `['brussels', '1996-08-22']`                                              | `['B-LOC', 'O']`                                         | 2  |
| 3   | `['The', 'European', 'Commission']`                                       | `['O', 'B-ORG', 'I-ORG']`                                | 3  |

 각 단어와 태그는 **1:1 pair** 관계

이후 과정: 정수 인코딩 → 패딩(Padding) → 딥러닝 입력

---

## 2. 시퀀스 레이블링 (Sequence Labeling)

* 정의: 입력 시퀀스 X의 각 단어에 대해, 대응되는 레이블 시퀀스 y를 생성하는 작업


* 대표 태스크:

  * 품사 태깅 (POS tagging)
  * 개체명 인식 (NER)

---

## 3. 다-대-일 vs 다-대-다

| 구조                       | 설명                   | 예시 태스크        |
| ------------------------ | -------------------- | ------------- |
| **다-대-일 (Many-to-One)**  | 입력 시퀀스 전체 → 단일 출력    | 문장 분류, 감성 분석  |
| **다-대-다 (Many-to-Many)** | 입력 시퀀스의 각 단어 → 개별 출력 | 품사 태깅, 개체명 인식 |

👉 태깅은 문장 내 **모든 단어에 레이블**을 달아야 하므로 **다-대-다 구조**가 필요하다.

---

## 4. 양방향 LSTM (Bidirectional LSTM)

* 단방향 RNN: 현재 단어 예측 시 과거 단어 정보만 사용
* 양방향 RNN: 현재 단어 예측 시 **과거 + 미래 문맥 모두 활용**

```python
from keras.layers import Bidirectional, LSTM

model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
```

* `hidden_units`: 은닉 상태 차원
* `return_sequences=True`: 모든 시점의 은닉 상태 출력 (태깅 필수 옵션)

---

## 5. 시각적 이해

### 단방향 LSTM (Many-to-One 예: 문장 분류)

```
w1 → w2 → w3 → w4 → [y]
```

👉 문장 전체를 보고 하나의 결과(감성 라벨 등)를 출력

---

### 단방향 LSTM (Many-to-Many 예: 태깅)

```
w1 → w2 → w3 → w4
y1   y2   y3   y4
```

👉 각 단어마다 레이블을 출력 (단, 과거 문맥만 반영)

---

### 양방향 LSTM (Many-to-Many 예: 태깅)

```
          → w1 → w2 → w3 → w4 →
Input: X ------------------------
          ← w1 ← w2 ← w3 ← w4 ←
Output:   y1   y2   y3   y4
```

👉 각 단어의 레이블 예측 시 **앞/뒤 문맥 모두 반영**

---



> ### 12-02 양방향 LSTM을 이용한 품사 태깅 (POS Tagging with Bi-LSTM)

---

## 0. 개요

* 품사 태깅(POS tagging): 문장의 각 단어에 대해 품사(명사, 동사, 형용사 등)를 부여하는 작업
* 기존에는 **NLTK, KoNLPy** 등을 이용해 태깅했지만, 이번에는 **딥러닝 모델**을 직접 구현
* 모델 구조: **양방향 LSTM (Bi-LSTM)** 기반의 시퀀스 레이블링 모델

---

## 1. 데이터 이해 및 전처리

```python
import nltk
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
```

* NLTK에서 품사 태깅이 된 영어 코퍼스 로드

```python
tagged_sentences = nltk.corpus.treebank.tagged_sents()
print("품사 태깅이 된 문장 개수: ", len(tagged_sentences))
# 3,914개
```

* 첫 번째 샘플 확인

```python
print(tagged_sentences[0])
# [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ... ]
```

* 단어와 태그를 분리

```python
sentences, pos_tags = [], [] 
for tagged_sentence in tagged_sentences:
    sentence, tag_info = zip(*tagged_sentence)
    sentences.append(list(sentence))
    pos_tags.append(list(tag_info))
```

---

## 2. 데이터 탐색

* 첫 번째 샘플

```python
print(sentences[0])
print(pos_tags[0])
```

* 문장 길이 분포 확인

```python
print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))
print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))
plt.hist([len(s) for s in sentences], bins=50)
plt.show()
```

* 최대 길이: 271
* 평균 길이: 약 25.7

👉 대부분 샘플은 길이가 150 이하 → 패딩 길이(max_len)를 150으로 설정

---

## 3. 정수 인코딩 및 패딩

```python
def tokenize(samples):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(samples)
    return tokenizer

src_tokenizer = tokenize(sentences)
tar_tokenizer = tokenize(pos_tags)

vocab_size = len(src_tokenizer.word_index) + 1
tag_size = len(tar_tokenizer.word_index) + 1
```

* 단어 집합 크기: 11,388

* 품사 태깅 집합 크기: 47

* 정수 인코딩

```python
X_train = src_tokenizer.texts_to_sequences(sentences)
y_train = tar_tokenizer.texts_to_sequences(pos_tags)
```

* 패딩 처리

```python
max_len = 150
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
y_train = pad_sequences(y_train, padding='post', maxlen=max_len)
```

* 데이터 분리

```python
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)
```

---

## 4. Bi-LSTM 모델 설계

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))

model.compile(loss='sparse_categorical_crossentropy',
              optimizer=Adam(0.001),
              metrics=['accuracy'])
```

---

## 5. 학습 및 평가

```python
model.fit(X_train, y_train, batch_size=128, epochs=7, validation_data=(X_test, y_test))
```

* 테스트 정확도 평가

```python
print("\n 테스트 정확도: %.4f" % (model.evaluate(X_test, y_test)[1]))
# 테스트 정확도: 0.9016
```

---

## 6. 결과 확인

```python
index_to_word = src_tokenizer.index_word
index_to_tag = tar_tokenizer.index_word

i = 10  # 샘플 인덱스
y_predicted = model.predict(np.array([X_test[i]]))
y_predicted = np.argmax(y_predicted, axis=-1)

print("{:15}|{:7}|{}".format("단어", "실제값", "예측값"))
print("*" * 40)

for word, tag, pred in zip(X_test[i], y_test[i], y_predicted[0]):
    if word != 0:
        print("{:15}: {:7} {}".format(index_to_word[word], 
                                      index_to_tag[tag].upper(), 
                                      index_to_tag[pred].upper()))
```



> ### 12-03 개체명 인식 (Named Entity Recognition, NER)

---

## 0. 개요

* **개체명 인식 (NER, Named Entity Recognition)**: 문장에서 단어가 어떤 의미를 가지는지를 분류하는 작업
* 예: 사람(Person), 장소(Location), 조직(Organization), 시간(Time) 등

예시 문장:
“유정이는 2018년에 골드만삭스에 입사했다.”

결과:

* 유정 → 사람 (PER)
* 2018년 → 시간 (TIME)
* 골드만삭스 → 조직 (ORG)

---

## 1. 개체명 인식이란?

* **Named Entity**: 이름을 가진 개체 (사람, 장소, 조직 등)
* **NER의 목표**: 주어진 문장에서 특정 단어가 어떤 개체 유형에 속하는지 판별

NER은 대표적인 **시퀀스 레이블링(Sequence Labeling) 문제**로, 단어 단위로 라벨을 부여하는 형태다.

---

## 2. NLTK를 이용한 NER 실습

NLTK는 기본적으로 **NER chunker**를 제공하므로, 손쉽게 개체명 인식을 실습할 수 있다.

```python
from nltk import word_tokenize, pos_tag, ne_chunk

sentence = "James is working at Disney in London"

# 1) 토큰화 + 품사 태깅
tokenized_sentence = pos_tag(word_tokenize(sentence))
print(tokenized_sentence)
```

출력:

```
[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), 
 ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]
```

* 단어별로 품사가 태깅됨

---

### 2-1. 개체명 인식 수행

```python
ner_sentence = ne_chunk(tokenized_sentence)
print(ner_sentence)
```

출력:

```
(S
  (PERSON James/NNP)
  is/VBZ
  working/VBG
  at/IN
  (ORGANIZATION Disney/NNP)
  in/IN
  (GPE London/NNP))
```

* James → PERSON (사람)
* Disney → ORGANIZATION (조직)
* London → GPE (지명/위치)

---

> ### 12-04 개체명 인식의 BIO 표현 이해하기

---

## 0. 개요

* 개체명 인식(NER)은 챗봇, QA 시스템, 검색 등에서 필수적인 **전처리 작업**
* 정확도를 높이려면 **도메인에 맞는 데이터셋**으로 직접 모델을 학습하는 것이 필요
* 본 절에서는 개체명 인식에서 가장 널리 쓰이는 **BIO 태깅 방식**을 설명

---

## 1. BIO 태깅 표현

* **B (Begin)** : 개체명 시작 단어
* **I (Inside)** : 개체명 내부 단어
* **O (Outside)** : 개체명이 아닌 단어

예시 (영화 제목 개체명 추출):

| 단어 | 태그 |
| -- | -- |
| 해  | B  |
| 리  | I  |
| 포  | I  |
| 터  | I  |
| 보  | O  |
| 러  | O  |
| 가  | O  |
| 자  | O  |

→ "해리포터"는 **B + I + I + I**, 그 외 단어는 O

---

## 2. 다중 개체 유형 태깅

실제 대화에는 여러 종류의 개체가 존재할 수 있다.
예: 영화(movie), 극장(theater)

| 단어 | 태그        |
| -- | --------- |
| 해  | B-movie   |
| 리  | I-movie   |
| 포  | I-movie   |
| 터  | I-movie   |
| 보  | O         |
| 러  | O         |
| 메  | B-theater |
| 가  | I-theater |
| 박  | I-theater |
| 스  | I-theater |
| 가  | O         |
| 자  | O         |

---

## 3. 개체명 인식 데이터 형식 (CONLL2003 예시)

전통적인 영어 데이터셋 **CONLL2003**은 다음과 같은 형식으로 제공됨:

```
EU        NNP   B-NP   B-ORG
rejects   VBZ   B-VP   O
German    JJ    B-NP   B-MISC
call      NN    I-NP   O
to        TO    B-VP   O
boycott   VB    I-VP   O
British   JJ    B-NP   B-MISC
lamb      NN    I-NP   O
.         .     O      O
```

형식:
`[단어] [품사 태깅] [청크 태깅] [개체명 태깅]`

* **품사 태깅**: Penn Treebank POS 태그 (예: NNP = 고유 명사 단수, VBZ = 3인칭 단수 동사 현재형)
* **개체명 태깅**: `LOC` (location), `ORG` (organization), `PER` (person), `MISC` (miscellaneous)

예:

* "EU" → B-ORG
* "German" → B-MISC
* "Peter Blackburn" → B-PER + I-PER

---

## 4. 데이터 전처리 과정 요약

* 데이터 파일: `train.txt`
* 각 문장은 빈 줄로 구분
* 각 단어는 `(단어, 개체명 태깅)` 형태로 저장
* zip()을 사용해 `sentences` (단어들)와 `ner_tags` (개체명 태깅) 분리

---

## 5. Bi-LSTM 기반 개체명 인식기

* 입력: 정수 인코딩된 단어 시퀀스 (패딩 포함)
* 출력: 시점별 개체명 태깅 (BIO 레이블)
* 모델 구조:

  * Embedding(mask_zero=True)
  * Bidirectional LSTM (return_sequences=True)
  * TimeDistributed(Dense(tag_size, softmax))
* 손실 함수: categorical_crossentropy
* 성능: 약 **95.7% 정확도**

---

## 6. 문제점 및 다음 단계

* 단순 정확도만 측정하면 'O' 비율이 높아 과대평가될 수 있음
* 해결책: **F1-score** 도입 (precision & recall 기반)

---


> ### 12-05 BiLSTM을 이용한 개체명 인식 (Named Entity Recognition, NER)

---

## 0. 개요

* 앞서 배운 **BIO 표현 방식**을 이용해 데이터셋 구성
* **양방향 LSTM (BiLSTM)** 모델로 개체명 인식 수행
* 기존에는 **정확도(accuracy)**로 평가했지만, 이번엔 **F1-score**로 성능을 검증

---

## 1. 데이터 준비 & 전처리

### 데이터셋

* 사용 데이터: 총 47,959개 문장, 단어 수 35,178개, 태그 수 17개

### 주요 전처리 단계

* `fillna(method='ffill')` → Sentence ID 누락 채우기
* 모든 단어 **소문자 변환**
* `(단어, 태그)` 쌍으로 묶어 문장 단위 리스트 생성
* `Tokenizer`로 단어 / 태그 정수 인코딩
* `pad_sequences`로 길이 70으로 패딩

---

## 2. BiLSTM 모델 구조

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
hidden_units = 256

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])
```

* **Embedding**: 단어 → 벡터 (mask_zero=True → PAD 무시)
* **BiLSTM**: 문맥을 양방향으로 학습
* **TimeDistributed(Dense)**: 각 시점별로 태그 예측 (다 대 다 구조)
* **Softmax + CrossEntropy**: 다중 클래스 분류

---

## 3. 학습 및 예측

* 배치 크기: 128
* Epoch: 6
* validation_split=0.1 (훈련 데이터 일부로 검증)
* 약 **95% 검증 정확도** 달성

예시 결과 (테스트 문장):

| 단어                | 실제값   | 예측값   |
| ----------------- | ----- | ----- |
| the               | O     | O     |
| statement         | O     | O     |
| u.n.              | B-ORG | B-ORG |
| secretary-general | I-ORG | I-ORG |
| kofi              | B-PER | B-PER |
| annan             | I-PER | I-PER |
| amman             | B-GEO | B-GEO |
| wednesday         | B-TIM | B-TIM |

→ 개체명을 정확하게 맞춤

---

## 4. 성능 평가: 왜 F1-score인가?

### 문제점

* 대부분의 토큰은 "O" (개체 아님)
* 따라서 단순 정확도는 과대평가됨

  * 예: 모든 토큰을 O로 예측해도 70% 이상 정확도

### 해결책: F1-score

* **정밀도(Precision)**: 특정 개체라고 예측한 것 중 맞춘 비율
* **재현율(Recall)**: 실제 특정 개체 중 맞춘 비율
* **F1-score**: 두 값을 조화평균

---

## 5. 실험 결과 (seqeval 사용)

```python
from seqeval.metrics import f1_score, classification_report
```

* 전체 F1-score: **78.5%**
* 세부 성능 예시:

| 개체 유형 | Precision | Recall | F1-score |
| ----- | --------- | ------ | -------- |
| geo   | 0.84      | 0.84   | 0.84     |
| gpe   | 0.96      | 0.94   | 0.95     |
| org   | 0.57      | 0.58   | 0.57     |
| per   | 0.73      | 0.70   | 0.71     |
| tim   | 0.84      | 0.85   | 0.84     |

→ 지명(geo, gpe), 시간(tim)은 성능이 높음
→ 조직(org), 인명(per) 인식은 상대적으로 낮음

---

