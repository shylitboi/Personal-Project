# ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸

# ch.9 ì›Œë“œ ì„ë² ë”©

> ### 9-01 ì›Œë“œ ì„ë² ë”© (Word Embedding)

ì›Œë“œ ì„ë² ë”©(Word Embedding)ì€ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë‹¨ì–´ë¥¼ ë°€ì§‘ í‘œí˜„(Dense Representation)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•.
í¬ì†Œ í‘œí˜„, ë°€ì§‘ í‘œí˜„, ê·¸ë¦¬ê³  ì›Œë“œ ì„ë² ë”© ê°œë…ì„ ì´í•´í•˜ëŠ” ê²ƒì´ í•µì‹¬.

---

### 1. í¬ì†Œ í‘œí˜„ (Sparse Representation)

* **ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)**ì—ì„œ ì–»ì–´ì§„ ë²¡í„°ëŠ” ëŒ€ë¶€ë¶„ì´ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë²¡í„° â†’ **í¬ì†Œ ë²¡í„°(Sparse Vector)**.
* ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ë²¡í„° ì°¨ì›ë„ ì»¤ì§ â†’ **ì°¨ì›ì˜ ì €ì£¼** ë°œìƒ.
* ì˜ˆì‹œ: ë‹¨ì–´ ì§‘í•©ì´ 10,000ê°œì¼ ë•Œ, `ê°•ì•„ì§€`ë¼ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ê°€ 4ë¼ë©´:

  ```
  ê°•ì•„ì§€ = [0 0 0 0 1 0 0 0 ... 0]  # ê¸¸ì´ 10,000
  ```
* ë¬¸ì œì :

  * ì°¨ì›ì´ ë„ˆë¬´ ì»¤ì„œ **ê³µê°„ì  ë‚­ë¹„** ë°œìƒ.
  * ë‹¨ì–´ ê°„ì˜ **ì˜ë¯¸ì  ìœ ì‚¬ì„±**ì„ ì „í˜€ ë°˜ì˜í•˜ì§€ ëª»í•¨.
  * DTM(Document-Term Matrix) ê°™ì€ í‘œí˜„ë„ ëŒ€ë¶€ë¶„ 0ìœ¼ë¡œ ì±„ì›Œì§„ **í¬ì†Œ í–‰ë ¬(Sparse Matrix)**ì´ ë¨.

---

### 2. ë°€ì§‘ í‘œí˜„ (Dense Representation)

* í¬ì†Œ í‘œí˜„ê³¼ ë°˜ëŒ€ë˜ëŠ” ê°œë….
* **ì°¨ì›ì„ ë‹¨ì–´ ì§‘í•© í¬ê¸°ë¡œ ì •í•˜ì§€ ì•Šê³ **, ì‚¬ìš©ìê°€ ì •í•œ ê³ ì •ëœ í¬ê¸°ë¡œ ì¶•ì†Œ (ì˜ˆ: 128ì°¨ì›).
* ë²¡í„°ì˜ ì›ì†Œê°€ **ì‹¤ìˆ˜ê°’**ìœ¼ë¡œ ì±„ì›Œì ¸ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ì°¨ì´ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆìŒ.

ì˜ˆì‹œ:

```
ê°•ì•„ì§€ = [0.2 1.8 1.1 -2.1 1.1 2.8 ...]  # ì°¨ì›: 128
```

* ì´ë ‡ê²Œ ì¡°ë°€í•˜ê²Œ í‘œí˜„ëœ ë²¡í„°ë¥¼ **ë°€ì§‘ ë²¡í„°(Dense Vector)**ë¼ê³  í•¨.

---

### 3. ì›Œë“œ ì„ë² ë”© (Word Embedding)

* ë‹¨ì–´ë¥¼ **ë°€ì§‘ ë²¡í„°**ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•.
* ì›Œë“œ ì„ë² ë”© ê³¼ì •ì„ ê±°ì³ í•™ìŠµëœ ë²¡í„°ë¥¼ **ì„ë² ë”© ë²¡í„°(Embedding Vector)**ë¼ê³  ë¶€ë¦„.

#### ì£¼ìš” ë°©ë²•ë¡ 

* **LSA (Latent Semantic Analysis)**
* **Word2Vec**
* **FastText**
* **GloVe**

#### ì¼€ë¼ìŠ¤(Keras) `Embedding()` ì¸µ

* Word2Vec, GloVe ë“±ì„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
* ë‹¨ì–´ë¥¼ **ëœë¤í•œ ë°€ì§‘ ë²¡í„°**ë¡œ ì´ˆê¸°í™”í•œ ë’¤, ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì—ì„œ ë²¡í„° ê°’ì„ í•¨ê»˜ í•™ìŠµ.
* ì¦‰, ì‹ ê²½ë§ì˜ **ê°€ì¤‘ì¹˜ í•™ìŠµ ë°©ì‹**ìœ¼ë¡œ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµ.

---

### 4. ì›-í•« ë²¡í„° vs. ì„ë² ë”© ë²¡í„°

| êµ¬ë¶„    | ì›-í•« ë²¡í„°         | ì„ë² ë”© ë²¡í„°           |
| ----- | -------------- | ---------------- |
| ì°¨ì›    | ê³ ì°¨ì› (ë‹¨ì–´ ì§‘í•© í¬ê¸°) | ì €ì°¨ì› (128, 256 ë“±) |
| í‘œí˜„ ìœ í˜• | í¬ì†Œ ë²¡í„°          | ë°€ì§‘ ë²¡í„°            |
| ìƒì„± ë°©ì‹ | ìˆ˜ë™             | ë°ì´í„° í•™ìŠµì„ í†µí•´ ìë™    |
| ê°’ì˜ íƒ€ì… | 0ê³¼ 1           | ì‹¤ìˆ˜               |

> ğŸ“Œ `Embedding()`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ Word2Vec, GloVe ë“±ì˜ ì‚¬ì „ í›ˆë ¨ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ì°¨ì´ëŠ” **ì‚¬ì „ í›ˆë ¨ëœ ì›Œë“œ ì„ë² ë”© íŒŒíŠ¸**ì—ì„œ ë‹¤ë£¸.

---

> ### 9-02 Word2Vec


### 0. Word2Vec ê°œìš”

* **ì›-í•« ë²¡í„°**ì˜ ë¬¸ì œ: ë‹¨ì–´ ê°„ ìœ ì˜ë¯¸í•œ ìœ ì‚¬ë„ ê³„ì‚° ë¶ˆê°€.
* **Word2Vec**: ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ìˆ˜ì¹˜í™”í•˜ì—¬ **ë‹¨ì–´ ê°„ ìœ ì‚¬ë„**ë¥¼ ë°˜ì˜í•˜ëŠ” ì„ë² ë”© ë°©ë²•.
* í™œìš© ì˜ˆì‹œ:

  ```
  í•œêµ­ - ì„œìš¸ + ë„ì¿„ â‰ˆ ì¼ë³¸
  ë°•ì°¬í˜¸ - ì•¼êµ¬ + ì¶•êµ¬ â‰ˆ í˜¸ë‚˜ìš°ë‘
  ```
* ì´ëŸ° ì—°ì‚°ì´ ê°€ëŠ¥í•œ ì´ìœ : Word2Vec ë²¡í„°ëŠ” **ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜**í•˜ì—¬ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸.

---

### 1. í¬ì†Œ í‘œí˜„ â†’ ë¶„ì‚° í‘œí˜„

* **í¬ì†Œ í‘œí˜„ (Sparse Representation)**: ì›-í•« ë²¡í„°, DTMì²˜ëŸ¼ ëŒ€ë¶€ë¶„ì´ 0ìœ¼ë¡œ ì±„ì›Œì§ â†’ ì˜ë¯¸ ì •ë³´ ë¶€ì¡±.
* **ë¶„ì‚° í‘œí˜„ (Distributed Representation)**:

  * ê¸°ë°˜ ê°€ì •: **ë¶„í¬ ê°€ì„¤ (Distributional Hypothesis)**
    â†’ *ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤*
  * ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ **ì €ì°¨ì›ì˜ ì—¬ëŸ¬ ì°¨ì›**ì— ë¶„ì‚°í•˜ì—¬ í‘œí˜„.
  * ì˜ˆ:

    ```
    ê°•ì•„ì§€ (ì›-í•«) = [0 0 0 0 1 0 ... 0]  # 10,000ì°¨ì›
    ê°•ì•„ì§€ (Word2Vec) = [0.2, 0.3, 0.5, 0.7, ...]  # 128ì°¨ì›
    ```

---

### 2. Word2Vec í•™ìŠµ ë°©ì‹

Word2Vecì—ëŠ” **CBOW**ì™€ **Skip-Gram** ë‘ ê°€ì§€ í•™ìŠµ ë°©ì‹ì´ ì¡´ì¬.

---

#### (1) CBOW (Continuous Bag of Words)

* **ì•„ì´ë””ì–´**: ì£¼ë³€ ë‹¨ì–´(Context Words) â†’ ì¤‘ì‹¬ ë‹¨ì–´(Center Word) ì˜ˆì¸¡.

* ì˜ˆë¬¸: `"The fat cat sat on the mat"`

  * ì¤‘ì‹¬ ë‹¨ì–´: `sat`
  * ì£¼ë³€ ë‹¨ì–´: `fat, cat, on, the` (ìœˆë„ìš° í¬ê¸°=2)

* **ìŠ¬ë¼ì´ë”© ìœˆë„ìš°**: ë¬¸ì¥ì„ ì›€ì§ì´ë©° `(ì£¼ë³€ ë‹¨ì–´, ì¤‘ì‹¬ ë‹¨ì–´)` ìŒ ë°ì´í„°ì…‹ ìƒì„±.

* **êµ¬ì¡°**:

  * ì…ë ¥: ì£¼ë³€ ë‹¨ì–´ë“¤ì˜ ì›-í•« ë²¡í„°
  * íˆ¬ì‚¬ì¸µ (Projection Layer): ê°€ì¤‘ì¹˜ í–‰ë ¬ `W`ì™€ ê³± â†’ ê° ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ (ë£©ì—… í…Œì´ë¸”)
  * í‰ê·  â†’ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ê²°í•©
  * ì¶œë ¥ì¸µ: ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ì¤‘ì‹¬ ë‹¨ì–´ í™•ë¥  ë¶„í¬ ì˜ˆì¸¡
  * ì†ì‹¤ í•¨ìˆ˜: **í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (Cross-Entropy)**

* **íŠ¹ì§•**:

  * ì€ë‹‰ì¸µ ì—†ìŒ (Shallow NN, ì–•ì€ ì‹ ê²½ë§)
  * íˆ¬ì‚¬ì¸µì˜ í¬ê¸° `M` = ì„ë² ë”© ë²¡í„° ì°¨ì›

---

#### (2) Skip-Gram

* **ì•„ì´ë””ì–´**: ì¤‘ì‹¬ ë‹¨ì–´(Center Word) â†’ ì£¼ë³€ ë‹¨ì–´(Context Words) ì˜ˆì¸¡.

* ì˜ˆë¬¸: `"The fat cat sat on the mat"`

  * ì¤‘ì‹¬ ë‹¨ì–´: `sat`
  * ì˜ˆì¸¡í•´ì•¼ í•  ì£¼ë³€ ë‹¨ì–´: `fat, cat, on, the`

* **êµ¬ì¡°**:

  * ì…ë ¥: ì¤‘ì‹¬ ë‹¨ì–´ì˜ ì›-í•« ë²¡í„°
  * íˆ¬ì‚¬ì¸µ: ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
  * ì¶œë ¥ì¸µ: ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ ì£¼ë³€ ë‹¨ì–´ í™•ë¥  ë¶„í¬ ì˜ˆì¸¡

* **ì°¨ì´ì **: ë²¡í„° í‰ê· ì„ êµ¬í•˜ì§€ ì•ŠìŒ (ì¤‘ì‹¬ ë‹¨ì–´ í•˜ë‚˜ë§Œ ì…ë ¥).

* **ì„±ëŠ¥**: ì¼ë°˜ì ìœ¼ë¡œ Skip-Gramì´ CBOWë³´ë‹¤ ë” ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŒ.

---

### 3. NNLM vs. Word2Vec

* **NNLM (Neural Network Language Model)**

  * ëª©ì : *ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡*
  * êµ¬ì¡°: íˆ¬ì‚¬ì¸µ + ì€ë‹‰ì¸µ + ì¶œë ¥ì¸µ
  * í•™ìŠµ ì†ë„: ëŠë¦¼

* **Word2Vec**

  * ëª©ì : *ì›Œë“œ ì„ë² ë”© ìì²´ í•™ìŠµ*
  * êµ¬ì¡°: íˆ¬ì‚¬ì¸µ â†’ ë°”ë¡œ ì¶œë ¥ì¸µ (ì€ë‹‰ì¸µ ì œê±°)
  * ì˜ˆì¸¡ ëŒ€ìƒ: ì¤‘ì‹¬ ë‹¨ì–´ (CBOW) ë˜ëŠ” ì£¼ë³€ ë‹¨ì–´ (Skip-Gram)
  * í•™ìŠµ ì†ë„: ë¹ ë¦„ (ì€ë‹‰ì¸µ ì œê±° + ìµœì í™” ê¸°ë²• ì ìš©)

#### ìµœì í™” ê¸°ë²•

* **ê³„ì¸µì  ì†Œí”„íŠ¸ë§¥ìŠ¤ (Hierarchical Softmax)**
* **ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ (Negative Sampling)**
  â†’ í•™ìŠµ íš¨ìœ¨ì„ í¬ê²Œ í–¥ìƒ.

---

### 4. Word2Vecì˜ í•µì‹¬

* ì›-í•« ë²¡í„°ì˜ í•œê³„ë¥¼ ê·¹ë³µ â†’ **ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° í‘œí˜„** ê°€ëŠ¥.
* ë¶„í¬ ê°€ì„¤ì— ê¸°ë°˜í•œ **ë¶„ì‚° í‘œí˜„** í•™ìŠµ.
* **CBOW**: ì£¼ë³€ ë‹¨ì–´ â†’ ì¤‘ì‹¬ ë‹¨ì–´
* **Skip-Gram**: ì¤‘ì‹¬ ë‹¨ì–´ â†’ ì£¼ë³€ ë‹¨ì–´
* **NNLM ê°œì„ íŒ**ìœ¼ë¡œ, ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì›Œë“œ ì„ë² ë”©ì„ í•™ìŠµ.

---

>### ğŸ“Œ 9_03,04 ìƒëµ~

---

> ### 9-05 GloVe (Global Vectors for Word Representation)

---

### 0. GloVe ê°œìš”

* **ê°œë°œ**: 2014ë…„, ìŠ¤íƒ í¬ë“œëŒ€í•™
* **ë°©ì‹**: ì¹´ìš´íŠ¸ ê¸°ë°˜(LSA) + ì˜ˆì¸¡ ê¸°ë°˜(Word2Vec)ì„ ê²°í•©í•œ **í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ë°©ë²•ë¡ **
* **ëª©í‘œ**: LSAì™€ Word2Vec ê°ê°ì˜ í•œê³„ ë³´ì™„
* **ì„±ëŠ¥**: Word2Vecê³¼ ë¹„ìŠ·í•˜ê²Œ ìš°ìˆ˜, ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ë‘˜ ë‹¤ ì‚¬ìš©í•´ë³´ê³  ì„ íƒí•˜ëŠ” ê²ƒì´ ë°”ëŒì§

---

### 1. ê¸°ì¡´ ë°©ë²•ë¡ ì˜ í•œê³„

* **LSA (Latent Semantic Analysis)**

  * ì¹´ìš´íŠ¸ ê¸°ë°˜ (DTM, TF-IDF)
  * ì „ì²´ í†µê³„ ì •ë³´ í™œìš©
  * í•˜ì§€ë§Œ **ë‹¨ì–´ ìœ ì¶”(Analogy Task)**ì—ëŠ” ì•½í•¨ (ex: ì™• - ë‚¨ì + ì—¬ì â‰ˆ ì—¬ì™• ì˜ ëª»í•¨)

* **Word2Vec**

  * ì˜ˆì¸¡ ê¸°ë°˜ (CBOW, Skip-Gram)
  * ë‹¨ì–´ ìœ ì¶” ì‘ì—…ì—ëŠ” ê°•í•¨
  * ê·¸ëŸ¬ë‚˜ **ìœˆë„ìš° ë‚´ êµ­ì†Œì  ë¬¸ë§¥ë§Œ** ë°˜ì˜, ì½”í¼ìŠ¤ ì „ì²´ í†µê³„ëŠ” ë°˜ì˜ ë¶ˆê°€

â¡ **GloVe**: ì¹´ìš´íŠ¸ ê¸°ë°˜ + ì˜ˆì¸¡ ê¸°ë°˜ì˜ ì¥ì  ëª¨ë‘ í™œìš©

---

### 2. ìœˆë„ìš° ê¸°ë°˜ ë™ì‹œ ë“±ì¥ í–‰ë ¬ (Co-occurrence Matrix)

* í–‰/ì—´ = ë‹¨ì–´ ì§‘í•©ì˜ ë‹¨ì–´ë“¤
* ì…€ ê°’ = íŠ¹ì • ë‹¨ì–´ i ì£¼ë³€(ìœˆë„ìš° í¬ê¸° ë‚´)ì— ë‹¨ì–´ kê°€ ë“±ì¥í•œ íšŸìˆ˜

ì˜ˆì‹œ (ìœˆë„ìš° í¬ê¸°=1):

| Count    | I | like | enjoy | deep | learning | NLP | flying |
| -------- | - | ---- | ----- | ---- | -------- | --- | ------ |
| I        | 0 | 2    | 1     | 0    | 0        | 0   | 0      |
| like     | 2 | 0    | 0     | 1    | 0        | 1   | 0      |
| enjoy    | 1 | 0    | 0     | 0    | 0        | 0   | 1      |
| deep     | 0 | 1    | 0     | 0    | 1        | 0   | 0      |
| learning | 0 | 0    | 0     | 1    | 0        | 0   | 0      |
| NLP      | 0 | 1    | 0     | 0    | 0        | 0   | 0      |
| flying   | 0 | 0    | 1     | 0    | 0        | 0   | 0      |

* ëŒ€ì¹­ í–‰ë ¬ (Transpose í•´ë„ ë™ì¼)
* ì´ìœ : i ì£¼ë³€ì— kê°€ ë“±ì¥í•œ íšŸìˆ˜ = k ì£¼ë³€ì— iê°€ ë“±ì¥í•œ íšŸìˆ˜

---

### 3. ë™ì‹œ ë“±ì¥ í™•ë¥  (Co-occurrence Probability)

* ì •ì˜:

  ```
  P(k | i) = iê°€ ë“±ì¥í–ˆì„ ë•Œ, kê°€ ë“±ì¥í•  í™•ë¥ 
  ```
* ê³„ì‚° ë°©ë²•:

  ```
  P(k | i) = (ë™ì‹œ ë“±ì¥ íšŸìˆ˜ Xik) / (ë‹¨ì–´ iì˜ ì „ì²´ ë“±ì¥ íšŸìˆ˜ Xi)
  ```
* í™•ë¥  ë¹„ìœ¨ì„ í†µí•´ ë‹¨ì–´ ê´€ê³„ íŒŒì•… ê°€ëŠ¥

ì˜ˆì‹œ (ë…¼ë¬¸ ì˜ˆì‹œ ë‹¨ìˆœí™”):

| k   | solid  | gas | water | fashion |    |
| --- | ------ | --- | ----- | ------- | -- |
| P(k | ice)   | í¬ë‹¤  | ì‘ë‹¤    | í¬ë‹¤      | ì‘ë‹¤ |
| P(k | steam) | ì‘ë‹¤  | í¬ë‹¤    | í¬ë‹¤      | ì‘ë‹¤ |
| ë¹„ìœ¨  | í¬ë‹¤     | ì‘ë‹¤  | â‰ˆ1    | â‰ˆ1      |    |

* **ice-solid**: í° ë¹„ìœ¨ (ì—°ê´€ ê°•í•¨)
* **ice-gas**: ì‘ì€ ë¹„ìœ¨ (steamê³¼ ë” ì—°ê´€)
* **ice-water**: 1ì— ê°€ê¹Œì›€ (ë‘˜ ë‹¤ ê´€ë ¨ ë§ìŒ)
* **ice-fashion**: 1ì— ê°€ê¹Œì›€ (ë‘˜ ë‹¤ ê±°ì˜ ë¬´ê´€)

---

### 4. ì†ì‹¤ í•¨ìˆ˜ (Loss Function)

* GloVeì˜ ì•„ì´ë””ì–´:

  > **ì„ë² ë”© ë²¡í„° ë‚´ì  â‰ˆ ë™ì‹œ ë“±ì¥ í™•ë¥  (ë˜ëŠ” ê·¸ ë¹„ìœ¨)ì˜ ë¡œê·¸ ê°’**

* ê¸°ë³¸ì‹:

  ```
  wi Â· wj + bi + bj â‰ˆ log(Xij)
  ```

  * `wi`: ì¤‘ì‹¬ ë‹¨ì–´ ì„ë² ë”© ë²¡í„°
  * `wj`: ì£¼ë³€ ë‹¨ì–´ ì„ë² ë”© ë²¡í„°
  * `bi, bj`: í¸í–¥ í•­
  * `Xij`: ë™ì‹œ ë“±ì¥ íšŸìˆ˜

* ìµœì¢… ì†ì‹¤ í•¨ìˆ˜:

  ```
  J = Î£ f(Xij) * (wi Â· wj + bi + bj - log(Xij))^2
  ```

  * `f(Xij)`: ê°€ì¤‘ì¹˜ í•¨ìˆ˜ â†’ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ìŒì€ ë†’ì€ ê°€ì¤‘ì¹˜, ë“œë¬¸ ë‹¨ì–´ ìŒì€ ë‚®ì€ ê°€ì¤‘ì¹˜
  * ë‹¨, ë¶ˆìš©ì–´(ì˜ˆ: it, the)ì²˜ëŸ¼ ì§€ë‚˜ì¹˜ê²Œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì—ëŠ” ê³¼ë„í•œ ê°€ì¤‘ì¹˜ X

---

### 5. ì‹¤ìŠµ 

```python
from glove import Corpus, Glove

# ë™ì‹œ ë“±ì¥ í–‰ë ¬ ìƒì„±
corpus = Corpus()
corpus.fit(result, window=5)

# GloVe ëª¨ë¸ í•™ìŠµ
glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

# ìœ ì‚¬ ë‹¨ì–´ í™•ì¸
print(glove.most_similar("man"))
# [('woman', 0.96), ('guy', 0.88), ('girl', 0.86), ('kid', 0.83)]
```

---

### 6. í•µì‹¬ ìš”ì•½

* **LSA(ì¹´ìš´íŠ¸ ê¸°ë°˜)** + **Word2Vec(ì˜ˆì¸¡ ê¸°ë°˜)** = **GloVe**
* **ì „ì²´ ì½”í¼ìŠ¤ í†µê³„ ì •ë³´**ì™€ **êµ­ì†Œ ë¬¸ë§¥ ì •ë³´**ë¥¼ ëª¨ë‘ ë°˜ì˜
* ì†ì‹¤ í•¨ìˆ˜: ì„ë² ë”© ë²¡í„° ë‚´ì  â‰ˆ ë™ì‹œ ë“±ì¥ í™•ë¥ ì˜ ë¡œê·¸
* Word2Vecë§Œí¼ ê°•ë ¥, ì‹¤ì œ ì ìš© ì‹œ ë‘ ê°€ì§€ ëª¨ë‘ ì‹¤í—˜ í›„ ì„ íƒ

---


> ### 9-06 FastText (Facebook AI, 2016)

---

### 0. FastText ê°œìš”

* ê°œë°œ: **Facebook AI Research** (2016)
* Word2Vecì˜ í™•ì¥íŒ â†’ **ë‹¨ì–´ ë‚´ë¶€ì˜ subword(n-gram) êµ¬ì¡°ê¹Œì§€ í•™ìŠµ**
* ì¥ì :

  1. **OOV(Out-of-Vocabulary) ë¬¸ì œ í•´ê²°**
  2. **í¬ê·€ ë‹¨ì–´(Rare word) ì„ë² ë”© ê°œì„ **
  3. **ì˜¤íƒ€/ë…¸ì´ì¦ˆ ë°ì´í„°ì— ê°•í•¨**

---

### 1. ë‚´ë¶€ ë‹¨ì–´(Subword) í•™ìŠµ

* Word2Vec: ë‹¨ì–´ë¥¼ **ìª¼ê°¤ ìˆ˜ ì—†ëŠ” ìµœì†Œ ë‹¨ìœ„**ë¡œ ì·¨ê¸‰
* FastText: ë‹¨ì–´ë¥¼ ê¸€ì ê¸°ë°˜ **n-gram subword**ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ì·¨ê¸‰

ì˜ˆì‹œ (n=3, trigram)

```
apple â†’ <ap, app, ppl, ple, le>, <apple>
```

* ì‹¤ì œë¡œëŠ” nì˜ ìµœì†Œê°’/ìµœëŒ€ê°’ì„ ì •í•´ í•™ìŠµ (ê¸°ë³¸: 3 ~ 6)
* ë‹¨ì–´ ë²¡í„° = ë‚´ë¶€ subword ë²¡í„°ë“¤ì˜ **í•©**

```
apple = Î£(subword embeddings)
```

---

### 2. OOV(Out of Vocabulary) ë¬¸ì œ ëŒ€ì‘

* Word2Vec/GloVe â†’ í•™ìŠµì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì²˜ë¦¬ ë¶ˆê°€
* FastText â†’ subword ë‹¨ìœ„ë¡œ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— **ëª¨ë¥´ëŠ” ë‹¨ì–´ë„ ë¶„í•´ í›„ ë²¡í„° ìƒì„± ê°€ëŠ¥**

ì˜ˆ:

* OOV ë‹¨ì–´: `birthplace`
* ë‚´ë¶€ subword: `birth`, `place`
* ë‘ subword ì„ë² ë”©ìœ¼ë¡œ í•©ì„±í•˜ì—¬ `birthplace` ë²¡í„° ìƒì„±

---

### 3. í¬ê·€ ë‹¨ì–´(Rare Word) ëŒ€ì‘

* Word2Vec: ë“±ì¥ íšŸìˆ˜ ì ìœ¼ë©´ ì˜ í•™ìŠµë˜ì§€ ì•ŠìŒ
* FastText:

  * í¬ê·€ ë‹¨ì–´ë¼ë„ **n-gramì´ ë‹¤ë¥¸ ë‹¨ì–´ì™€ ê³µìœ **ë˜ë©´ ì˜ë¯¸ í•™ìŠµ ê°€ëŠ¥
  * ì˜¤íƒ€(`appple`) ê°™ì€ ê²½ìš°ë„ ëŒ€ë¶€ë¶„ ë™ì¼í•œ subword í¬í•¨ â†’ **ì„ë² ë”© ê°•ê±´ì„± ìœ ì§€**

---

### 4. Word2Vec vs FastText ì‹¤ìŠµ ë¹„êµ

```python
# Word2Vec
model.wv.most_similar("electrofishing")
# KeyError: "word 'electrofishing' not in vocabulary"

# FastText
from gensim.models import FastText
model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)

model.wv.most_similar("electrofishing")
# [('electrolux', 0.79), ('electrolyte', 0.78), ('electro', 0.77), ...]
```

* Word2Vec â†’ OOV ë‹¨ì–´ ì²˜ë¦¬ ë¶ˆê°€
* FastText â†’ ìœ ì‚¬ ë‹¨ì–´ íƒìƒ‰ ê°€ëŠ¥ (subword ê¸°ë°˜)

---

### 5. í•œêµ­ì–´ì—ì„œì˜ FastText

#### (1) ìŒì ˆ ë‹¨ìœ„

* ì˜ˆ: "ìì—°ì–´ì²˜ë¦¬", n=3

```
<ìì—°, ìì—°ì–´, ì—°ì–´ì²˜, ì–´ì²˜ë¦¬, ì²˜ë¦¬>
```

#### (2) ìëª¨ ë‹¨ìœ„ (ì´ˆì„±/ì¤‘ì„±/ì¢…ì„± ë¶„í•´)

* ì˜ˆ: "ìì—°ì–´ì²˜ë¦¬" â†’ ã…ˆ ã… _ ã…‡ ã…• ã„´ ã…‡ ã…“ _ ã…Š ã…“ _ ã„¹ ã…£ _
* n=3 ì ìš© ì‹œ:

```
<ã…ˆã…_, ã…_ã…‡, ã…‡ã…•ã„´, ...>
```

* ì¥ì : **ì˜¤íƒ€Â·ë…¸ì´ì¦ˆì—ë„ ê°•í•œ ì„ë² ë”© ê°€ëŠ¥**

---

### 6. í•µì‹¬ ìš”ì•½

* FastText = Word2Vec + Subword í•™ìŠµ
* ì¥ì :
  âœ… OOV ë‹¨ì–´ ì²˜ë¦¬
  âœ… í¬ê·€ ë‹¨ì–´/ì˜¤íƒ€ ëŒ€ì‘
  âœ… í•œêµ­ì–´ ë“± í˜•íƒœì†Œì  íŠ¹ì„±ì´ ê°•í•œ ì–¸ì–´ì—ì„œ íš¨ê³¼ì 
* ì‹¤ì œ NLP í˜„ì—…ì—ì„œë„ **ë…¸ì´ì¦ˆ ë§ì€ ë°ì´í„°ì…‹ ì²˜ë¦¬**ì— ê°•ë ¥

---

> ### 9-08 ì‚¬ì „ í›ˆë ¨ëœ ì›Œë“œ ì„ë² ë”© (Pre-trained Word Embedding)

---

### 0. ê°œìš”

* **ì¼€ë¼ìŠ¤ ì„ë² ë”© ì¸µ (Embedding layer)**

  * í›ˆë ¨ ë°ì´í„°ì—ì„œ ë‹¨ì–´ë¥¼ ì •ìˆ˜ ì¸ì½”ë”© â†’ ëœë¤ ì´ˆê¸°í™”ëœ ë²¡í„° â†’ í•™ìŠµ ê³¼ì •ì—ì„œ ê°±ì‹ 
  * ì¥ì : ì‘ì—…(task)ì— ë§ê²Œ ì„ë² ë”© ë²¡í„° ìµœì í™”
  * ë‹¨ì : ë°ì´í„°ê°€ ì ìœ¼ë©´ ì¼ë°˜í™”ê°€ ì˜ ì•ˆë¨

* **ì‚¬ì „ í›ˆë ¨ëœ ì„ë² ë”© (Pre-trained Embedding)**

  * ë°©ëŒ€í•œ ì½”í¼ìŠ¤(Wikipedia, Google News ë“±)ë¡œ ë¯¸ë¦¬ í•™ìŠµëœ ë²¡í„° (Word2Vec, GloVe, FastText ë“±) ë¶ˆëŸ¬ì˜¤ê¸°
  * ì¥ì : ì ì€ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ ê°œì„ 
  * ë‹¨ì : í˜„ì¬ ì‘ì—… íŠ¹í™” ì •ë³´ëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìŒ

---

### 1. ì¼€ë¼ìŠ¤ ì„ë² ë”© ì¸µ (Keras Embedding Layer)

* `Embedding()` í•¨ìˆ˜ ì‚¬ìš©
* ì…ë ¥: ì •ìˆ˜ ì¸ì½”ë”©ëœ ì‹œí€€ìŠ¤
* ì¶œë ¥: **ì„ë² ë”© ë²¡í„° ì‹œí€€ìŠ¤ (3D í…ì„œ)**

```python
from tensorflow.keras.layers import Embedding

vocab_size = 20000   # ë‹¨ì–´ ì§‘í•© í¬ê¸°
output_dim = 128     # ì„ë² ë”© ì°¨ì›
input_length = 500   # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´

embedding_layer = Embedding(vocab_size, output_dim, input_length=input_length)
```

* ë™ì‘ ë°©ì‹: **ë£©ì—… í…Œì´ë¸”(Lookup Table)**

  * ì •ìˆ˜ ì¸ë±ìŠ¤ â†’ í•´ë‹¹ í–‰(ì„ë² ë”© ë²¡í„°) ë°˜í™˜
  * ì—­ì „íŒŒ ì‹œ ê°€ì¤‘ì¹˜(ì„ë² ë”© ë²¡í„° ê°’)ë„ í•™ìŠµ

---

### 2. ì¼€ë¼ìŠ¤ ì„ë² ë”© ì¸µ ì‹¤ìŠµ

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

# ë°ì´í„° ì¤€ë¹„
sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd',
             'excellent work', 'supreme quality', 'bad', 'highly respectable']
y_train = [1, 0, 0, 1, 1, 0, 1]

# í† í¬ë‚˜ì´ì €
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
vocab_size = len(tokenizer.word_index) + 1

# ì •ìˆ˜ ì¸ì½”ë”© + íŒ¨ë”©
X_encoded = tokenizer.texts_to_sequences(sentences)
max_len = max(len(l) for l in X_encoded)
X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')
y_train = np.array(y_train)

# ëª¨ë¸ ì„¤ê³„
embedding_dim = 4
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.fit(X_train, y_train, epochs=100, verbose=2)
```

---

### 3. ì‚¬ì „ í›ˆë ¨ëœ ì›Œë“œ ì„ë² ë”© í™œìš©

#### (1) GloVe

```python
embedding_dict = {}
f = open('glove.6B.100d.txt', encoding="utf8")

for line in f:
    word_vector = line.split()
    word = word_vector[0]
    vector = np.asarray(word_vector[1:], dtype='float32')
    embedding_dict[word] = vector
f.close()

# ì„ë² ë”© í–‰ë ¬ ìƒì„±
embedding_matrix = np.zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    vector = embedding_dict.get(word)
    if vector is not None:
        embedding_matrix[index] = vector

# ëª¨ë¸ ì ìš©
e = Embedding(vocab_size, 100, weights=[embedding_matrix],
              input_length=max_len, trainable=False)
```

---

#### (2) Word2Vec (Google News, 3M words, 300-dim)


```python
import gensim
word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(
    'GoogleNews-vectors-negative300.bin.gz', binary=True
)

# ì„ë² ë”© í–‰ë ¬ ìƒì„±
embedding_matrix = np.zeros((vocab_size, 300))
for word, index in tokenizer.word_index.items():
    if word in word2vec_model:
        embedding_matrix[index] = word2vec_model[word]

# ëª¨ë¸ ì ìš©
e = Embedding(vocab_size, 300, weights=[embedding_matrix],
              input_length=max_len, trainable=False)
```

---

### 4. í•µì‹¬ ìš”ì•½

| êµ¬ë¶„      | ì¼€ë¼ìŠ¤ Embedding() | ì‚¬ì „ í›ˆë ¨ëœ Embedding         |
| ------- | --------------- | ------------------------ |
| ë°ì´í„° ì˜ì¡´ì„± | í˜„ì¬ ë°ì´í„°ì— ë§ê²Œ í•™ìŠµ   | ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ ê¸°ë°˜               |
| ì¥ì       | íŠ¹ì • taskì— ìµœì í™”    | ì ì€ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ ê°œì„           |
| ë‹¨ì       | ë°ì´í„° ë¶€ì¡± ì‹œ ì¼ë°˜í™” ì•½í•¨ | task íŠ¹í™” ë¶€ì¡±               |
| í™œìš© ì˜ˆì‹œ   | ê°ì„± ë¶„ë¥˜, íƒœê¹… ë“±     | ì „ì´ í•™ìŠµ(Transfer Learning) |

---


ì¢‹ìŠµë‹ˆë‹¤ ğŸ‘
ì´ë²ˆì—ëŠ” **ch.9-09 ELMo (Embeddings from Language Model)** ë‚´ìš©ì„ ê¹ƒí—ˆë¸Œ ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ë¡œ ì •ë¦¬í•´ì¤„ê²Œìš”.

---

# ch.9 ì›Œë“œ ì„ë² ë”© (Word Embedding)

> ### 9-09 ELMo (Embeddings from Language Model)

---

## 0. ê°œìš”

* **ELMo**: 2018ë…„ ì œì•ˆëœ **ë¬¸ë§¥ ê¸°ë°˜ ì›Œë“œ ì„ë² ë”© (Contextualized Word Embedding)**
* ê¸°ì¡´ Word2Vec, GloVe â†’ **ë‹¨ì–´ë³„ ê³ ì • ë²¡í„°**
* ELMo â†’ **ë¬¸ë§¥(Context)ì— ë”°ë¼ ë‹¨ì–´ ì„ë² ë”©ì´ ë‹¬ë¼ì§**
* ì´ë¦„ ì˜ë¯¸: *Embeddings from Language Model*
* ê¸°ë°˜ ëª¨ë¸: **biLM (Bidirectional Language Model)**
* êµ¬í˜„: TensorFlow Hub ì œê³µ (ë‹¨, TF2.xì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€ â†’ TF1.x í•„ìš”)

---

## 1. í•œê³„ì ê³¼ ë“±ì¥ ë°°ê²½

* Word2Vec, GloVe ë¬¸ì œì :

  * `bank` (ì€í–‰ vs ê°•ë‘‘) ê°™ì€ ë‹¤ì˜ì–´ êµ¬ë¶„ ë¶ˆê°€
  * ë™ì¼í•œ ê³ ì • ë²¡í„° ì‚¬ìš©

* í•´ê²°ì±…:

  * **ELMoëŠ” ë¬¸ë§¥ì„ ë°˜ì˜**
  * ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ì¥ ë§¥ë½ì— ë”°ë¼ ë‹¤ë¥¸ ë²¡í„° ìƒì„±

---

## 2. biLM (Bidirectional Language Model)

* **ì–¸ì–´ ëª¨ë¸ë§ (Language Modeling)**:

  * ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (ìˆœë°©í–¥)
  * ì´ì „ ë‹¨ì–´ ì˜ˆì¸¡ (ì—­ë°©í–¥)

* **ELMoì˜ biLM**:

  * ìˆœë°©í–¥ RNN + ì—­ë°©í–¥ RNN **ë™ì‹œì— í•™ìŠµ**
  * ë‹¤ì¸µ êµ¬ì¡° (multi-layer)
  * ì…ë ¥: **ë¬¸ì ì„ë² ë”© (character-level CNN)**

    * subword ì •ë³´ ë°˜ì˜
    * OOV ë‹¨ì–´ì—ë„ ê°•ê±´í•¨

âš ï¸ ì£¼ì˜:

* ì¼ë°˜ì ì¸ Bi-RNN = ì€ë‹‰ ìƒíƒœë¥¼ concat
* ELMoì˜ biLM = ìˆœ/ì—­ ëª¨ë¸ì„ ë³„ë„ë¡œ í•™ìŠµ í›„ í™œìš©

---

## 3. ELMo í‘œí˜„ (Representation)

ELMo ë²¡í„° ìƒì„± ê³¼ì •:

1. **biLM ê° ì¸µì˜ ì¶œë ¥ê°’** (ì„ë² ë”© ì¸µ + ì€ë‹‰ì¸µë“¤) ìˆ˜ì§‘
2. **ìˆœë°©í–¥ + ì—­ë°©í–¥ ì¶œë ¥ concat**
3. ì¸µë§ˆë‹¤ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬

   * ( s_1, s_2, s_3 )
4. ê°€ì¤‘í•©(Weighted Sum)
5. ìŠ¤ì¹¼ë¼ íŒŒë¼ë¯¸í„° ( \gamma ) ê³±í•¨

ğŸ“Œ ìµœì¢…:
[
ELMo = \gamma \sum_i s_i h_i
]

* ì—¬ê¸°ì„œ ( h_i ) = ê° ì¸µì˜ ì¶œë ¥ê°’

* **ê¸°ì¡´ ì„ë² ë”© (ì˜ˆ: GloVe) + ELMo ë²¡í„° concat â†’ downstream task ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥**


---

## 4. ìš”ì•½

| íŠ¹ì§•     | Word2Vec/GloVe | ELMo                |
| ------ | -------------- | ------------------- |
| ë²¡í„° ìœ í˜•  | ì •ì  (Static)    | ë™ì  (Contextualized) |
| ë¬¸ë§¥ ë°˜ì˜  | X              | O                   |
| ë‹¤ì˜ì–´ êµ¬ë¶„ | ë¶ˆê°€ëŠ¥            | ê°€ëŠ¥                  |
| ì…ë ¥ ë‹¨ìœ„  | ë‹¨ì–´             | ë¬¸ì ì„ë² ë”© ê¸°ë°˜           |
| ì¥ì      | ê°„ë‹¨, ë¹ ë¦„         | ë¬¸ë§¥ ì •ë³´, OOV ê°•ê±´       |

---
# ch.12 íƒœê¹…

ì¢‹ì€ í¬ì¸íŠ¸ì•¼ ğŸ‘ ì•ë¶€ë¶„ì— â€œíƒœê¹…ì´ë€ ë¬´ì—‡ì¸ì§€ â†’ ì™œ ë‹¤-ëŒ€-ë‹¤ êµ¬ì¡°ì¸ì§€ â†’ ì™œ ì–‘ë°©í–¥ RNNì´ í•„ìš”í•œì§€â€ë¼ëŠ” **ìŠ¤í† ë¦¬ íë¦„**ì„ ë¶™ì´ë©´ í›¨ì”¬ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬ë¼. ë°©ê¸ˆ ë‹µë³€ì„ ë³´ê°•í•´ì„œ ê¹”ë”í•˜ê²Œ ë‹¤ì‹œ ì •ë¦¬í•´ì¤„ê²Œ.

---

# ch.12 íƒœê¹… ì‘ì—… (Tagging Task)

> ### 12-01 ì¼€ë¼ìŠ¤ë¥¼ ì´ìš©í•œ íƒœê¹… ì‘ì—… ê°œìš” (Tagging Task using Keras)

---

## 0. ìŠ¤í† ë¦¬ & ê°œìš”

ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ë¬¸ì œê°€ ìˆë‹¤:

* **ë¬¸ì¥ ë‹¨ìœ„ ì˜ˆì¸¡ (ë‹¤-ëŒ€-ì¼, Many-to-One)**
  â†’ ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ê°€ í•˜ë‚˜ì˜ ì¶œë ¥(ë ˆì´ë¸”)ë¡œ ë§¤í•‘
  â†’ ì˜ˆ: ê°ì„± ë¶„ì„, ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜

* **ë‹¨ì–´ ë‹¨ìœ„ ì˜ˆì¸¡ (ë‹¤-ëŒ€-ë‹¤, Many-to-Many)**
  â†’ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë‹¨ì–´ê°€ ê°œë³„ ë ˆì´ë¸”ê³¼ ë§¤í•‘
  â†’ ì˜ˆ: í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹

ì´ë²ˆ ì±•í„°ì˜ **íƒœê¹… ì‘ì—…(Tagging Task)** ì€ í›„ìì— í•´ë‹¹í•œë‹¤.
ì¦‰, ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë§ˆë‹¤ "í’ˆì‚¬"ë‚˜ "ê°œì²´ ìœ í˜•"ì„ ë¶™ì—¬ì£¼ëŠ” ë¬¸ì œë‹¤.

* ë”°ë¼ì„œ íƒœê¹…ì€ **ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§(Sequence Labeling)** ë¬¸ì œì˜ ëŒ€í‘œ ì‚¬ë¡€ì´ë©°, **RNNì˜ ë‹¤-ëŒ€-ë‹¤ êµ¬ì¡°**ê°€ í•„ìš”í•˜ë‹¤.
* ë˜í•œ ë‹¨ì–´ì˜ í’ˆì‚¬ë‚˜ ê°œì²´ëŠ” ì•ë’¤ ë¬¸ë§¥ ëª¨ë‘ì— ì˜ì¡´í•˜ë¯€ë¡œ **ì–‘ë°©í–¥ RNN(Bidirectional RNN)** ì´ íš¨ê³¼ì ì´ë‹¤.

---

## 1. í›ˆë ¨ ë°ì´í„° êµ¬ì¡°

* ì…ë ¥(X): í† í°í™”ëœ ë¬¸ì¥ (ë‹¨ì–´ ì‹œí€€ìŠ¤)
* ì¶œë ¥(y): ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” íƒœê·¸ ì‹œí€€ìŠ¤
* íŠ¹ì§•: **Xì™€ yëŠ” ë³‘ë ¬ êµ¬ì¡°ì´ë©° ê¸¸ì´ê°€ ë™ì¼**

ì˜ˆì‹œ (ê°œì²´ëª… ì¸ì‹, NER ë°ì´í„°):

| idx | X_train (ë‹¨ì–´ ì‹œí€€ìŠ¤)                                                          | y_train (íƒœê·¸ ì‹œí€€ìŠ¤)                                         | ê¸¸ì´ |
| --- | ------------------------------------------------------------------------- | -------------------------------------------------------- | -- |
| 0   | `['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']` | `['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']` | 8  |
| 1   | `['peter', 'blackburn']`                                                  | `['B-PER', 'I-PER']`                                     | 2  |
| 2   | `['brussels', '1996-08-22']`                                              | `['B-LOC', 'O']`                                         | 2  |
| 3   | `['The', 'European', 'Commission']`                                       | `['O', 'B-ORG', 'I-ORG']`                                | 3  |

 ê° ë‹¨ì–´ì™€ íƒœê·¸ëŠ” **1:1 pair** ê´€ê³„

ì´í›„ ê³¼ì •: ì •ìˆ˜ ì¸ì½”ë”© â†’ íŒ¨ë”©(Padding) â†’ ë”¥ëŸ¬ë‹ ì…ë ¥

---

## 2. ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§ (Sequence Labeling)

* ì •ì˜: ì…ë ¥ ì‹œí€€ìŠ¤ Xì˜ ê° ë‹¨ì–´ì— ëŒ€í•´, ëŒ€ì‘ë˜ëŠ” ë ˆì´ë¸” ì‹œí€€ìŠ¤ yë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…


* ëŒ€í‘œ íƒœìŠ¤í¬:

  * í’ˆì‚¬ íƒœê¹… (POS tagging)
  * ê°œì²´ëª… ì¸ì‹ (NER)

---

## 3. ë‹¤-ëŒ€-ì¼ vs ë‹¤-ëŒ€-ë‹¤

| êµ¬ì¡°                       | ì„¤ëª…                   | ì˜ˆì‹œ íƒœìŠ¤í¬        |
| ------------------------ | -------------------- | ------------- |
| **ë‹¤-ëŒ€-ì¼ (Many-to-One)**  | ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ â†’ ë‹¨ì¼ ì¶œë ¥    | ë¬¸ì¥ ë¶„ë¥˜, ê°ì„± ë¶„ì„  |
| **ë‹¤-ëŒ€-ë‹¤ (Many-to-Many)** | ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë‹¨ì–´ â†’ ê°œë³„ ì¶œë ¥ | í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ |

ğŸ‘‰ íƒœê¹…ì€ ë¬¸ì¥ ë‚´ **ëª¨ë“  ë‹¨ì–´ì— ë ˆì´ë¸”**ì„ ë‹¬ì•„ì•¼ í•˜ë¯€ë¡œ **ë‹¤-ëŒ€-ë‹¤ êµ¬ì¡°**ê°€ í•„ìš”í•˜ë‹¤.

---

## 4. ì–‘ë°©í–¥ LSTM (Bidirectional LSTM)

* ë‹¨ë°©í–¥ RNN: í˜„ì¬ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œ ê³¼ê±° ë‹¨ì–´ ì •ë³´ë§Œ ì‚¬ìš©
* ì–‘ë°©í–¥ RNN: í˜„ì¬ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œ **ê³¼ê±° + ë¯¸ë˜ ë¬¸ë§¥ ëª¨ë‘ í™œìš©**

```python
from keras.layers import Bidirectional, LSTM

model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
```

* `hidden_units`: ì€ë‹‰ ìƒíƒœ ì°¨ì›
* `return_sequences=True`: ëª¨ë“  ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ ì¶œë ¥ (íƒœê¹… í•„ìˆ˜ ì˜µì…˜)

---

## 5. ì‹œê°ì  ì´í•´

### ë‹¨ë°©í–¥ LSTM (Many-to-One ì˜ˆ: ë¬¸ì¥ ë¶„ë¥˜)

```
w1 â†’ w2 â†’ w3 â†’ w4 â†’ [y]
```

ğŸ‘‰ ë¬¸ì¥ ì „ì²´ë¥¼ ë³´ê³  í•˜ë‚˜ì˜ ê²°ê³¼(ê°ì„± ë¼ë²¨ ë“±)ë¥¼ ì¶œë ¥

---

### ë‹¨ë°©í–¥ LSTM (Many-to-Many ì˜ˆ: íƒœê¹…)

```
w1 â†’ w2 â†’ w3 â†’ w4
y1   y2   y3   y4
```

ğŸ‘‰ ê° ë‹¨ì–´ë§ˆë‹¤ ë ˆì´ë¸”ì„ ì¶œë ¥ (ë‹¨, ê³¼ê±° ë¬¸ë§¥ë§Œ ë°˜ì˜)

---

### ì–‘ë°©í–¥ LSTM (Many-to-Many ì˜ˆ: íƒœê¹…)

```
          â†’ w1 â†’ w2 â†’ w3 â†’ w4 â†’
Input: X ------------------------
          â† w1 â† w2 â† w3 â† w4 â†
Output:   y1   y2   y3   y4
```

ğŸ‘‰ ê° ë‹¨ì–´ì˜ ë ˆì´ë¸” ì˜ˆì¸¡ ì‹œ **ì•/ë’¤ ë¬¸ë§¥ ëª¨ë‘ ë°˜ì˜**

---



> ### 12-02 ì–‘ë°©í–¥ LSTMì„ ì´ìš©í•œ í’ˆì‚¬ íƒœê¹… (POS Tagging with Bi-LSTM)

---

## 0. ê°œìš”

* í’ˆì‚¬ íƒœê¹…(POS tagging): ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— ëŒ€í•´ í’ˆì‚¬(ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)ë¥¼ ë¶€ì—¬í•˜ëŠ” ì‘ì—…
* ê¸°ì¡´ì—ëŠ” **NLTK, KoNLPy** ë“±ì„ ì´ìš©í•´ íƒœê¹…í–ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” **ë”¥ëŸ¬ë‹ ëª¨ë¸**ì„ ì§ì ‘ êµ¬í˜„
* ëª¨ë¸ êµ¬ì¡°: **ì–‘ë°©í–¥ LSTM (Bi-LSTM)** ê¸°ë°˜ì˜ ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§ ëª¨ë¸

---

## 1. ë°ì´í„° ì´í•´ ë° ì „ì²˜ë¦¬

```python
import nltk
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
```

* NLTKì—ì„œ í’ˆì‚¬ íƒœê¹…ì´ ëœ ì˜ì–´ ì½”í¼ìŠ¤ ë¡œë“œ

```python
tagged_sentences = nltk.corpus.treebank.tagged_sents()
print("í’ˆì‚¬ íƒœê¹…ì´ ëœ ë¬¸ì¥ ê°œìˆ˜: ", len(tagged_sentences))
# 3,914ê°œ
```

* ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸

```python
print(tagged_sentences[0])
# [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ... ]
```

* ë‹¨ì–´ì™€ íƒœê·¸ë¥¼ ë¶„ë¦¬

```python
sentences, pos_tags = [], [] 
for tagged_sentence in tagged_sentences:
    sentence, tag_info = zip(*tagged_sentence)
    sentences.append(list(sentence))
    pos_tags.append(list(tag_info))
```

---

## 2. ë°ì´í„° íƒìƒ‰

* ì²« ë²ˆì§¸ ìƒ˜í”Œ

```python
print(sentences[0])
print(pos_tags[0])
```

* ë¬¸ì¥ ê¸¸ì´ ë¶„í¬ í™•ì¸

```python
print('ìƒ˜í”Œì˜ ìµœëŒ€ ê¸¸ì´ : %d' % max(len(l) for l in sentences))
print('ìƒ˜í”Œì˜ í‰ê·  ê¸¸ì´ : %f' % (sum(map(len, sentences))/len(sentences)))
plt.hist([len(s) for s in sentences], bins=50)
plt.show()
```

* ìµœëŒ€ ê¸¸ì´: 271
* í‰ê·  ê¸¸ì´: ì•½ 25.7

ğŸ‘‰ ëŒ€ë¶€ë¶„ ìƒ˜í”Œì€ ê¸¸ì´ê°€ 150 ì´í•˜ â†’ íŒ¨ë”© ê¸¸ì´(max_len)ë¥¼ 150ìœ¼ë¡œ ì„¤ì •

---

## 3. ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©

```python
def tokenize(samples):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(samples)
    return tokenizer

src_tokenizer = tokenize(sentences)
tar_tokenizer = tokenize(pos_tags)

vocab_size = len(src_tokenizer.word_index) + 1
tag_size = len(tar_tokenizer.word_index) + 1
```

* ë‹¨ì–´ ì§‘í•© í¬ê¸°: 11,388

* í’ˆì‚¬ íƒœê¹… ì§‘í•© í¬ê¸°: 47

* ì •ìˆ˜ ì¸ì½”ë”©

```python
X_train = src_tokenizer.texts_to_sequences(sentences)
y_train = tar_tokenizer.texts_to_sequences(pos_tags)
```

* íŒ¨ë”© ì²˜ë¦¬

```python
max_len = 150
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
y_train = pad_sequences(y_train, padding='post', maxlen=max_len)
```

* ë°ì´í„° ë¶„ë¦¬

```python
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)
```

---

## 4. Bi-LSTM ëª¨ë¸ ì„¤ê³„

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))

model.compile(loss='sparse_categorical_crossentropy',
              optimizer=Adam(0.001),
              metrics=['accuracy'])
```

---

## 5. í•™ìŠµ ë° í‰ê°€

```python
model.fit(X_train, y_train, batch_size=128, epochs=7, validation_data=(X_test, y_test))
```

* í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€

```python
print("\n í…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f" % (model.evaluate(X_test, y_test)[1]))
# í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9016
```

---

## 6. ê²°ê³¼ í™•ì¸

```python
index_to_word = src_tokenizer.index_word
index_to_tag = tar_tokenizer.index_word

i = 10  # ìƒ˜í”Œ ì¸ë±ìŠ¤
y_predicted = model.predict(np.array([X_test[i]]))
y_predicted = np.argmax(y_predicted, axis=-1)

print("{:15}|{:7}|{}".format("ë‹¨ì–´", "ì‹¤ì œê°’", "ì˜ˆì¸¡ê°’"))
print("*" * 40)

for word, tag, pred in zip(X_test[i], y_test[i], y_predicted[0]):
    if word != 0:
        print("{:15}: {:7} {}".format(index_to_word[word], 
                                      index_to_tag[tag].upper(), 
                                      index_to_tag[pred].upper()))
```



> ### 12-03 ê°œì²´ëª… ì¸ì‹ (Named Entity Recognition, NER)

---

## 0. ê°œìš”

* **ê°œì²´ëª… ì¸ì‹ (NER, Named Entity Recognition)**: ë¬¸ì¥ì—ì„œ ë‹¨ì–´ê°€ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…
* ì˜ˆ: ì‚¬ëŒ(Person), ì¥ì†Œ(Location), ì¡°ì§(Organization), ì‹œê°„(Time) ë“±

ì˜ˆì‹œ ë¬¸ì¥:
â€œìœ ì •ì´ëŠ” 2018ë…„ì— ê³¨ë“œë§Œì‚­ìŠ¤ì— ì…ì‚¬í–ˆë‹¤.â€

ê²°ê³¼:

* ìœ ì • â†’ ì‚¬ëŒ (PER)
* 2018ë…„ â†’ ì‹œê°„ (TIME)
* ê³¨ë“œë§Œì‚­ìŠ¤ â†’ ì¡°ì§ (ORG)

---

## 1. ê°œì²´ëª… ì¸ì‹ì´ë€?

* **Named Entity**: ì´ë¦„ì„ ê°€ì§„ ê°œì²´ (ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ë“±)
* **NERì˜ ëª©í‘œ**: ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ì–´ë–¤ ê°œì²´ ìœ í˜•ì— ì†í•˜ëŠ”ì§€ íŒë³„

NERì€ ëŒ€í‘œì ì¸ **ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§(Sequence Labeling) ë¬¸ì œ**ë¡œ, ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¼ë²¨ì„ ë¶€ì—¬í•˜ëŠ” í˜•íƒœë‹¤.

---

## 2. NLTKë¥¼ ì´ìš©í•œ NER ì‹¤ìŠµ

NLTKëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **NER chunker**ë¥¼ ì œê³µí•˜ë¯€ë¡œ, ì†ì‰½ê²Œ ê°œì²´ëª… ì¸ì‹ì„ ì‹¤ìŠµí•  ìˆ˜ ìˆë‹¤.

```python
from nltk import word_tokenize, pos_tag, ne_chunk

sentence = "James is working at Disney in London"

# 1) í† í°í™” + í’ˆì‚¬ íƒœê¹…
tokenized_sentence = pos_tag(word_tokenize(sentence))
print(tokenized_sentence)
```

ì¶œë ¥:

```
[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), 
 ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]
```

* ë‹¨ì–´ë³„ë¡œ í’ˆì‚¬ê°€ íƒœê¹…ë¨

---

### 2-1. ê°œì²´ëª… ì¸ì‹ ìˆ˜í–‰

```python
ner_sentence = ne_chunk(tokenized_sentence)
print(ner_sentence)
```

ì¶œë ¥:

```
(S
  (PERSON James/NNP)
  is/VBZ
  working/VBG
  at/IN
  (ORGANIZATION Disney/NNP)
  in/IN
  (GPE London/NNP))
```

* James â†’ PERSON (ì‚¬ëŒ)
* Disney â†’ ORGANIZATION (ì¡°ì§)
* London â†’ GPE (ì§€ëª…/ìœ„ì¹˜)

---

> ### 12-04 ê°œì²´ëª… ì¸ì‹ì˜ BIO í‘œí˜„ ì´í•´í•˜ê¸°

---

## 0. ê°œìš”

* ê°œì²´ëª… ì¸ì‹(NER)ì€ ì±—ë´‡, QA ì‹œìŠ¤í…œ, ê²€ìƒ‰ ë“±ì—ì„œ í•„ìˆ˜ì ì¸ **ì „ì²˜ë¦¬ ì‘ì—…**
* ì •í™•ë„ë¥¼ ë†’ì´ë ¤ë©´ **ë„ë©”ì¸ì— ë§ëŠ” ë°ì´í„°ì…‹**ìœ¼ë¡œ ì§ì ‘ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ í•„ìš”
* ë³¸ ì ˆì—ì„œëŠ” ê°œì²´ëª… ì¸ì‹ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” **BIO íƒœê¹… ë°©ì‹**ì„ ì„¤ëª…

---

## 1. BIO íƒœê¹… í‘œí˜„

* **B (Begin)** : ê°œì²´ëª… ì‹œì‘ ë‹¨ì–´
* **I (Inside)** : ê°œì²´ëª… ë‚´ë¶€ ë‹¨ì–´
* **O (Outside)** : ê°œì²´ëª…ì´ ì•„ë‹Œ ë‹¨ì–´

ì˜ˆì‹œ (ì˜í™” ì œëª© ê°œì²´ëª… ì¶”ì¶œ):

| ë‹¨ì–´ | íƒœê·¸ |
| -- | -- |
| í•´  | B  |
| ë¦¬  | I  |
| í¬  | I  |
| í„°  | I  |
| ë³´  | O  |
| ëŸ¬  | O  |
| ê°€  | O  |
| ì  | O  |

â†’ "í•´ë¦¬í¬í„°"ëŠ” **B + I + I + I**, ê·¸ ì™¸ ë‹¨ì–´ëŠ” O

---

## 2. ë‹¤ì¤‘ ê°œì²´ ìœ í˜• íƒœê¹…

ì‹¤ì œ ëŒ€í™”ì—ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ê°œì²´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤.
ì˜ˆ: ì˜í™”(movie), ê·¹ì¥(theater)

| ë‹¨ì–´ | íƒœê·¸        |
| -- | --------- |
| í•´  | B-movie   |
| ë¦¬  | I-movie   |
| í¬  | I-movie   |
| í„°  | I-movie   |
| ë³´  | O         |
| ëŸ¬  | O         |
| ë©”  | B-theater |
| ê°€  | I-theater |
| ë°•  | I-theater |
| ìŠ¤  | I-theater |
| ê°€  | O         |
| ì  | O         |

---

## 3. ê°œì²´ëª… ì¸ì‹ ë°ì´í„° í˜•ì‹ (CONLL2003 ì˜ˆì‹œ)

ì „í†µì ì¸ ì˜ì–´ ë°ì´í„°ì…‹ **CONLL2003**ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì œê³µë¨:

```
EU        NNP   B-NP   B-ORG
rejects   VBZ   B-VP   O
German    JJ    B-NP   B-MISC
call      NN    I-NP   O
to        TO    B-VP   O
boycott   VB    I-VP   O
British   JJ    B-NP   B-MISC
lamb      NN    I-NP   O
.         .     O      O
```

í˜•ì‹:
`[ë‹¨ì–´] [í’ˆì‚¬ íƒœê¹…] [ì²­í¬ íƒœê¹…] [ê°œì²´ëª… íƒœê¹…]`

* **í’ˆì‚¬ íƒœê¹…**: Penn Treebank POS íƒœê·¸ (ì˜ˆ: NNP = ê³ ìœ  ëª…ì‚¬ ë‹¨ìˆ˜, VBZ = 3ì¸ì¹­ ë‹¨ìˆ˜ ë™ì‚¬ í˜„ì¬í˜•)
* **ê°œì²´ëª… íƒœê¹…**: `LOC` (location), `ORG` (organization), `PER` (person), `MISC` (miscellaneous)

ì˜ˆ:

* "EU" â†’ B-ORG
* "German" â†’ B-MISC
* "Peter Blackburn" â†’ B-PER + I-PER

---

## 4. ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • ìš”ì•½

* ë°ì´í„° íŒŒì¼: `train.txt`
* ê° ë¬¸ì¥ì€ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„
* ê° ë‹¨ì–´ëŠ” `(ë‹¨ì–´, ê°œì²´ëª… íƒœê¹…)` í˜•íƒœë¡œ ì €ì¥
* zip()ì„ ì‚¬ìš©í•´ `sentences` (ë‹¨ì–´ë“¤)ì™€ `ner_tags` (ê°œì²´ëª… íƒœê¹…) ë¶„ë¦¬

---

## 5. Bi-LSTM ê¸°ë°˜ ê°œì²´ëª… ì¸ì‹ê¸°

* ì…ë ¥: ì •ìˆ˜ ì¸ì½”ë”©ëœ ë‹¨ì–´ ì‹œí€€ìŠ¤ (íŒ¨ë”© í¬í•¨)
* ì¶œë ¥: ì‹œì ë³„ ê°œì²´ëª… íƒœê¹… (BIO ë ˆì´ë¸”)
* ëª¨ë¸ êµ¬ì¡°:

  * Embedding(mask_zero=True)
  * Bidirectional LSTM (return_sequences=True)
  * TimeDistributed(Dense(tag_size, softmax))
* ì†ì‹¤ í•¨ìˆ˜: categorical_crossentropy
* ì„±ëŠ¥: ì•½ **95.7% ì •í™•ë„**

---

## 6. ë¬¸ì œì  ë° ë‹¤ìŒ ë‹¨ê³„

* ë‹¨ìˆœ ì •í™•ë„ë§Œ ì¸¡ì •í•˜ë©´ 'O' ë¹„ìœ¨ì´ ë†’ì•„ ê³¼ëŒ€í‰ê°€ë  ìˆ˜ ìˆìŒ
* í•´ê²°ì±…: **F1-score** ë„ì… (precision & recall ê¸°ë°˜)

---


> ### 12-05 BiLSTMì„ ì´ìš©í•œ ê°œì²´ëª… ì¸ì‹ (Named Entity Recognition, NER)

---

## 0. ê°œìš”

* ì•ì„œ ë°°ìš´ **BIO í‘œí˜„ ë°©ì‹**ì„ ì´ìš©í•´ ë°ì´í„°ì…‹ êµ¬ì„±
* **ì–‘ë°©í–¥ LSTM (BiLSTM)** ëª¨ë¸ë¡œ ê°œì²´ëª… ì¸ì‹ ìˆ˜í–‰
* ê¸°ì¡´ì—ëŠ” **ì •í™•ë„(accuracy)**ë¡œ í‰ê°€í–ˆì§€ë§Œ, ì´ë²ˆì—” **F1-score**ë¡œ ì„±ëŠ¥ì„ ê²€ì¦

---

## 1. ë°ì´í„° ì¤€ë¹„ & ì „ì²˜ë¦¬

### ë°ì´í„°ì…‹

* ì‚¬ìš© ë°ì´í„°: ì´ 47,959ê°œ ë¬¸ì¥, ë‹¨ì–´ ìˆ˜ 35,178ê°œ, íƒœê·¸ ìˆ˜ 17ê°œ

### ì£¼ìš” ì „ì²˜ë¦¬ ë‹¨ê³„

* `fillna(method='ffill')` â†’ Sentence ID ëˆ„ë½ ì±„ìš°ê¸°
* ëª¨ë“  ë‹¨ì–´ **ì†Œë¬¸ì ë³€í™˜**
* `(ë‹¨ì–´, íƒœê·¸)` ìŒìœ¼ë¡œ ë¬¶ì–´ ë¬¸ì¥ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
* `Tokenizer`ë¡œ ë‹¨ì–´ / íƒœê·¸ ì •ìˆ˜ ì¸ì½”ë”©
* `pad_sequences`ë¡œ ê¸¸ì´ 70ìœ¼ë¡œ íŒ¨ë”©

---

## 2. BiLSTM ëª¨ë¸ êµ¬ì¡°

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
hidden_units = 256

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])
```

* **Embedding**: ë‹¨ì–´ â†’ ë²¡í„° (mask_zero=True â†’ PAD ë¬´ì‹œ)
* **BiLSTM**: ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ í•™ìŠµ
* **TimeDistributed(Dense)**: ê° ì‹œì ë³„ë¡œ íƒœê·¸ ì˜ˆì¸¡ (ë‹¤ ëŒ€ ë‹¤ êµ¬ì¡°)
* **Softmax + CrossEntropy**: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜

---

## 3. í•™ìŠµ ë° ì˜ˆì¸¡

* ë°°ì¹˜ í¬ê¸°: 128
* Epoch: 6
* validation_split=0.1 (í›ˆë ¨ ë°ì´í„° ì¼ë¶€ë¡œ ê²€ì¦)
* ì•½ **95% ê²€ì¦ ì •í™•ë„** ë‹¬ì„±

ì˜ˆì‹œ ê²°ê³¼ (í…ŒìŠ¤íŠ¸ ë¬¸ì¥):

| ë‹¨ì–´                | ì‹¤ì œê°’   | ì˜ˆì¸¡ê°’   |
| ----------------- | ----- | ----- |
| the               | O     | O     |
| statement         | O     | O     |
| u.n.              | B-ORG | B-ORG |
| secretary-general | I-ORG | I-ORG |
| kofi              | B-PER | B-PER |
| annan             | I-PER | I-PER |
| amman             | B-GEO | B-GEO |
| wednesday         | B-TIM | B-TIM |

â†’ ê°œì²´ëª…ì„ ì •í™•í•˜ê²Œ ë§ì¶¤

---

## 4. ì„±ëŠ¥ í‰ê°€: ì™œ F1-scoreì¸ê°€?

### ë¬¸ì œì 

* ëŒ€ë¶€ë¶„ì˜ í† í°ì€ "O" (ê°œì²´ ì•„ë‹˜)
* ë”°ë¼ì„œ ë‹¨ìˆœ ì •í™•ë„ëŠ” ê³¼ëŒ€í‰ê°€ë¨

  * ì˜ˆ: ëª¨ë“  í† í°ì„ Oë¡œ ì˜ˆì¸¡í•´ë„ 70% ì´ìƒ ì •í™•ë„

### í•´ê²°ì±…: F1-score

* **ì •ë°€ë„(Precision)**: íŠ¹ì • ê°œì²´ë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
* **ì¬í˜„ìœ¨(Recall)**: ì‹¤ì œ íŠ¹ì • ê°œì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨
* **F1-score**: ë‘ ê°’ì„ ì¡°í™”í‰ê· 

---

## 5. ì‹¤í—˜ ê²°ê³¼ (seqeval ì‚¬ìš©)

```python
from seqeval.metrics import f1_score, classification_report
```

* ì „ì²´ F1-score: **78.5%**
* ì„¸ë¶€ ì„±ëŠ¥ ì˜ˆì‹œ:

| ê°œì²´ ìœ í˜• | Precision | Recall | F1-score |
| ----- | --------- | ------ | -------- |
| geo   | 0.84      | 0.84   | 0.84     |
| gpe   | 0.96      | 0.94   | 0.95     |
| org   | 0.57      | 0.58   | 0.57     |
| per   | 0.73      | 0.70   | 0.71     |
| tim   | 0.84      | 0.85   | 0.84     |

â†’ ì§€ëª…(geo, gpe), ì‹œê°„(tim)ì€ ì„±ëŠ¥ì´ ë†’ìŒ
â†’ ì¡°ì§(org), ì¸ëª…(per) ì¸ì‹ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ

---

